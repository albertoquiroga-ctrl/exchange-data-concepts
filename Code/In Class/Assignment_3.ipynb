{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a170c570",
   "metadata": {},
   "source": [
    "# ECON7880 — Assignment 3 (Template)\n",
    "\n",
    "**Notes**  \n",
    "- **Note 1:** for answers with Python, display both codes and results clearly.  \n",
    "- **Note 2:** for answers with manual calculation, please display all calculation steps clearly.\n",
    "\n",
    "This notebook mirrors the structure and style you used for Assignment 2: a brief environment setup followed by one section per question, each with Markdown of the question text and ready-to-run code stubs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4802a",
   "metadata": {},
   "source": [
    "## 0) Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c671971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML utilities (you can import more as needed)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (7.5, 5.0)\n",
    "plt.rcParams['axes.grid'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156722b",
   "metadata": {},
   "source": [
    "## 1) Question 1. [30 points @ 6 points each]\n",
    "\n",
    "A firm collected 5 training instances with 2 features $X_1$ and $X_2$, and their **Type** values:\n",
    "\n",
    "| Instance | $X_1$ | $X_2$ | Type |\n",
    "|:--:|:--:|:--:|:--:|\n",
    "| 0 | 13.4 | 11.2 | 1 |\n",
    "| 1 | 7.9  | 2.1  | 0 |\n",
    "| 2 | 7.1  | 8.9  | 1 |\n",
    "| 3 | 7.3  | 6.9  | 0 |\n",
    "| 4 | 10.7 | 8.9  | 1 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95d35f",
   "metadata": {},
   "source": [
    "**(a)** Use Python to plot the 5 instances with $X_1$ on the x-axis and $X_2$ on the y-axis. Visualize instances with different color according to their **Type** values.\n",
    "\n",
    "With a new instance with $(X_1, X_2) = (6.5, 2.1)$, complete the following tasks with either Python or manual calculation. **Round results to 4 decimal places if you use manual calculation. No need to round if you work with Python.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Q1 (a) Plot points by Type ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.DataFrame({\n",
    "    \"Instance\": [0, 1, 2, 3, 4],\n",
    "    \"X1\": [13.4, 7.9, 7.1, 7.3, 10.7],\n",
    "    \"X2\": [11.2, 2.1, 8.9, 6.9, 8.9],\n",
    "    \"Type\": [1, 0, 1, 0, 1]\n",
    "})\n",
    "new_pt = pd.Series({\"X1\": 6.5, \"X2\": 2.1})\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "for t, df_g in train.groupby(\"Type\"):\n",
    "    ax.scatter(df_g[\"X1\"], df_g[\"X2\"], label=f\"Type={t}\", s=70)\n",
    "ax.scatter(new_pt[\"X1\"], new_pt[\"X2\"], marker=\"*\", s=200, label=\"New point\")\n",
    "ax.set_xlabel(\"X1\")\n",
    "ax.set_ylabel(\"X2\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e287d4",
   "metadata": {},
   "source": [
    "**(b)** Calculate the **Euclidean Distance** between the new instance and each training instance using both $X_1$ and $X_2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae65f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Q1 (b) Euclidean distance to new_pt ---\n",
    "import numpy as np\n",
    "\n",
    "def euclidean(u, v):\n",
    "    u = np.asarray(u); v = np.asarray(v)\n",
    "    return float(np.linalg.norm(u - v))\n",
    "\n",
    "new_xy = np.array([new_pt[\"X1\"], new_pt[\"X2\"]])\n",
    "train[\"euclid_dist\"] = train[[\"X1\",\"X2\"]].apply(lambda r: euclidean(r.values, new_xy), axis=1)\n",
    "train[[\"Instance\",\"X1\",\"X2\",\"Type\",\"euclid_dist\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0d897",
   "metadata": {},
   "source": [
    "**(c)** Calculate the **Cosine Distance** between the new instance and each training instance using both $X_1$ and $X_2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Q1 (c) Cosine distance to new_pt ---\n",
    "def cosine_distance(u, v):\n",
    "    u = np.asarray(u); v = np.asarray(v)\n",
    "    num = np.dot(u, v)\n",
    "    den = np.linalg.norm(u) * np.linalg.norm(v)\n",
    "    cos_sim = num / den\n",
    "    return float(1 - cos_sim)\n",
    "\n",
    "train[\"cosine_dist\"] = train[[\"X1\",\"X2\"]].apply(lambda r: cosine_distance(r.values, new_xy), axis=1)\n",
    "train[[\"Instance\",\"X1\",\"X2\",\"Type\",\"euclid_dist\",\"cosine_dist\"]].sort_values(\"Instance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a76c0",
   "metadata": {},
   "source": [
    "**(d)** What is the predicted **Type** value for the new instance using **3-NN and majority vote (based on Cosine Distance)**? What is the estimated class probability?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Q1 (d) 3-NN majority vote using COSINE distance ---\n",
    "k = 3\n",
    "nbrs_cos = train.sort_values(\"cosine_dist\").head(k)\n",
    "pred_type_majority = int((nbrs_cos[\"Type\"].sum() >= (k/2)))  # tie -> 1\n",
    "prob_majority = nbrs_cos[\"Type\"].mean()\n",
    "\n",
    "print(f\"Predicted Type (3-NN majority on cosine): {pred_type_majority}\")\n",
    "print(f\"Estimated class probability: {prob_majority:.4f}\")\n",
    "nbrs_cos[[\"Instance\",\"Type\",\"cosine_dist\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (d):** The three cosine-distance neighbors are instances 1, 4, and 0; two of them have Type=1, so 3-NN majority predicts `Type = 1` with an estimated class probability of 0.6667.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a4715",
   "metadata": {},
   "source": [
    "**(e)** What’s the predicted **Type** value for the new instance using **3-NN and weighted voting (based on Euclidean Distance)**? What is the estimated class probability?\n",
    "\n",
    "Please report the results in one or two tables. For example, answers for Q1(b)-(c) can be organized as below:\n",
    "\n",
    "| Instance | X1 | X2 | Type | (b) Euclidean Distance | (c) Cosine Distance |\n",
    "|---:|---:|---:|---:|---:|---:|\n",
    "| 0 | 13.4 | 11.2 | 1 |   |   |\n",
    "| 1 | 7.9  | 2.1  | 0 |   |   |\n",
    "| 2 | 7.1  | 8.9  | 1 |   |   |\n",
    "| 3 | 7.3  | 6.9  | 0 |   |   |\n",
    "| 4 | 10.7 | 8.9  | 1 |   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Q1 (e) 3-NN weighted vote using EUCLIDEAN distance ---\n",
    "# Weights = 1 / distance (guarding against zero distance)\n",
    "eps = 1e-9\n",
    "nbrs_euc = train.sort_values(\"euclid_dist\").head(3).copy()\n",
    "nbrs_euc[\"w\"] = 1.0 / (nbrs_euc[\"euclid_dist\"] + eps)\n",
    "weighted_sum = (nbrs_euc[\"w\"] * nbrs_euc[\"Type\"]).sum()\n",
    "total_w = nbrs_euc[\"w\"].sum()\n",
    "prob_weighted = float(weighted_sum / total_w)\n",
    "pred_type_weighted = int(prob_weighted >= 0.5)\n",
    "\n",
    "print(f\"Predicted Type (3-NN weighted on euclidean): {pred_type_weighted}\")\n",
    "print(f\"Estimated class probability: {prob_weighted:.4f}\")\n",
    "nbrs_euc[[\"Instance\",\"Type\",\"euclid_dist\",\"w\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (e):** Inverse-distance weights heavily favor instance 1 (Type=0), so the weighted 3-NN vote predicts `Type = 0` with an estimated probability of 0.1374 for Type=1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff026f",
   "metadata": {},
   "source": [
    "## 2) Question 2. [30 points]\n",
    "\n",
    "A firm collected 6 instances with 2 features $X_1$ and $X_2$:\n",
    "\n",
    "| Instance | $X_1$ | $X_2$ |\n",
    "|:--:|:--:|:--:|\n",
    "| 0 | 1 | 4 |\n",
    "| 1 | 1 | 3 |\n",
    "| 2 | 0 | 5 |\n",
    "| 3 | 5 | 2 |\n",
    "| 4 | 6 | 3 |\n",
    "| 5 | 4 | 0 |\n",
    "\n",
    "With instance **0** and **3** selected as initial centroids, we’d like to simulate the **$k$-means** algorithm to separate all instances into two clusters ($k=2$). Complete the following with Python or manual calculation. **Round results to 4 decimal places if manual calculation; no need to round if using Python.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5dbbc",
   "metadata": {},
   "source": [
    "**(a)** Compute Euclidean distance from each instance to the initial centroids.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a581b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q2 setup & (a) Distances to initial centroids ---\n",
    "points = np.array([\n",
    "    [1, 4],\n",
    "    [1, 3],\n",
    "    [0, 5],\n",
    "    [5, 2],\n",
    "    [6, 3],\n",
    "    [4, 0],\n",
    "], dtype=float)\n",
    "\n",
    "idx0, idx3 = 0, 3\n",
    "centroids = np.vstack([points[idx0], points[idx3]])  # initial centroids (instances 0 and 3)\n",
    "\n",
    "def pairwise_euclid(a, b):\n",
    "    # a: (n, d), b: (m, d) -> (n, m)\n",
    "    return np.linalg.norm(a[:, None, :] - b[None, :, :], axis=2)\n",
    "\n",
    "dists = pairwise_euclid(points, centroids)\n",
    "df2 = pd.DataFrame(points, columns=[\"X1\", \"X2\"])\n",
    "df2.insert(0, \"Instance\", range(len(df2)))\n",
    "df2[\"dist_c0\"] = dists[:, 0]\n",
    "df2[\"dist_c1\"] = dists[:, 1]\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278965b9",
   "metadata": {},
   "source": [
    "**(b)** Assign instances to the two clusters by finding their closest centroids.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q2 (b) Cluster assignments via nearest centroid ---\n",
    "assign = dists.argmin(axis=1)\n",
    "df2[\"cluster\"] = assign\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8083a",
   "metadata": {},
   "source": [
    "**(c)** Compute the clustering quality with $\\text{SSE} = \\sum_{i=1}^k \\sum_{p \\in C_i} d(p, m_i)^2$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74088f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q2 (c) SSE for current clustering ---\n",
    "sse = float(np.sum((dists[np.arange(len(points)), assign]) ** 2))\n",
    "print(f\"SSE (iteration 1): {sse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416a2af",
   "metadata": {},
   "source": [
    "**(d)** Compute the **mean feature values** for instances in the two clusters respectively, in the format of $(X_1, X_2)$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ab842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q2 (d) Updated centroids from cluster means ---\n",
    "means = np.vstack([\n",
    "    points[assign == 0].mean(axis=0),\n",
    "    points[assign == 1].mean(axis=0),\n",
    "])\n",
    "print(\"Updated centroids (iteration 1 means):\", means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7699812",
   "metadata": {},
   "source": [
    "**(e)** **Update** the cluster centroids with the means from (d), then repeat steps (a)–(d) once. Will the clustering result (i.e., cluster labels) change? Any improvement in terms of **SSE**?\n",
    "\n",
    "**Suggested table for (a)–(d):**\n",
    "\n",
    "| Instance | X1 | X2 | (a) Dist to **Inst 0** | (a) Dist to **Inst 3** | (b) Cluster Label | (d) Updated Centroid |\n",
    "|---:|---:|---:|---:|---:|:--:|:--:|\n",
    "| 0 |   |   |   |   |   |   |\n",
    "| … |   |   |   |   |   |   |\n",
    "| 5 |   |   |   |   |   |   |\n",
    "\n",
    "**(c) SSE:** ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q2 (e) Repeat after updating centroids ---\n",
    "centroids2 = means.copy()\n",
    "d2 = pairwise_euclid(points, centroids2)\n",
    "assign2 = d2.argmin(axis=1)\n",
    "sse2 = float(np.sum((d2[np.arange(len(points)), assign2]) ** 2))\n",
    "\n",
    "df2_iter2 = pd.DataFrame(points, columns=[\"X1\", \"X2\"])\n",
    "df2_iter2.insert(0, \"Instance\", range(len(df2_iter2)))\n",
    "df2_iter2[\"dist_c0\"] = d2[:, 0]\n",
    "df2_iter2[\"dist_c1\"] = d2[:, 1]\n",
    "df2_iter2[\"cluster\"] = assign2\n",
    "\n",
    "display(df2_iter2)\n",
    "print(f\"SSE (iteration 2): {sse2:.4f}\")\n",
    "print(\"Centroids (iteration 2):\", centroids2)\n",
    "print(\"Did labels change?\", not np.array_equal(assign, assign2))\n",
    "print(f\"SSE improvement: {sse - sse2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (e):** After updating the centroids to (0.667, 4.000) and (5.000, 1.667), every instance keeps the same label `[0,0,0,1,1,1]`, but SSE drops from 10.0000 to 9.3333 (an improvement of 0.6667).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a71cc",
   "metadata": {},
   "source": [
    "## 3) Question 3. [24 points]\n",
    "\n",
    "A bank trained a classification model to predict the likelihood of default for each customer. There are **1000 customers** in the database: the “No Default” cases take up **80%** of the data while the “Default” cases take up **20%**. Applying this classifier on this dataset yields the following confusion matrix:\n",
    "\n",
    "**Confusion matrix**\n",
    "\n",
    "|              | Predicted: Default | Predicted: No Default |\n",
    "|:-------------|-------------------:|----------------------:|\n",
    "| **Actual: Default**    | 150 | 50  |\n",
    "| **Actual: No Default** | 100 | 700 |\n",
    "\n",
    "As the average lending amount is **$100** and interest rate is **15%**, the **cost-benefit matrix** (negative numbers mean cost) is:\n",
    "\n",
    "|              | Predicted: Default | Predicted: No Default |\n",
    "|:-------------|-------------------:|----------------------:|\n",
    "| **Actual: Default**    | $0  | $100 |\n",
    "| **Actual: No Default** | $0  | $15  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6f180",
   "metadata": {},
   "source": [
    "**(a)** Which group (“Default” or “No Default”) will you consider as the positive class?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (a):** Treat `Default` as the positive class because the firm's goal is to catch defaults; all recall/precision metrics therefore refer to the default customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6646647",
   "metadata": {},
   "source": [
    "**(b)** **[8 points @ 2 points each]** Calculate the following scores for this model: (i) **Accuracy**; (ii) **True positive rate (Sensitivity/Recall)**; (iii) **True negative rate (Specificity)**; (iv) **Precision (for the positive class only)**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q3 setup & (b) Classification metrics ---\n",
    "cm = np.array([[150, 50],\n",
    "               [100, 700]])\n",
    "TP, FN = cm[0, 0], cm[0, 1]\n",
    "FP, TN = cm[1, 0], cm[1, 1]\n",
    "\n",
    "N = cm.sum()\n",
    "pos = cm[0].sum()\n",
    "neg = cm[1].sum()\n",
    "\n",
    "accuracy = (TP + TN) / N\n",
    "tpr = TP / pos\n",
    "tnr = TN / neg\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"TPR:       {tpr:.4f}\")\n",
    "print(f\"TNR:       {tnr:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (b):** Accuracy = 0.8500, TPR = 0.7500, TNR = 0.8750, Precision (Default) = 0.6000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88a950",
   "metadata": {},
   "source": [
    "**(c)** Calculate the **expected value (per person)** for this model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b96244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q3 (c) Expected value per person ---\n",
    "benefit = np.array([[0, -100],\n",
    "                    [0,   15]], dtype=float)\n",
    "ev_total = float((cm * benefit).sum())\n",
    "ev_per_person = ev_total / N\n",
    "print(f\"Expected value per person (this model): ${ev_per_person:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (c):** Using the cost-benefit matrix (default + approved loan costs $100, hence -$100), the model yields $5,500 total benefit, i.e., **$5.50 per customer**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f59c59",
   "metadata": {},
   "source": [
    "**(d)** Assume we aim to target the same proportion of customers as in the first table, with only **positive predictions** targeted. **Write down the confusion matrix for a random classifier.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q3 (d) Random classifier confusion matrix ---\n",
    "pred_pos_rate = (TP + FP) / N\n",
    "rand_tp = round(pos * pred_pos_rate)\n",
    "rand_fp = round(neg * pred_pos_rate)\n",
    "rand_fn = pos - rand_tp\n",
    "rand_tn = neg - rand_fp\n",
    "\n",
    "cm_rand = np.array([[rand_tp, rand_fn],\n",
    "                    [rand_fp, rand_tn]])\n",
    "print(\"Random classifier confusion matrix (matching positive rate):\")\n",
    "print(cm_rand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (d):** Matching the same 25% positive prediction rate gives the random-classifier confusion matrix `[[50, 150], [200, 600]]` (rows = Actual Default/No Default).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f5fdf",
   "metadata": {},
   "source": [
    "**(e)** Calculate the overall **expected value (per person)** for the random classifier in step (d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q3 (e) Expected value for random classifier ---\n",
    "ev_rand_total = float((cm_rand * benefit).sum())\n",
    "ev_rand_per_person = ev_rand_total / N\n",
    "print(f\"Expected value per person (random base): ${ev_rand_per_person:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (e):** Plugging that matrix into the same payoffs gives an expected value of **-$6.00 per customer**, so the trained model is substantially better than random guessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344dc4c",
   "metadata": {},
   "source": [
    "## 4) Question 4. [16 points]\n",
    "\n",
    "Two classifiers (**Model A** and **Model B**) are used to predict whether the **Fed Funds rate will increase** or not (class label: 1 = increase, 0 = no increase), with each quarter considered as an instance. The estimated probabilities of increase over the past 6 quarters by model A and B respectively are displayed in the following table:\n",
    "\n",
    "| Quarter | Actual Class | Model A | Model B |\n",
    "|:--:|:--:|:--:|:--:|\n",
    "| 0 | 1 | 0.43 | 0.63 |\n",
    "| 1 | 1 | 0.52 | 0.53 |\n",
    "| 2 | 1 | 0.85 | 0.56 |\n",
    "| 3 | 1 | 0.69 | 0.71 |\n",
    "| 4 | 0 | 0.03 | 0.18 |\n",
    "| 5 | 0 | 0.31 | 0.76 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b626b",
   "metadata": {},
   "source": [
    "**(a)** Plot the **ROC curve** for the 2 classifiers together with the **random classifier**. Please calculate the **TP** and **FP** rates with the following cutoff values **[0, 0.2, 0.4, 0.5, 0.6, 0.8, 1]** before plotting the ROC curve.  \n",
    "(**Note:** you may need to calculate each model’s TP and FP rates at each cut-off first. The visualization can be done with either manually or with Python.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bf1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Q4 ROC computation and plot ---\n",
    "y_true = np.array([1,1,1,1,0,0])\n",
    "pA = np.array([0.43, 0.52, 0.85, 0.69, 0.03, 0.31])\n",
    "pB = np.array([0.63, 0.53, 0.56, 0.71, 0.18, 0.76])\n",
    "\n",
    "cutoffs = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "def tpr_fpr_at_cutoffs(y, p, cuts):\n",
    "    pts = []\n",
    "    pos = (y==1).sum()\n",
    "    neg = (y==0).sum()\n",
    "    for c in cuts:\n",
    "        yhat = (p >= c).astype(int)\n",
    "        TP = ((yhat==1) & (y==1)).sum()\n",
    "        FP = ((yhat==1) & (y==0)).sum()\n",
    "        TPR = TP / pos if pos else 0.0\n",
    "        FPR = FP / neg if neg else 0.0\n",
    "        pts.append((FPR, TPR))\n",
    "    return np.array(pts)\n",
    "\n",
    "ptsA = tpr_fpr_at_cutoffs(y_true, pA, cutoffs)\n",
    "ptsB = tpr_fpr_at_cutoffs(y_true, pB, cutoffs)\n",
    "\n",
    "# Also compute AUC (continuous ROC using sklearn for reference)\n",
    "fprA, tprA, _ = roc_curve(y_true, pA)\n",
    "fprB, tprB, _ = roc_curve(y_true, pB)\n",
    "aucA = auc(fprA, tprA)\n",
    "aucB = auc(fprB, tprB)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0,1], [0,1], 'k--', label=\"Random\")\n",
    "ax.plot(ptsA[:,0], ptsA[:,1], 'o-', label=f\"Model A (AUC={aucA:.3f})\")\n",
    "ax.plot(ptsB[:,0], ptsB[:,1], 'o-', label=f\"Model B (AUC={aucB:.3f})\")\n",
    "ax.set_xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "ax.set_ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "ax.set_title(\"ROC Curves at Specified Cutoffs\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Write your reasoning for (b) below this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a6980",
   "metadata": {},
   "source": [
    "**(b)** Which model is better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (b):** Model A strictly dominates Model B on the ROC curve (AUC 1.000 vs. 0.500), so Model A is better at every cutoff while Model B behaves like random guessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bf68a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### References / reminders (not graded)\n",
    "- Problem statements are transcribed from **Assignment 3**.  \n",
    "- This template mirrors the layout you used in Assignment 2 (section headers, environment setup, and code stubs), but **no solutions are provided**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}