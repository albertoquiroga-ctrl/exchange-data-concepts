{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6434662d",
   "metadata": {},
   "source": [
    "We will use a new package ``mlxtend`` for association rules mining. Check the [documentation](https://rasbt.github.io/mlxtend/) for details.  \n",
    "\n",
    "You may need to install the package first with the following code. \n",
    "\n",
    "```python\n",
    "pip install mlxtend\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98df572",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing\n",
    "\n",
    "We sampled 9958 Tweets with two or more food/drink emojis, each tweets can be considered as a itemset for emojis.   \n",
    "\n",
    "- Let's start by loading the dataset and inspect its first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets_df = pd.read_csv(\"food_drink_emoji_tweets.txt\", sep=\"\\t\", header=None)  # tab separated data, ignore header\n",
    "\n",
    "tweets_df.columns = ['text']     # rename the column name  \n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261af02",
   "metadata": {},
   "source": [
    "### 1.1 Extract food/drink emojis in each tweet\n",
    "\n",
    "Each row is a tweet/post. Let's extract the emojis that appear in each post as an itemset. \n",
    "\n",
    "- We are only interested in emojis that are food and drinks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36d484-0e50-41bd-b70e-cdc3abd34178",
   "metadata": {},
   "source": [
    "**Step 1: create a emoji set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97eb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list = \"ğŸ‡ğŸˆğŸ‰ğŸŠğŸ‹ğŸŒğŸğŸ¥­ğŸğŸğŸğŸ‘ğŸ’ğŸ“ğŸ¥ğŸ…ğŸ¥¥ğŸ¥‘ğŸ†ğŸ¥”ğŸ¥•ğŸŒ½ğŸŒ¶ğŸ¥’ğŸ¥¬ğŸ¥¦ğŸ„ğŸ¥œğŸŒ°ğŸğŸ¥ğŸ¥–ğŸ¥¨ğŸ¥¯ğŸ¥ğŸ§€ğŸ–ğŸ—ğŸ¥©ğŸ¥“ğŸ”ğŸŸğŸ•ğŸŒ­ğŸ¥ªğŸŒ®ğŸŒ¯ğŸ¥™ğŸ¥šğŸ³ğŸ¥˜ğŸ²ğŸ¥£ğŸ¥—ğŸ¿ğŸ§‚ğŸ¥«ğŸ±ğŸ˜ğŸ™ğŸšğŸ›ğŸœğŸğŸ ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¥®ğŸ¡ğŸ¥ŸğŸ¥ ğŸ¥¡ğŸ¦€ğŸ¦ğŸ¦ğŸ¦‘ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ‚ğŸ°ğŸ§ğŸ¥§ğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ¼ğŸ¥›â˜•ğŸµğŸ¶ğŸ¾ğŸ·ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¥‚ğŸ¥ƒ\"\n",
    "        \n",
    "emoji_set = set(emoji_list)  # convert the sequence of emjois as itemset\n",
    "\n",
    "#emoji_set    # uncomment to see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4231386-6d11-40f9-b75b-828e6ba854bb",
   "metadata": {},
   "source": [
    "**Step 2: extract emoji from each tweet as a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff72fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_emoji(text):\n",
    "    emoji_list_per_tweet = [emoji for emoji in text if emoji in emoji_set]       # list comphrehension\n",
    "    emoji_set_per_tweet = set(emoji_list_per_tweet)                              # convert the list as a set \n",
    "    return  emoji_set_per_tweet                                        \n",
    "\n",
    "tweets_df['emojis'] = tweets_df['text'].apply(extract_unique_emoji)        # apply the function to column 'text'\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb579598",
   "metadata": {},
   "source": [
    "### 1.2 Transform emoji items into matrix \n",
    "\n",
    "Before we use the `mlxtend` for frequent itemset mining, we need to encode the items into a matrix where (1) each row represents an itemset (tweet) and (2) each column represents an item, and (3) each cell record whether an item is in the itemset (tweet) or not.  \n",
    "\n",
    "- Let's use the `TransactionEncoder` function from `mlxtend.preprocessing`. Check [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.preprocessing/#transactionencoder) for details.\n",
    "- Alternatively, you may also use the `MultiLabelBinarizer` from `scikit-learn`. Check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadce3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "encoder = TransactionEncoder()\n",
    "\n",
    "arr = encoder.fit_transform(tweets_df['emojis'])   # return a matrix\n",
    "\n",
    "arr   # a 2D array of boolean values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ebb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert as dataframe\n",
    "\n",
    "emoji_df = pd.DataFrame(data = arr,  columns = encoder.columns_)\n",
    "\n",
    "emoji_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba87d73",
   "metadata": {},
   "source": [
    "# 2.  Frequent Itemsets Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a778a9e",
   "metadata": {},
   "source": [
    "## 2.1 Find frequent itemsets \n",
    "\n",
    "Now, let's apply the Apriori Algorithm to the emoji dataset.\n",
    "\n",
    "Call the `apriori` API from `mlxtend.frequent_patterns` and specify the minimal support we want. Check the [User Guide](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/) and [Documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/#apriori) for more details.\n",
    "\n",
    "- set `min_support=0.005`, i.e., half percent of the tweets. \n",
    "- set ``use_colnames=True`` to convert column indices of each item (default) into the respective item names for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac274de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemsets = apriori(df = emoji_df, min_support = 0.005, use_colnames = True)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96679811",
   "metadata": {},
   "source": [
    "## 2.2 Sort frequent itemsets by their support values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bcbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.sort_values(by = 'support',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202569a",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 1: Your Codes Here***</font>  \n",
    "\n",
    "**Step 1**: How many itemsets appears in at least 10% of the posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd85618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c32fca1b",
   "metadata": {},
   "source": [
    "**Step 2**: Extract frequent itemsets with at least two 2 items.\n",
    "\n",
    "- Hint: use `pandas.Series.apply` function together with the `len` to check the number of items in each itemset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0d0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efb3649d",
   "metadata": {},
   "source": [
    "# 3.  Association Rules \n",
    "\n",
    "In this section, we will practice how to evaluate the relationship between frequent itemsets. \n",
    "\n",
    "- Let's call the `association_rules` API from `mlxtend.frequent_patterns`. Visit the [User Guide](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/)  and [Documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/#association_rules) for more details.\n",
    "\n",
    "The association rules represent the two relationship between to frequent itemsets: an `antecedent` item(set) and a `consequent` item(set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(df = frequent_itemsets,     # frequent itemsets generated by apriori algorihm\n",
    "                          metric = \"lift\",         \n",
    "                          min_threshold = 1)          # min lift = 1\n",
    "\n",
    "rules.sort_values(by = 'lift',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f8a47",
   "metadata": {},
   "source": [
    "In the returned data frame, each row examines a pair of `antecedent` (X) -> `consequent` (Y). \n",
    "\n",
    "- While the `Antecedent support` and `consequent support` measure *P*(X) and *P*(Y) respectively, the `support` measures the join probability *P*(X, Y).  \n",
    "\n",
    "Other metrics such as `confidence`, `lift`, `leverage`, and `conviction` can be derived from the three support values. \n",
    "\n",
    "For example, $$\\text{confidence}=\\frac{\\text{support}}{\\text{antecedent\\_support}},$$ and \n",
    "\n",
    "\n",
    "$$\\text{lift} =\\frac{\\text{confidence}}{\\text{consequent\\_support}}=\\frac{\\text{support}}{\\text{antecedent\\_support} \\times \\text{consequent\\_support}},$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\\text{leverage}= \\text{support} - \\text{antecedent\\_support} * \\text{consequent\\_support}, $$ \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509612ab",
   "metadata": {},
   "source": [
    "# 4.  Itemset Similarity\n",
    "\n",
    "Pattern and similarity are two basic outputs of data mining. So far, we have been playing with patterns - frequent itemsets and association rules can all be seen as \"patterns\". In the last part, let's work on itemset similarities.\n",
    " \n",
    "**Jaccard similarity** is a simple but powerful measurement of itemset similarity, defined as follows:\n",
    "$$\\text{Jaccard\\_similarity(A, B)} = \\frac{|A\\cap B|}{|A\\cup B|}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(A, B):\n",
    "    intersection = A.intersection(B)\n",
    "    union = A.union(B)\n",
    "    jaccard_sim = len(intersection)/len(union)\n",
    "    return jaccard_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d56366",
   "metadata": {},
   "source": [
    "Calculate the jaccard similarity of two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_A = set(['ğŸ‡', 'ğŸ’', 'ğŸ'])\n",
    "fruit_B = set(['ğŸŠ', 'ğŸ’', 'ğŸ'])\n",
    "\n",
    "jaccard_similarity(fruit_A, fruit_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae608b",
   "metadata": {},
   "source": [
    "**Which tweet is most similiar to the first tweet post in terms of the set of food and drink emojis used?** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dacf91d",
   "metadata": {},
   "source": [
    "**Step 1**: define a function which returns the jaccard similarity between the first itemset and any itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_one(X):\n",
    "    first_tweet = tweets_df.loc[0, 'emojis']\n",
    "    jaccard_sim = jaccard_similarity(first_tweet, X)\n",
    "    return jaccard_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751c1e8",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 2: Your Codes Here***</font>  \n",
    "\n",
    "**Step 2**: calculate the jaccard similarity of the first emoji itemset and the itemsets in each row, and save the jaccard similarities a separate column named `jaccard`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe60b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb72d2c",
   "metadata": {},
   "source": [
    "**Step 3**: sort the dataframe by the column `jaccard` and check the top 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1a965",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
