{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426c05a3",
   "metadata": {},
   "source": [
    "# Credit Default Modeling (Beginner Walkthrough)\n",
    "We build several simple classifiers to predict whether a credit card customer will default. The dataset comes from the UCI credit card default file (Excel). This notebook favors clear explanations over compact code.\n",
    "You will see how we connect model predictions to money gained or lost, and we repeat the experiment with a few random seeds to check stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea55be",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "We load pandas/numpy for data handling, matplotlib/seaborn for charts, and scikit-learn for the models. Everything is imported up front so you can see the full toolbox. (xlrd is pulled automatically when pandas reads the Excel file.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08110cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data wrangling and math helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting libraries for quick visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Path handling and scikit-learn pieces for modeling\n",
    "from pathlib import Path\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "# Make plots look clean and allow wide tables in outputs\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301e508",
   "metadata": {},
   "source": [
    "## 2. Load the data\n",
    "We read the Excel file with the credit card records, rename the target column to `TARGET` (1 = will default, 0 = will pay), and peek at the first rows to sanity check the load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94df1e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30,000 rows and 25 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913</td>\n",
       "      <td>3102</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682</td>\n",
       "      <td>1725</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239</td>\n",
       "      <td>14027</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990</td>\n",
       "      <td>48233</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617</td>\n",
       "      <td>5670</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0     -2     -2       3913       3102        689          0          0   \n",
       "1      0      2       2682       1725       2682       3272       3455   \n",
       "2      0      0      29239      14027      13559      14331      14948   \n",
       "3      0      0      46990      48233      49291      28314      28959   \n",
       "4      0      0       8617       5670      35835      20940      19146   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0          0         0       689         0         0         0         0   \n",
       "1       3261         0      1000      1000      1000         0      2000   \n",
       "2      15549      1518      1500      1000      1000      1000      5000   \n",
       "3      29547      2000      2019      1200      1100      1069      1000   \n",
       "4      19131      2000     36681     10000      9000       689       679   \n",
       "\n",
       "   TARGET  \n",
       "0       1  \n",
       "1       1  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate the Excel data file. We check two possible paths so the notebook works from different folders.\n",
    "DATA_FILE = Path(\"default of credit card clients.xls\")\n",
    "if not DATA_FILE.exists():\n",
    "    DATA_FILE = Path(\"Code\") / \"Final Project\" / \"default of credit card clients.xls\"\n",
    "\n",
    "# Stop early with a clear message if the file is missing.\n",
    "if not DATA_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Could not find data file at {DATA_FILE}\")\n",
    "\n",
    "# Read the Excel file.\n",
    "# header=1 skips the first row of metadata in the UCI file.\n",
    "# We rename the target column to TARGET to make the code shorter to read.\n",
    "raw_df = (\n",
    "    pd.read_excel(DATA_FILE, header=1)\n",
    "    .rename(columns={\"default payment next month\": \"TARGET\"})\n",
    ")\n",
    "\n",
    "print(f\"Loaded {raw_df.shape[0]:,} rows and {raw_df.shape[1]} columns.\")\n",
    "# Peek at the first few rows to make sure the load worked.\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35deab",
   "metadata": {},
   "source": [
    "## 3. Split features and target\n",
    "We separate the input columns (`X`) from the answer column (`y`). We also print shapes and the number of people who defaulted vs. not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b230d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (30000, 23)\n",
      "Target breakdown: {0: 23364, 1: 6636}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913</td>\n",
       "      <td>3102</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682</td>\n",
       "      <td>1725</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239</td>\n",
       "      <td>14027</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990</td>\n",
       "      <td>48233</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617</td>\n",
       "      <td>5670</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0      20000    2          2         1   24      2      2     -1     -1   \n",
       "1     120000    2          2         2   26     -1      2      0      0   \n",
       "2      90000    2          2         2   34      0      0      0      0   \n",
       "3      50000    2          2         1   37      0      0      0      0   \n",
       "4      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0     -2     -2       3913       3102        689          0          0   \n",
       "1      0      2       2682       1725       2682       3272       3455   \n",
       "2      0      0      29239      14027      13559      14331      14948   \n",
       "3      0      0      46990      48233      49291      28314      28959   \n",
       "4      0      0       8617       5670      35835      20940      19146   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "0          0         0       689         0         0         0         0  \n",
       "1       3261         0      1000      1000      1000         0      2000  \n",
       "2      15549      1518      1500      1000      1000      1000      5000  \n",
       "3      29547      2000      2019      1200      1100      1069      1000  \n",
       "4      19131      2000     36681     10000      9000       689       679  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect every column except the unique ID and the target label.\n",
    "feature_cols = []\n",
    "for col in raw_df.columns:\n",
    "    if col not in {\"TARGET\", \"ID\"}:\n",
    "        feature_cols.append(col)\n",
    "\n",
    "# X holds the input features, y holds the answer we want to predict (1 = will default, 0 = will pay).\n",
    "X = raw_df[feature_cols]\n",
    "y = raw_df[\"TARGET\"].astype(int)\n",
    "\n",
    "# Quick sanity checks on shapes and class balance.\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target breakdown: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Show the very first rows of X and y so beginners can see the structure.\n",
    "display(X.head(), y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979aa98",
   "metadata": {},
   "source": [
    "## 4. Reusable splits (shared across seeds)\n",
    "We create train/validation/test splits for several random seeds so each model is trained and evaluated on the exact same partitions. This keeps comparisons fair across all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b9168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 2025, 'train_sub': (18000, 23), 'val': (6000, 23), 'test': (6000, 23)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will try a few different random seeds to see how stable the results are.\n",
    "SEED_PLAN = [2025, 0, 1033]\n",
    "VAL_SEED = 0  # keep the validation split fixed so threshold tuning is comparable\n",
    "BASELINE_SEED = SEED_PLAN[0]  # this seed will be used for most plots\n",
    "\n",
    "# Helper function to create train/validation/test splits for one seed.\n",
    "def build_split_bundle(split_seed, val_seed=VAL_SEED):\n",
    "    # First split once into train and test (stratified keeps class ratios similar in each split).\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=split_seed,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    # Then carve out a validation slice from the training set for threshold tuning.\n",
    "    X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=0.25,\n",
    "        random_state=val_seed,\n",
    "        stratify=y_train,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"seed\": split_seed,\n",
    "        \"X_train_full\": X_train,\n",
    "        \"y_train_full\": y_train,\n",
    "        \"X_train_sub\": X_train_sub,\n",
    "        \"X_val\": X_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train_sub\": y_train_sub,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "\n",
    "# Build and store the splits for each seed in a simple loop (more verbose but clearer for beginners).\n",
    "split_store = {}\n",
    "for seed in SEED_PLAN:\n",
    "    split_store[seed] = build_split_bundle(seed, val_seed=VAL_SEED)\n",
    "\n",
    "baseline_split = split_store[BASELINE_SEED]\n",
    "\n",
    "display(\n",
    "    {\n",
    "        \"seed\": BASELINE_SEED,\n",
    "        \"train_sub\": baseline_split[\"X_train_sub\"].shape,\n",
    "        \"val\": baseline_split[\"X_val\"].shape,\n",
    "        \"test\": baseline_split[\"X_test\"].shape,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a4d3e",
   "metadata": {},
   "source": [
    "## 5. Business assumptions and cost matrix\n",
    "We translate model outcomes into dollars. Approving a good customer earns interest; approving a bad one loses money. We compute those numbers once on the training data and keep them fixed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd20ff",
   "metadata": {},
   "source": [
    "### 5.1 Observation window and APR assumptions\n",
    "We assume 18% APR, monthly compounding, and only look at six months. Using the average credit limits of good/bad customers, we estimate expected profit from a good approval and expected loss from a bad approval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8c5feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodic rate: 1.50%\n",
      "Approx profit per good customer: 16,647.94\n",
      "Approx loss per bad customer:    65,062.88\n"
     ]
    }
   ],
   "source": [
    "baseline_train = split_store[BASELINE_SEED]\n",
    "\n",
    "# Make a copy so we can add the TARGET column back for easy filtering by default status.\n",
    "train_full_df = baseline_train[\"X_train_full\"].copy()\n",
    "train_full_df[\"TARGET\"] = baseline_train[\"y_train_full\"].values\n",
    "\n",
    "# Compute business parameters only on the training sample to avoid peeking at test.\n",
    "# These averages will stay fixed across seeds so the dollar values are comparable.\n",
    "df_default = train_full_df[train_full_df[\"TARGET\"] == 1]       # customers who will default\n",
    "df_no_default = train_full_df[train_full_df[\"TARGET\"] == 0]    # customers who will not default\n",
    "\n",
    "mean_limit_default = df_default[\"LIMIT_BAL\"].mean()\n",
    "mean_limit_no_default = df_no_default[\"LIMIT_BAL\"].mean()\n",
    "\n",
    "# Assumptions pulled from the data dictionary PDF.\n",
    "assumption_config = {\n",
    "    \"annual_apr\": 0.18,           # 18% annual percentage rate\n",
    "    \"periods_per_year\": 12,       # monthly compounding\n",
    "    \"observation_months\": 6,      # we only look at 6 billing cycles\n",
    "    \"loss_given_default\": 0.5,    # we expect to lose 50% of the limit if they default\n",
    "}\n",
    "\n",
    "# Unpack the assumptions into readable variable names.\n",
    "annual_apr = assumption_config[\"annual_apr\"]\n",
    "periods_per_year = assumption_config[\"periods_per_year\"]\n",
    "observation_months = assumption_config[\"observation_months\"]\n",
    "loss_given_default = assumption_config[\"loss_given_default\"]\n",
    "\n",
    "# Translate APR to the window we care about.\n",
    "periodic_rate = annual_apr / periods_per_year\n",
    "period_length_months = 12 / periods_per_year\n",
    "periods_in_window = observation_months / period_length_months\n",
    "\n",
    "# Expected profit if we APPROVE someone who will pay back.\n",
    "profit_good = mean_limit_no_default * ((1 + periodic_rate) ** periods_in_window - 1)\n",
    "# Expected loss if we APPROVE someone who will default.\n",
    "loss_bad = mean_limit_default * loss_given_default\n",
    "\n",
    "print(f\"Periodic rate: {periodic_rate:.2%}\")\n",
    "print(f\"Approx profit per good customer: {profit_good:,.2f}\")\n",
    "print(f\"Approx loss per bad customer:    {loss_bad:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8923e7c",
   "metadata": {},
   "source": [
    "### 5.2 Cost/benefit matrix\n",
    "We build a small table that maps each actual/predicted outcome (TP, FP, TN, FN) to the dollar value from the assumptions. This table powers the utility calculations later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24058a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16647.939093</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-65062.881899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted             0    1\n",
       "Actual                      \n",
       "0          16647.939093  0.0\n",
       "1         -65062.881899  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dollar values for each possible outcome.\n",
    "value_TN = profit_good    # Actual 0, Pred 0: good customer approved -> earn interest\n",
    "value_FP = 0.0            # Actual 0, Pred 1: good customer rejected -> miss the profit\n",
    "value_FN = -loss_bad      # Actual 1, Pred 0: bad customer approved -> lose money\n",
    "value_TP = 0.0            # Actual 1, Pred 1: bad customer rejected -> avoid the loss\n",
    "\n",
    "# Store them in a small lookup table (rows = actual, columns = predicted).\n",
    "value_matrix = pd.DataFrame(\n",
    "    {\n",
    "        0: {0: value_TN, 1: value_FN},  # Predicted 0\n",
    "        1: {0: value_FP, 1: value_TP},  # Predicted 1\n",
    "    }\n",
    ")\n",
    "value_matrix.index.name = \"Actual\"\n",
    "value_matrix.columns.name = \"Predicted\"\n",
    "\n",
    "value_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf8832",
   "metadata": {},
   "source": [
    "## 6. Utility and evaluation helpers\n",
    "Helper functions to get model scores, sweep decision thresholds, compute utilities, and build tidy summary tables with common metrics (accuracy, precision, recall, ROC AUC, etc.). Each helper is commented for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c197f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_FOLDS = 5  # how many folds to use during cross-validation\n",
    "\n",
    "# Thresholds we will try when turning scores into 0/1 predictions.\n",
    "THRESH_QUANTILES = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "\n",
    "def score_predictions(model, features):\n",
    "    # Return a continuous score for each row.\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(features)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(features)\n",
    "    # Fall back to hard class predictions if the model has no scoring method.\n",
    "    return model.predict(features)\n",
    "\n",
    "\n",
    "def utility_from_predictions(y_true, y_pred, values=value_matrix):\n",
    "    # Count true negatives, false positives, false negatives, and true positives.\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Multiply each count by its dollar value to get total utility.\n",
    "    utility_total = (\n",
    "        tn * values.loc[0, 0]\n",
    "        + fp * values.loc[0, 1]\n",
    "        + fn * values.loc[1, 0]\n",
    "        + tp * values.loc[1, 1]\n",
    "    )\n",
    "    # Utility per case lets us compare across datasets of different sizes.\n",
    "    utility_per_case = utility_total / len(y_true)\n",
    "    return utility_total, utility_per_case, {\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp}\n",
    "\n",
    "\n",
    "def _utility(y_true, y_pred):\n",
    "    # Thin wrapper so we can pass the function around easily.\n",
    "    return utility_from_predictions(y_true, y_pred, value_matrix)\n",
    "\n",
    "\n",
    "def sweep_thresholds(y_true, scores, utility_fn, grid=None):\n",
    "    # Try many thresholds and keep the one that gives the highest utility.\n",
    "    if grid is None:\n",
    "        # Use score quantiles so thresholds cover the range evenly.\n",
    "        grid = np.unique(np.quantile(scores, THRESH_QUANTILES))\n",
    "\n",
    "    records = []\n",
    "    for t in grid:\n",
    "        preds = (scores >= t).astype(int)\n",
    "        utility_total, utility_pc, _ = utility_fn(y_true, preds)\n",
    "        records.append(\n",
    "            {\n",
    "                \"threshold\": float(t),\n",
    "                \"utility\": utility_total,\n",
    "                \"utility_per_case\": utility_pc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Pick the threshold with the largest total utility.\n",
    "    best = max(records, key=lambda r: r[\"utility\"])\n",
    "    return {\n",
    "        \"threshold\": best[\"threshold\"],\n",
    "        \"val_utility\": best[\"utility\"],\n",
    "        \"val_utility_per_case\": best[\"utility_per_case\"],\n",
    "        \"sweep\": pd.DataFrame(records),\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_model(\n",
    "    model_name,\n",
    "    dataset_name,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_score,\n",
    "    threshold,\n",
    "    seed,\n",
    "    utility_fn=_utility,\n",
    "):\n",
    "    # Basic classification counts.\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    # Utility and ROC AUC capture the money side and the ranking quality.\n",
    "    utility_total, utility_pc, _ = utility_fn(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_score) if y_score is not None else np.nan\n",
    "\n",
    "    total = len(y_pred)\n",
    "    acceptance_rate = (tn + fn) / total  # predicted 0 = approved\n",
    "    rejection_rate = (fp + tp) / total   # predicted 1 = rejected\n",
    "\n",
    "    # Return a one-row DataFrame so everything is easy to stack later.\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"seed\": seed,\n",
    "                \"threshold\": threshold,\n",
    "                \"roc_auc\": roc_auc,\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "                \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "                \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "                \"utility_total\": utility_total,\n",
    "                \"utility_per_case\": utility_pc,\n",
    "                \"acceptance_rate\": acceptance_rate,\n",
    "                \"rejection_rate\": rejection_rate,\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"tn\": tn,\n",
    "                \"fn\": fn,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def utility_from_scores(y_true, scores):\n",
    "    # Utility per case using the best threshold on the provided scores.\n",
    "    sweep = sweep_thresholds(y_true, scores, _utility)\n",
    "    return sweep[\"val_utility_per_case\"]\n",
    "\n",
    "\n",
    "# Custom scorer for cross-validation so models are compared on utility, not just accuracy.\n",
    "utility_scorer = make_scorer(utility_from_scores, needs_threshold=True)\n",
    "\n",
    "\n",
    "def run_model_for_seed(\n",
    "    model,\n",
    "    model_name,\n",
    "    split_seed,\n",
    "    threshold_grid=None,\n",
    "    refit_on_full_train=True,\n",
    "):\n",
    "    # Pull the pre-built splits for this seed.\n",
    "    split = split_store[split_seed]\n",
    "    X_train = split[\"X_train_sub\"]\n",
    "    X_val = split[\"X_val\"]\n",
    "    X_test = split[\"X_test\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "    y_val = split[\"y_val\"]\n",
    "    y_test = split[\"y_test\"]\n",
    "\n",
    "    # Clone the model so the original is untouched.\n",
    "    working_model = clone(model)\n",
    "    working_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get scores on the validation set and find the best threshold.\n",
    "    val_scores = score_predictions(working_model, X_val)\n",
    "    sweep = sweep_thresholds(y_val, val_scores, _utility, grid=threshold_grid)\n",
    "\n",
    "    # Optionally refit on the full training data before testing.\n",
    "    final_model = working_model\n",
    "    if refit_on_full_train:\n",
    "        final_model = clone(model)\n",
    "        final_model.fit(split[\"X_train_full\"], split[\"y_train_full\"])\n",
    "\n",
    "    # Apply the chosen threshold on the test scores.\n",
    "    test_scores = score_predictions(final_model, X_test)\n",
    "    test_pred = (test_scores >= sweep[\"threshold\"]).astype(int)\n",
    "\n",
    "    summary = summarize_model(\n",
    "        model_name=model_name,\n",
    "        dataset_name=f\"Test (seed={split_seed})\",\n",
    "        y_true=y_test,\n",
    "        y_pred=test_pred,\n",
    "        y_score=test_scores,\n",
    "        threshold=sweep[\"threshold\"],\n",
    "        seed=split_seed,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"seed\": split_seed,\n",
    "        \"model_name\": model_name,\n",
    "        \"model\": final_model,\n",
    "        \"threshold\": sweep[\"threshold\"],\n",
    "        \"val_sweep\": sweep[\"sweep\"],\n",
    "        \"val_utility\": sweep[\"val_utility\"],\n",
    "        \"summary\": summary,\n",
    "        \"test_scores\": test_scores,\n",
    "        \"test_pred\": test_pred,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3a2cf",
   "metadata": {},
   "source": [
    "## 7. Models trained across seeds\n",
    "For each random seed we fit every model, find the best threshold on the validation fold, then evaluate on the test fold. This gives us comparable rows per seed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d208764",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression\n",
    "Simple linear classifier with L2 regularization. We pick the best `C` (regularization strength) via cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0455c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression helper with cross-validation on C to balance regularization strength.\n",
    "def run_logreg_for_seed(\n",
    "    split_seed,\n",
    "    C_grid=(0.01, 0.1, 1.0, 10.0),\n",
    "    threshold_grid=None,\n",
    "    cv_folds=CV_FOLDS,\n",
    "):\n",
    "    # Pull training data for this seed.\n",
    "    split = split_store[split_seed]\n",
    "    X_train = split[\"X_train_sub\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "\n",
    "    # Stratified folds keep the class balance similar in each fold.\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=split_seed)\n",
    "    cv_rows = []\n",
    "    best_mean = -float(\"inf\")\n",
    "    best_C = None\n",
    "\n",
    "    for C in C_grid:\n",
    "        # Pipeline: scale features then fit logistic regression.\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                penalty=\"l2\",\n",
    "                solver=\"liblinear\",\n",
    "                max_iter=500,\n",
    "                random_state=split_seed,\n",
    "                class_weight=\"balanced\",\n",
    "                C=C,\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Evaluate utility with cross-validation.\n",
    "        scores = cross_val_score(\n",
    "            pipe,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=utility_scorer,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        cv_mean = float(scores.mean())\n",
    "        cv_std = float(scores.std())\n",
    "        cv_rows.append({\"C\": C, \"cv_mean_utility\": cv_mean, \"cv_std_utility\": cv_std})\n",
    "\n",
    "        # Track the best C value. Skip NaN means to avoid leaving best_C as None.\n",
    "        if not np.isnan(cv_mean) and cv_mean > best_mean:\n",
    "            best_mean = cv_mean\n",
    "            best_C = C\n",
    "\n",
    "    # Fallback in case all cv_mean values were NaN.\n",
    "    if best_C is None:\n",
    "        best_C = C_grid[0]\n",
    "\n",
    "    # Fit the best model on the full training subset.\n",
    "    best_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            penalty=\"l2\",\n",
    "            solver=\"liblinear\",\n",
    "            max_iter=500,\n",
    "            random_state=split_seed,\n",
    "            class_weight=\"balanced\",\n",
    "            C=best_C,\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    result = run_model_for_seed(\n",
    "        best_model,\n",
    "        model_name=\"Logistic Regression\",\n",
    "        split_seed=split_seed,\n",
    "        threshold_grid=threshold_grid,\n",
    "        refit_on_full_train=True,\n",
    "    )\n",
    "    result[\"cv_results\"] = pd.DataFrame(cv_rows)\n",
    "    result[\"cv_best_params\"] = {\"C\": best_C, \"cv_mean_utility\": best_mean}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6fc7fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>seed</th>\n",
       "      <th>threshold</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>utility_total</th>\n",
       "      <th>utility_per_case</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>rejection_rate</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Test (seed=2025)</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.525875</td>\n",
       "      <td>0.722673</td>\n",
       "      <td>0.735667</td>\n",
       "      <td>0.428963</td>\n",
       "      <td>0.589299</td>\n",
       "      <td>0.496508</td>\n",
       "      <td>2.500604e+07</td>\n",
       "      <td>4167.674025</td>\n",
       "      <td>0.696167</td>\n",
       "      <td>0.303833</td>\n",
       "      <td>782</td>\n",
       "      <td>1041</td>\n",
       "      <td>3632</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Test (seed=0)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.544998</td>\n",
       "      <td>0.714441</td>\n",
       "      <td>0.761333</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.541070</td>\n",
       "      <td>0.500697</td>\n",
       "      <td>2.447127e+07</td>\n",
       "      <td>4078.545072</td>\n",
       "      <td>0.743167</td>\n",
       "      <td>0.256833</td>\n",
       "      <td>718</td>\n",
       "      <td>823</td>\n",
       "      <td>3850</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Test (seed=1033)</td>\n",
       "      <td>1033</td>\n",
       "      <td>0.549389</td>\n",
       "      <td>0.713097</td>\n",
       "      <td>0.762667</td>\n",
       "      <td>0.466621</td>\n",
       "      <td>0.510927</td>\n",
       "      <td>0.487770</td>\n",
       "      <td>2.266786e+07</td>\n",
       "      <td>3777.976039</td>\n",
       "      <td>0.757833</td>\n",
       "      <td>0.242167</td>\n",
       "      <td>678</td>\n",
       "      <td>775</td>\n",
       "      <td>3898</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_name      dataset_name  seed  threshold   roc_auc  accuracy  \\\n",
       "0  Logistic Regression  Test (seed=2025)  2025   0.525875  0.722673  0.735667   \n",
       "1  Logistic Regression     Test (seed=0)     0   0.544998  0.714441  0.761333   \n",
       "2  Logistic Regression  Test (seed=1033)  1033   0.549389  0.713097  0.762667   \n",
       "\n",
       "   precision    recall        f1  utility_total  utility_per_case  \\\n",
       "0   0.428963  0.589299  0.496508   2.500604e+07       4167.674025   \n",
       "1   0.465931  0.541070  0.500697   2.447127e+07       4078.545072   \n",
       "2   0.466621  0.510927  0.487770   2.266786e+07       3777.976039   \n",
       "\n",
       "   acceptance_rate  rejection_rate   tp    fp    tn   fn  \n",
       "0         0.696167        0.303833  782  1041  3632  545  \n",
       "1         0.743167        0.256833  718   823  3850  609  \n",
       "2         0.757833        0.242167  678   775  3898  649  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run logistic regression for every seed and collect the summaries.\n",
    "logreg_runs = [run_logreg_for_seed(s) for s in SEED_PLAN]\n",
    "\n",
    "# Combine per-seed results into one table.\n",
    "logreg_summary_df = pd.concat([r[\"summary\"] for r in logreg_runs], ignore_index=True)\n",
    "logreg_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b80ca5",
   "metadata": {},
   "source": [
    "### 7.2 Decision Tree\n",
    "Non-linear model that splits the data into rules. We search over tree depth and minimum samples per leaf with cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf54bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision Tree helper with cross-validation on depth and min_samples_leaf.\n",
    "def run_dt_for_seed(\n",
    "    split_seed,\n",
    "    max_depth_grid=(None, 6, 10, 14),\n",
    "    min_samples_leaf_grid=(5, 10, 20),\n",
    "    threshold_grid=None,\n",
    "    cv_folds=CV_FOLDS,\n",
    "):\n",
    "    # Pull training data for this seed.\n",
    "    split = split_store[split_seed]\n",
    "    X_train = split[\"X_train_sub\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=split_seed)\n",
    "    cv_rows = []\n",
    "    best_mean = -float(\"inf\")\n",
    "    best_params = None\n",
    "\n",
    "    for depth in max_depth_grid:\n",
    "        for min_leaf in min_samples_leaf_grid:\n",
    "            model = DecisionTreeClassifier(\n",
    "                random_state=split_seed,\n",
    "                max_depth=depth,\n",
    "                min_samples_leaf=min_leaf,\n",
    "                class_weight=\"balanced\",\n",
    "            )\n",
    "            scores = cross_val_score(\n",
    "                model,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                cv=cv,\n",
    "                scoring=utility_scorer,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            cv_mean = float(scores.mean())\n",
    "            cv_std = float(scores.std())\n",
    "            cv_rows.append(\n",
    "                {\n",
    "                    \"max_depth\": depth,\n",
    "                    \"min_samples_leaf\": min_leaf,\n",
    "                    \"cv_mean_utility\": cv_mean,\n",
    "                    \"cv_std_utility\": cv_std,\n",
    "                }\n",
    "            )\n",
    "            # Ignore NaN scores so we do not leave best_params unset.\n",
    "            if not np.isnan(cv_mean) and cv_mean > best_mean:\n",
    "                best_mean = cv_mean\n",
    "                best_params = {\"max_depth\": depth, \"min_samples_leaf\": min_leaf}\n",
    "\n",
    "    # Fallback in case all cv_mean values were NaN.\n",
    "    if best_params is None:\n",
    "        best_params = {\n",
    "            \"max_depth\": max_depth_grid[0],\n",
    "            \"min_samples_leaf\": min_samples_leaf_grid[0],\n",
    "        }\n",
    "        best_mean = np.nan\n",
    "\n",
    "    best_model = DecisionTreeClassifier(\n",
    "        random_state=split_seed,\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "        class_weight=\"balanced\",\n",
    "    )\n",
    "    result = run_model_for_seed(\n",
    "        best_model,\n",
    "        model_name=\"Decision Tree\",\n",
    "        split_seed=split_seed,\n",
    "        threshold_grid=threshold_grid,\n",
    "        refit_on_full_train=True,\n",
    "    )\n",
    "    result[\"cv_results\"] = pd.DataFrame(cv_rows)\n",
    "    result[\"cv_best_params\"] = {**best_params, \"cv_mean_utility\": best_mean}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df285ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>seed</th>\n",
       "      <th>threshold</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>utility_total</th>\n",
       "      <th>utility_per_case</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>rejection_rate</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Test (seed=2025)</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.468109</td>\n",
       "      <td>0.677834</td>\n",
       "      <td>0.679333</td>\n",
       "      <td>0.360839</td>\n",
       "      <td>0.583271</td>\n",
       "      <td>0.445853</td>\n",
       "      <td>1.899172e+07</td>\n",
       "      <td>3165.286866</td>\n",
       "      <td>0.6425</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>774</td>\n",
       "      <td>1371</td>\n",
       "      <td>3302</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Test (seed=0)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468109</td>\n",
       "      <td>0.661388</td>\n",
       "      <td>0.672167</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.554635</td>\n",
       "      <td>0.428031</td>\n",
       "      <td>1.643609e+07</td>\n",
       "      <td>2739.348665</td>\n",
       "      <td>0.6480</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>736</td>\n",
       "      <td>1376</td>\n",
       "      <td>3297</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Test (seed=1033)</td>\n",
       "      <td>1033</td>\n",
       "      <td>0.468109</td>\n",
       "      <td>0.647238</td>\n",
       "      <td>0.661167</td>\n",
       "      <td>0.333804</td>\n",
       "      <td>0.534288</td>\n",
       "      <td>0.410895</td>\n",
       "      <td>1.403012e+07</td>\n",
       "      <td>2338.354092</td>\n",
       "      <td>0.6460</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>709</td>\n",
       "      <td>1415</td>\n",
       "      <td>3258</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_name      dataset_name  seed  threshold   roc_auc  accuracy  \\\n",
       "0  Decision Tree  Test (seed=2025)  2025   0.468109  0.677834  0.679333   \n",
       "1  Decision Tree     Test (seed=0)     0   0.468109  0.661388  0.672167   \n",
       "2  Decision Tree  Test (seed=1033)  1033   0.468109  0.647238  0.661167   \n",
       "\n",
       "   precision    recall        f1  utility_total  utility_per_case  \\\n",
       "0   0.360839  0.583271  0.445853   1.899172e+07       3165.286866   \n",
       "1   0.348485  0.554635  0.428031   1.643609e+07       2739.348665   \n",
       "2   0.333804  0.534288  0.410895   1.403012e+07       2338.354092   \n",
       "\n",
       "   acceptance_rate  rejection_rate   tp    fp    tn   fn  \n",
       "0           0.6425          0.3575  774  1371  3302  553  \n",
       "1           0.6480          0.3520  736  1376  3297  591  \n",
       "2           0.6460          0.3540  709  1415  3258  618  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run decision trees for every seed and collect the summaries.\n",
    "dt_runs = [run_dt_for_seed(s) for s in SEED_PLAN]\n",
    "\n",
    "dt_summary_df = pd.concat([r[\"summary\"] for r in dt_runs], ignore_index=True)\n",
    "dt_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7af63",
   "metadata": {},
   "source": [
    "### 7.3 SVM (RBF)\n",
    "Support Vector Machine with a radial basis function kernel. We tune `C` and `gamma` to balance margin width and flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2808357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVM (RBF) helper with cross-validation on C and gamma.\n",
    "def run_svm_for_seed(\n",
    "    split_seed,\n",
    "    C_grid=(0.5, 1.0, 2.0, 5.0),\n",
    "    gamma_grid=(\"scale\", \"auto\"),\n",
    "    threshold_grid=None,\n",
    "    cv_folds=CV_FOLDS,\n",
    "):\n",
    "    # Pull training data for this seed.\n",
    "    split = split_store[split_seed]\n",
    "    X_train = split[\"X_train_sub\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=split_seed)\n",
    "    cv_rows = []\n",
    "    best_mean = -float(\"inf\")\n",
    "    best_params = None\n",
    "\n",
    "    for C in C_grid:\n",
    "        for gamma in gamma_grid:\n",
    "            model = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"clf\", SVC(\n",
    "                    kernel=\"rbf\",\n",
    "                    C=C,\n",
    "                    gamma=gamma,\n",
    "                    probability=False,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=split_seed,\n",
    "                )),\n",
    "            ])\n",
    "            scores = cross_val_score(\n",
    "                model,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                cv=cv,\n",
    "                scoring=utility_scorer,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            cv_mean = float(scores.mean())\n",
    "            cv_std = float(scores.std())\n",
    "            cv_rows.append(\n",
    "                {\n",
    "                    \"C\": C,\n",
    "                    \"gamma\": gamma,\n",
    "                    \"cv_mean_utility\": cv_mean,\n",
    "                    \"cv_std_utility\": cv_std,\n",
    "                }\n",
    "            )\n",
    "            # Ignore NaN scores so best_params is always set.\n",
    "            if not np.isnan(cv_mean) and cv_mean > best_mean:\n",
    "                best_mean = cv_mean\n",
    "                best_params = {\"C\": C, \"gamma\": gamma}\n",
    "\n",
    "    # Fallback in case all cv_mean values were NaN.\n",
    "    if best_params is None:\n",
    "        best_params = {\"C\": C_grid[0], \"gamma\": gamma_grid[0]}\n",
    "        best_mean = np.nan\n",
    "\n",
    "    best_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(\n",
    "            kernel=\"rbf\",\n",
    "            C=best_params[\"C\"],\n",
    "            gamma=best_params[\"gamma\"],\n",
    "            probability=False,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=split_seed,\n",
    "        )),\n",
    "    ])\n",
    "    result = run_model_for_seed(\n",
    "        best_model,\n",
    "        model_name=\"SVM (RBF)\",\n",
    "        split_seed=split_seed,\n",
    "        threshold_grid=threshold_grid,\n",
    "        refit_on_full_train=True,\n",
    "    )\n",
    "    result[\"cv_results\"] = pd.DataFrame(cv_rows)\n",
    "    result[\"cv_best_params\"] = {**best_params, \"cv_mean_utility\": best_mean}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb915ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>seed</th>\n",
       "      <th>threshold</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>utility_total</th>\n",
       "      <th>utility_per_case</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>rejection_rate</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM (RBF)</td>\n",
       "      <td>Test (seed=2025)</td>\n",
       "      <td>2025</td>\n",
       "      <td>-0.362265</td>\n",
       "      <td>0.756387</td>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.440907</td>\n",
       "      <td>0.615674</td>\n",
       "      <td>0.513836</td>\n",
       "      <td>2.736648e+07</td>\n",
       "      <td>4561.080786</td>\n",
       "      <td>0.691167</td>\n",
       "      <td>0.308833</td>\n",
       "      <td>817</td>\n",
       "      <td>1036</td>\n",
       "      <td>3637</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM (RBF)</td>\n",
       "      <td>Test (seed=0)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037092</td>\n",
       "      <td>0.758524</td>\n",
       "      <td>0.776833</td>\n",
       "      <td>0.495924</td>\n",
       "      <td>0.550113</td>\n",
       "      <td>0.521615</td>\n",
       "      <td>2.660051e+07</td>\n",
       "      <td>4433.418014</td>\n",
       "      <td>0.754667</td>\n",
       "      <td>0.245333</td>\n",
       "      <td>730</td>\n",
       "      <td>742</td>\n",
       "      <td>3931</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (RBF)</td>\n",
       "      <td>Test (seed=1033)</td>\n",
       "      <td>1033</td>\n",
       "      <td>-0.558620</td>\n",
       "      <td>0.756318</td>\n",
       "      <td>0.726167</td>\n",
       "      <td>0.420683</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>0.504971</td>\n",
       "      <td>2.676835e+07</td>\n",
       "      <td>4461.391403</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>838</td>\n",
       "      <td>1154</td>\n",
       "      <td>3519</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name      dataset_name  seed  threshold   roc_auc  accuracy  \\\n",
       "0  SVM (RBF)  Test (seed=2025)  2025  -0.362265  0.756387  0.742333   \n",
       "1  SVM (RBF)     Test (seed=0)     0   0.037092  0.758524  0.776833   \n",
       "2  SVM (RBF)  Test (seed=1033)  1033  -0.558620  0.756318  0.726167   \n",
       "\n",
       "   precision    recall        f1  utility_total  utility_per_case  \\\n",
       "0   0.440907  0.615674  0.513836   2.736648e+07       4561.080786   \n",
       "1   0.495924  0.550113  0.521615   2.660051e+07       4433.418014   \n",
       "2   0.420683  0.631500  0.504971   2.676835e+07       4461.391403   \n",
       "\n",
       "   acceptance_rate  rejection_rate   tp    fp    tn   fn  \n",
       "0         0.691167        0.308833  817  1036  3637  510  \n",
       "1         0.754667        0.245333  730   742  3931  597  \n",
       "2         0.668000        0.332000  838  1154  3519  489  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run SVMs for every seed and collect the summaries.\n",
    "svm_runs = [run_svm_for_seed(s) for s in SEED_PLAN]\n",
    "\n",
    "svm_summary_df = pd.concat([r[\"summary\"] for r in svm_runs], ignore_index=True)\n",
    "svm_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5430b6",
   "metadata": {},
   "source": [
    "### 7.4 Naive Bayes (Gaussian)\n",
    "Probabilistic model that assumes each feature is normally distributed and independent. We tune the smoothing value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ce1a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Naive Bayes helper with cross-validation on var_smoothing.\n",
    "def run_nb_for_seed(\n",
    "    split_seed,\n",
    "    var_smoothing_grid=(1e-9, 1e-8, 1e-7, 1e-6),\n",
    "    threshold_grid=None,\n",
    "    cv_folds=CV_FOLDS,\n",
    "):\n",
    "    # Pull training data for this seed.\n",
    "    split = split_store[split_seed]\n",
    "    X_train = split[\"X_train_sub\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=split_seed)\n",
    "    cv_rows = []\n",
    "    best_mean = -float(\"inf\")\n",
    "    best_vs = None\n",
    "\n",
    "    for vs in var_smoothing_grid:\n",
    "        model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", GaussianNB(var_smoothing=vs)),\n",
    "        ])\n",
    "        scores = cross_val_score(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=utility_scorer,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        cv_mean = float(scores.mean())\n",
    "        cv_std = float(scores.std())\n",
    "        cv_rows.append({\"var_smoothing\": vs, \"cv_mean_utility\": cv_mean, \"cv_std_utility\": cv_std})\n",
    "        # Skip NaN so best_vs is always set.\n",
    "        if not np.isnan(cv_mean) and cv_mean > best_mean:\n",
    "            best_mean = cv_mean\n",
    "            best_vs = vs\n",
    "\n",
    "    # Fallback in case all cv_mean values were NaN.\n",
    "    if best_vs is None:\n",
    "        best_vs = var_smoothing_grid[0]\n",
    "        best_mean = np.nan\n",
    "\n",
    "    best_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", GaussianNB(var_smoothing=best_vs)),\n",
    "    ])\n",
    "    result = run_model_for_seed(\n",
    "        best_model,\n",
    "        model_name=\"Naive Bayes\",\n",
    "        split_seed=split_seed,\n",
    "        threshold_grid=threshold_grid,\n",
    "        refit_on_full_train=True,\n",
    "    )\n",
    "    result[\"cv_results\"] = pd.DataFrame(cv_rows)\n",
    "    result[\"cv_best_params\"] = {\"var_smoothing\": best_vs, \"cv_mean_utility\": best_mean}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4ec6f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>seed</th>\n",
       "      <th>threshold</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>utility_total</th>\n",
       "      <th>utility_per_case</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>rejection_rate</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Test (seed=2025)</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.671414</td>\n",
       "      <td>0.733645</td>\n",
       "      <td>0.763667</td>\n",
       "      <td>0.471473</td>\n",
       "      <td>0.566692</td>\n",
       "      <td>0.514716</td>\n",
       "      <td>2.635045e+07</td>\n",
       "      <td>4391.741606</td>\n",
       "      <td>0.734167</td>\n",
       "      <td>0.265833</td>\n",
       "      <td>752</td>\n",
       "      <td>843</td>\n",
       "      <td>3830</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Test (seed=0)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606108</td>\n",
       "      <td>0.715027</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.350975</td>\n",
       "      <td>0.664657</td>\n",
       "      <td>0.459375</td>\n",
       "      <td>2.169005e+07</td>\n",
       "      <td>3615.008046</td>\n",
       "      <td>0.581167</td>\n",
       "      <td>0.418833</td>\n",
       "      <td>882</td>\n",
       "      <td>1631</td>\n",
       "      <td>3042</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Test (seed=1033)</td>\n",
       "      <td>1033</td>\n",
       "      <td>0.659010</td>\n",
       "      <td>0.735828</td>\n",
       "      <td>0.764667</td>\n",
       "      <td>0.473354</td>\n",
       "      <td>0.568953</td>\n",
       "      <td>0.516769</td>\n",
       "      <td>2.659558e+07</td>\n",
       "      <td>4432.597016</td>\n",
       "      <td>0.734167</td>\n",
       "      <td>0.265833</td>\n",
       "      <td>755</td>\n",
       "      <td>840</td>\n",
       "      <td>3833</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name      dataset_name  seed  threshold   roc_auc  accuracy  \\\n",
       "0  Naive Bayes  Test (seed=2025)  2025   0.671414  0.733645  0.763667   \n",
       "1  Naive Bayes     Test (seed=0)     0   0.606108  0.715027  0.654000   \n",
       "2  Naive Bayes  Test (seed=1033)  1033   0.659010  0.735828  0.764667   \n",
       "\n",
       "   precision    recall        f1  utility_total  utility_per_case  \\\n",
       "0   0.471473  0.566692  0.514716   2.635045e+07       4391.741606   \n",
       "1   0.350975  0.664657  0.459375   2.169005e+07       3615.008046   \n",
       "2   0.473354  0.568953  0.516769   2.659558e+07       4432.597016   \n",
       "\n",
       "   acceptance_rate  rejection_rate   tp    fp    tn   fn  \n",
       "0         0.734167        0.265833  752   843  3830  575  \n",
       "1         0.581167        0.418833  882  1631  3042  445  \n",
       "2         0.734167        0.265833  755   840  3833  572  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Naive Bayes for every seed and collect the summaries.\n",
    "nb_runs = [run_nb_for_seed(s) for s in SEED_PLAN]\n",
    "\n",
    "nb_summary_df = pd.concat([r[\"summary\"] for r in nb_runs], ignore_index=True)\n",
    "nb_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304eff37",
   "metadata": {},
   "source": [
    "### 7.5 k-NN\n",
    "Instance-based classifier that looks at the closest neighbors. We try different numbers of neighbors and keep the one with the best utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9047fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN-specific helper to train and evaluate per seed with CV-selected k.\n",
    "def run_knn_for_seed(\n",
    "    split_seed,\n",
    "    k_grid=(5, 10, 15, 20, 25, 30, 35, 40, 45, 50),\n",
    "    threshold_grid=None,\n",
    "    cv_folds=CV_FOLDS,\n",
    "):\n",
    "    # Pull training data for this seed.\n",
    "    split = split_store[split_seed]\n",
    "    X_train = split[\"X_train_sub\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=split_seed)\n",
    "    cv_rows = []\n",
    "    best_mean = -float(\"inf\")\n",
    "    best_k = None\n",
    "\n",
    "    for k in k_grid:\n",
    "        model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", KNeighborsClassifier(n_neighbors=k, weights=\"distance\")),\n",
    "        ])\n",
    "        scores = cross_val_score(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=utility_scorer,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        cv_mean = float(scores.mean())\n",
    "        cv_std = float(scores.std())\n",
    "        cv_rows.append({\"k\": k, \"cv_mean_utility\": cv_mean, \"cv_std_utility\": cv_std})\n",
    "        if not np.isnan(cv_mean) and (best_k is None or cv_mean > best_mean):\n",
    "            best_mean = cv_mean\n",
    "            best_k = k\n",
    "\n",
    "    # Fallback in case cross-val produced NaN for every k.\n",
    "    if best_k is None:\n",
    "        best_k = k_grid[0]\n",
    "        best_mean = np.nan\n",
    "\n",
    "    best_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=best_k, weights=\"distance\")),\n",
    "    ])\n",
    "    result = run_model_for_seed(\n",
    "        best_model,\n",
    "        model_name=\"k-NN\",\n",
    "        split_seed=split_seed,\n",
    "        threshold_grid=threshold_grid,\n",
    "        refit_on_full_train=True,\n",
    "    )\n",
    "    result[\"cv_results\"] = pd.DataFrame(cv_rows)\n",
    "    result[\"cv_best_params\"] = {\"k\": best_k, \"cv_mean_utility\": best_mean}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf7e7f9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run k-NN for every seed and collect the summaries.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m knn_runs = [\u001b[43mrun_knn_for_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m SEED_PLAN]\n\u001b[32m      4\u001b[39m knn_summary_df = pd.concat([r[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m knn_runs], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m knn_summary_df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mrun_knn_for_seed\u001b[39m\u001b[34m(split_seed, k_grid, threshold_grid, cv_folds)\u001b[39m\n\u001b[32m     36\u001b[39m         best_k = k\n\u001b[32m     38\u001b[39m best_model = Pipeline([\n\u001b[32m     39\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m\"\u001b[39m, StandardScaler()),\n\u001b[32m     40\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m\"\u001b[39m, KNeighborsClassifier(n_neighbors=best_k, weights=\u001b[33m\"\u001b[39m\u001b[33mdistance\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     41\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m result = \u001b[43mrun_model_for_seed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mk-NN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefit_on_full_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m result[\u001b[33m\"\u001b[39m\u001b[33mcv_results\u001b[39m\u001b[33m\"\u001b[39m] = pd.DataFrame(cv_rows)\n\u001b[32m     50\u001b[39m result[\u001b[33m\"\u001b[39m\u001b[33mcv_best_params\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: best_k, \u001b[33m\"\u001b[39m\u001b[33mcv_mean_utility\u001b[39m\u001b[33m\"\u001b[39m: best_mean}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mrun_model_for_seed\u001b[39m\u001b[34m(model, model_name, split_seed, threshold_grid, refit_on_full_train)\u001b[39m\n\u001b[32m    142\u001b[39m working_model.fit(X_train, y_train)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Get scores on the validation set and find the best threshold.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m val_scores = \u001b[43mscore_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m sweep = sweep_thresholds(y_val, val_scores, _utility, grid=threshold_grid)\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Optionally refit on the full training data before testing.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mscore_predictions\u001b[39m\u001b[34m(model, features)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore_predictions\u001b[39m(model, features):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Return a continuous score for each row.\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m]\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mdecision_function\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m model.decision_function(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\betoq\\OneDrive\\Documentos\\GitHub\\exchange-data-concepts\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:905\u001b[39m, in \u001b[36mPipeline.predict_proba\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    904\u001b[39m         Xt = transform.transform(Xt)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n\u001b[32m    908\u001b[39m routed_params = process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\betoq\\OneDrive\\Documentos\\GitHub\\exchange-data-concepts\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:374\u001b[39m, in \u001b[36mKNeighborsClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    372\u001b[39m     neigh_dist = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     neigh_dist, neigh_ind = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m classes_ = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m    377\u001b[39m _y = \u001b[38;5;28mself\u001b[39m._y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\betoq\\OneDrive\\Documentos\\GitHub\\exchange-data-concepts\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:848\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    838\u001b[39m         X = validate_data(\n\u001b[32m    839\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    840\u001b[39m             X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    844\u001b[39m             order=\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    845\u001b[39m         )\n\u001b[32m    847\u001b[39m n_samples_fit = \u001b[38;5;28mself\u001b[39m.n_samples_fit_\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples_fit\u001b[49m:\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_is_train:\n\u001b[32m    850\u001b[39m         n_neighbors -= \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# ok to modify inplace because an error is raised\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# Run k-NN for every seed and collect the summaries.\n",
    "knn_runs = [run_knn_for_seed(s, k_grid=(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)) for s in SEED_PLAN]\n",
    "\n",
    "knn_summary_df = pd.concat([r[\"summary\"] for r in knn_runs], ignore_index=True)\n",
    "knn_summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a10d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle model runs and summaries in dictionaries for easy lookups later.\n",
    "model_runs = {\n",
    "    \"Logistic Regression\": logreg_runs,\n",
    "    \"Decision Tree\": dt_runs,\n",
    "    \"SVM (RBF)\": svm_runs,\n",
    "    \"Naive Bayes\": nb_runs,\n",
    "    \"k-NN\": knn_runs,\n",
    "}\n",
    "\n",
    "model_summary_frames = {\n",
    "    \"Logistic Regression\": logreg_summary_df,\n",
    "    \"Decision Tree\": dt_summary_df,\n",
    "    \"SVM (RBF)\": svm_summary_df,\n",
    "    \"Naive Bayes\": nb_summary_df,\n",
    "    \"k-NN\": knn_summary_df,\n",
    "}\n",
    "\n",
    "# Stack all seed-level summaries into one DataFrame.\n",
    "combined_model_summaries = pd.concat(model_summary_frames.values(), ignore_index=True)\n",
    "combined_model_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae023d",
   "metadata": {},
   "source": [
    "## 8. Summary table (all seeds)\n",
    "A long table with one row per model per seed plus three simple baselines (random guesses, approve all, reject all). Use this to see how consistent each model is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_true = baseline_split[\"y_test\"].values\n",
    "\n",
    "# Helper to reuse summarize_model for baselines that do not produce scores.\n",
    "def baseline_summary(name, y_pred):\n",
    "    return summarize_model(\n",
    "        model_name=name,\n",
    "        dataset_name=\"Test (baseline seed)\",\n",
    "        y_true=baseline_true,\n",
    "        y_pred=y_pred,\n",
    "        y_score=None,\n",
    "        threshold=np.nan,\n",
    "        seed=np.nan,\n",
    "    )\n",
    "\n",
    "# Create three simple baselines: random guesses, approve everyone, reject everyone.\n",
    "rng = np.random.default_rng(0)\n",
    "baseline_random = baseline_summary(\"Baseline - Random\", rng.integers(0, 2, size=len(baseline_true)))\n",
    "baseline_approve = baseline_summary(\"Baseline - Approve all\", np.zeros_like(baseline_true))\n",
    "baseline_reject = baseline_summary(\"Baseline - Reject all\", np.ones_like(baseline_true))\n",
    "\n",
    "baseline_df = pd.concat([baseline_random, baseline_approve, baseline_reject], ignore_index=True)\n",
    "\n",
    "# Combine baselines and model results into one big table.\n",
    "summary_table = pd.concat([baseline_df, combined_model_summaries], ignore_index=True)\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c99d89",
   "metadata": {},
   "source": [
    "## 9. Model comparison summary (baselines + averages)\n",
    "We average metrics across seeds for every model and line them up next to the baselines so you can spot the overall winner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a29b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average metrics across seeds for an easy side-by-side comparison.\n",
    "def average_rows(df, label):\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    numeric_cols_no_seed = [c for c in numeric_cols if c != \"seed\"]\n",
    "    avg_row = df[numeric_cols_no_seed].mean().to_frame().T\n",
    "    avg_row[\"model_name\"] = f\"{label} (avg seeds)\"\n",
    "    avg_row[\"dataset_name\"] = \"Test (avg seeds)\"\n",
    "    cols = [c for c in df.columns if c != \"seed\"]\n",
    "    return avg_row.reindex(columns=cols)\n",
    "\n",
    "avg_rows = [\n",
    "    average_rows(logreg_summary_df, \"Logistic Regression\"),\n",
    "    average_rows(dt_summary_df, \"Decision Tree\"),\n",
    "    average_rows(svm_summary_df, \"SVM (RBF)\"),\n",
    "    average_rows(nb_summary_df, \"Naive Bayes\"),\n",
    "    average_rows(knn_summary_df, \"k-NN\"),\n",
    "]\n",
    "\n",
    "comparison_df = pd.concat([baseline_df] + avg_rows, ignore_index=True)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b16f19",
   "metadata": {},
   "source": [
    "## 10. Multi-model visualizations\n",
    "Bar chart of utility per case by seed, ROC curves for the baseline seed, and a confusion matrix for the top average-utility model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58abebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility per case for each model and seed.\n",
    "plot_data = combined_model_summaries.copy()\n",
    "plot_data[\"seed\"] = plot_data[\"seed\"].astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(data=plot_data, x=\"seed\", y=\"utility_per_case\", hue=\"model_name\", ax=ax)\n",
    "ax.set_title(\"Utility per case by model and seed\")\n",
    "ax.set_xlabel(\"Seed\")\n",
    "ax.set_ylabel(\"Utility per case\")\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for each model using the baseline seed only.\n",
    "baseline_y_true = baseline_split[\"y_test\"].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for name, runs in model_runs.items():\n",
    "    baseline_run = next(r for r in runs if r[\"seed\"] == BASELINE_SEED)\n",
    "    fpr, tpr, _ = roc_curve(baseline_y_true, baseline_run[\"test_scores\"])\n",
    "    auc_val = baseline_run[\"summary\"][\"roc_auc\"].iloc[0]\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "ax.set_title(\"ROC curves (baseline seed)\")\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdae13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the top model (by average utility) on the baseline seed.\n",
    "avg_util = combined_model_summaries.groupby(\"model_name\")[\"utility_per_case\"].mean().sort_values(ascending=False)\n",
    "top_model_name = avg_util.index[0]\n",
    "top_run = next(r for r in model_runs[top_model_name] if r[\"seed\"] == BASELINE_SEED)\n",
    "\n",
    "cm = confusion_matrix(baseline_y_true, top_run[\"test_pred\"], labels=[0, 1])\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    ax=ax,\n",
    "    xticklabels=[\"Approve\", \"Reject\"],\n",
    "    yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    ")\n",
    "ax.set_title(f\"Confusion matrix (baseline seed, {top_model_name})\")\n",
    "ax.set_xlabel(\"Prediction\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
