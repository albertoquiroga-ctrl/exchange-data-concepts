{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426c05a3",
   "metadata": {},
   "source": [
    "# Credit Default Modeling (Utility-Focused, Multi-Seed)\n",
    "Utility-based comparison of Logistic Regression, Decision Tree, SVM (RBF), Gaussian Naive Bayes, and k-NN on the UCI credit card default data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea55be",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Load analysis, modeling, and plotting libraries. `xlrd` is pulled in for the Excel source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08110cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q xlrd seaborn scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301e508",
   "metadata": {},
   "source": [
    "## 2. Load the data\n",
    "Read the Excel file, rename the target column to `TARGET`, and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = Path(\"default of credit card clients.xls\")\n",
    "if not DATA_FILE.exists():\n",
    "    DATA_FILE = Path(\"Code\") / \"Final Project\" / \"default of credit card clients.xls\"\n",
    "\n",
    "if not DATA_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Could not find data file at {DATA_FILE}\")\n",
    "\n",
    "raw_df = (\n",
    "    pd.read_excel(DATA_FILE, header=1)\n",
    "    .rename(columns={\"default payment next month\": \"TARGET\"})\n",
    ")\n",
    "\n",
    "print(f\"Loaded {raw_df.shape[0]:,} rows and {raw_df.shape[1]} columns.\")\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35deab",
   "metadata": {},
   "source": [
    "## 3. Split features and target\n",
    "Separate predictors (`X`) from the binary target (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_df.drop(columns=\"TARGET\")\n",
    "y = raw_df[\"TARGET\"].astype(int)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target breakdown: {y.value_counts().to_dict()}\")\n",
    "display(X.head(), y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979aa98",
   "metadata": {},
   "source": [
    "## 4. Reusable splits and scalers (shared across seeds)\n",
    "Build consistent train/validation/test splits for each seed and fit a scaler per seed. All models reuse these splits to stay comparable across seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_PLAN = [2025, 0, 1033]\n",
    "VAL_SEED = 0\n",
    "BASELINE_SEED = SEED_PLAN[0]\n",
    "\n",
    "\n",
    "def build_split_bundle(split_seed, val_seed=VAL_SEED):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=split_seed,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=0.25,\n",
    "        random_state=val_seed,\n",
    "        stratify=y_train,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_sub)\n",
    "\n",
    "    return {\n",
    "        \"seed\": split_seed,\n",
    "        \"X_train_sub\": X_train_sub,\n",
    "        \"X_val\": X_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train_sub\": y_train_sub,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test,\n",
    "        \"scaler\": scaler,\n",
    "        \"X_train_sub_scaled\": scaler.transform(X_train_sub),\n",
    "        \"X_val_scaled\": scaler.transform(X_val),\n",
    "        \"X_test_scaled\": scaler.transform(X_test),\n",
    "    }\n",
    "\n",
    "\n",
    "split_store = {s: build_split_bundle(s, val_seed=VAL_SEED) for s in SEED_PLAN}\n",
    "baseline_split = split_store[BASELINE_SEED]\n",
    "\n",
    "display(\n",
    "    {\n",
    "        \"seed\": BASELINE_SEED,\n",
    "        \"train_sub\": baseline_split[\"X_train_sub\"].shape,\n",
    "        \"val\": baseline_split[\"X_val\"].shape,\n",
    "        \"test\": baseline_split[\"X_test\"].shape,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a4d3e",
   "metadata": {},
   "source": [
    "## 5. Business assumptions and cost matrix\n",
    "Derive monetary gains/losses from the PDF assumptions to evaluate models on profit rather than accuracy alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd20ff",
   "metadata": {},
   "source": [
    "### 5.1 Observation window and APR assumptions\n",
    "Compute profit from approving a good customer and loss from approving a bad customer over a six-month window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_default = raw_df[raw_df[\"TARGET\"] == 1]   # will default\n",
    "df_no_default = raw_df[raw_df[\"TARGET\"] == 0]  # will not default\n",
    "\n",
    "mean_limit_default = df_default[\"LIMIT_BAL\"].mean()\n",
    "mean_limit_no_default = df_no_default[\"LIMIT_BAL\"].mean()\n",
    "\n",
    "assumption_config = {\n",
    "    \"annual_apr\": 0.18,           # 18% annual percentage rate from the data dictionary PDF\n",
    "    \"periods_per_year\": 12,       # monthly compounding\n",
    "    \"observation_months\": 6,      # six billing cycles (Apr-Sep 2005 in the PDF)\n",
    "    \"loss_given_default\": 0.5,    # lose 50% of the limit if they default\n",
    "}\n",
    "\n",
    "annual_apr = assumption_config[\"annual_apr\"]\n",
    "periods_per_year = assumption_config[\"periods_per_year\"]\n",
    "observation_months = assumption_config[\"observation_months\"]\n",
    "loss_given_default = assumption_config[\"loss_given_default\"]\n",
    "\n",
    "periodic_rate = annual_apr / periods_per_year\n",
    "period_length_months = 12 / periods_per_year\n",
    "periods_in_window = observation_months / period_length_months\n",
    "\n",
    "profit_good = mean_limit_no_default * ((1 + periodic_rate) ** periods_in_window - 1)\n",
    "loss_bad = mean_limit_default * loss_given_default\n",
    "\n",
    "print(f\"Periodic rate: {periodic_rate:.2%}\")\n",
    "print(f\"Approx profit per good customer: {profit_good:,.2f}\")\n",
    "print(f\"Approx loss per bad customer:    {loss_bad:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8923e7c",
   "metadata": {},
   "source": [
    "### 5.2 Cost/benefit matrix\n",
    "Map actual/predicted outcomes to dollar values for the utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24058a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_TN = profit_good    # Actual 0, Pred 0: good customer, approved -> earn interest\n",
    "value_FP = 0.0              # Actual 0, Pred 1: good customer, rejected -> forego profit\n",
    "value_FN = -loss_bad        # Actual 1, Pred 0: bad customer, approved -> lose money\n",
    "value_TP = 0.0              # Actual 1, Pred 1: bad customer, rejected -> avoided loss\n",
    "\n",
    "value_matrix = pd.DataFrame(\n",
    "    {\n",
    "        0: {0: value_TN, 1: value_FN},  # Predicted 0\n",
    "        1: {0: value_FP, 1: value_TP},  # Predicted 1\n",
    "    }\n",
    ")\n",
    "value_matrix.index.name = \"Actual\"\n",
    "value_matrix.columns.name = \"Predicted\"\n",
    "value_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf8832",
   "metadata": {},
   "source": [
    "## 6. Utility and evaluation helpers\n",
    "Shared helpers for threshold sweeps, utility calculation, and concise metric summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c197f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH_QUANTILES = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "def score_predictions(model, features):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(features)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(features)\n",
    "    return model.predict(features)\n",
    "\n",
    "def utility_from_predictions(y_true, y_pred, values=value_matrix):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    utility_total = (\n",
    "        tn * values.loc[0, 0]\n",
    "        + fp * values.loc[0, 1]\n",
    "        + fn * values.loc[1, 0]\n",
    "        + tp * values.loc[1, 1]\n",
    "    )\n",
    "    utility_per_case = utility_total / len(y_true)\n",
    "    return utility_total, utility_per_case, {\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp}\n",
    "\n",
    "def _utility(y_true, y_pred):\n",
    "    return utility_from_predictions(y_true, y_pred, value_matrix)\n",
    "\n",
    "def sweep_thresholds(y_true, scores, utility_fn, grid=None):\n",
    "    grid = np.unique(np.quantile(scores, THRESH_QUANTILES)) if grid is None else grid\n",
    "    records = []\n",
    "    for t in grid:\n",
    "        preds = (scores >= t).astype(int)\n",
    "        utility_total, utility_pc, _ = utility_fn(y_true, preds)\n",
    "        records.append(\n",
    "            {\n",
    "                \"threshold\": float(t),\n",
    "                \"utility\": utility_total,\n",
    "                \"utility_per_case\": utility_pc,\n",
    "            }\n",
    "        )\n",
    "    best = max(records, key=lambda r: r[\"utility\"])\n",
    "    return {\n",
    "        \"threshold\": best[\"threshold\"],\n",
    "        \"val_utility\": best[\"utility\"],\n",
    "        \"val_utility_per_case\": best[\"utility_per_case\"],\n",
    "        \"sweep\": pd.DataFrame(records),\n",
    "    }\n",
    "\n",
    "def summarize_model(\n",
    "    model_name,\n",
    "    dataset_name,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_score,\n",
    "    threshold,\n",
    "    seed,\n",
    "    utility_fn=_utility,\n",
    "):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    utility_total, utility_pc, _ = utility_fn(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_score) if y_score is not None else np.nan\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"seed\": seed,\n",
    "                \"threshold\": threshold,\n",
    "                \"roc_auc\": roc_auc,\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "                \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "                \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "                \"utility_total\": utility_total,\n",
    "                \"utility_per_case\": utility_pc,\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"tn\": tn,\n",
    "                \"fn\": fn,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def run_model_for_seed(\n",
    "    model,\n",
    "    model_name,\n",
    "    split_seed,\n",
    "    use_scaled=True,\n",
    "    threshold_grid=None,\n",
    "):\n",
    "    split = split_store[split_seed]\n",
    "    suffix = \"_scaled\" if use_scaled else \"\"\n",
    "    X_train = split[f\"X_train_sub{suffix}\"]\n",
    "    X_val = split[f\"X_val{suffix}\"]\n",
    "    X_test = split[f\"X_test{suffix}\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "    y_val = split[\"y_val\"]\n",
    "    y_test = split[\"y_test\"]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    val_scores = score_predictions(model, X_val)\n",
    "    sweep = sweep_thresholds(y_val, val_scores, _utility, grid=threshold_grid)\n",
    "\n",
    "    test_scores = score_predictions(model, X_test)\n",
    "    test_pred = (test_scores >= sweep[\"threshold\"]).astype(int)\n",
    "\n",
    "    summary = summarize_model(\n",
    "        model_name=model_name,\n",
    "        dataset_name=f\"Test (seed={split_seed})\",\n",
    "        y_true=y_test,\n",
    "        y_pred=test_pred,\n",
    "        y_score=test_scores,\n",
    "        threshold=sweep[\"threshold\"],\n",
    "        seed=split_seed,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"seed\": split_seed,\n",
    "        \"model_name\": model_name,\n",
    "        \"model\": model,\n",
    "        \"threshold\": sweep[\"threshold\"],\n",
    "        \"val_sweep\": sweep[\"sweep\"],\n",
    "        \"val_utility\": sweep[\"val_utility\"],\n",
    "        \"summary\": summary,\n",
    "        \"test_scores\": test_scores,\n",
    "        \"test_pred\": test_pred,\n",
    "    }\n",
    "\n",
    "def run_knn_for_seed(split_seed, k_grid=(5, 15, 25, 35), threshold_grid=None):\n",
    "    split = split_store[split_seed]\n",
    "    X_train = split[\"X_train_sub_scaled\"]\n",
    "    X_val = split[\"X_val_scaled\"]\n",
    "    X_test = split[\"X_test_scaled\"]\n",
    "    y_train = split[\"y_train_sub\"]\n",
    "    y_val = split[\"y_val\"]\n",
    "    y_test = split[\"y_test\"]\n",
    "\n",
    "    best = None\n",
    "    for k in k_grid:\n",
    "        model = KNeighborsClassifier(n_neighbors=k, weights=\"distance\")\n",
    "        model.fit(X_train, y_train)\n",
    "        val_scores = score_predictions(model, X_val)\n",
    "        sweep = sweep_thresholds(y_val, val_scores, _utility, grid=threshold_grid)\n",
    "\n",
    "        if best is None or sweep[\"val_utility\"] > best[\"val_utility\"]:\n",
    "            best = {\n",
    "                \"k\": k,\n",
    "                \"model\": model,\n",
    "                \"sweep\": sweep,\n",
    "                \"val_utility\": sweep[\"val_utility\"],\n",
    "            }\n",
    "\n",
    "    best_model = best[\"model\"]\n",
    "    test_scores = score_predictions(best_model, X_test)\n",
    "    test_pred = (test_scores >= best[\"sweep\"][\"threshold\"]).astype(int)\n",
    "\n",
    "    summary = summarize_model(\n",
    "        model_name=\"k-NN\",\n",
    "        dataset_name=f\"Test (seed={split_seed})\",\n",
    "        y_true=y_test,\n",
    "        y_pred=test_pred,\n",
    "        y_score=test_scores,\n",
    "        threshold=best[\"sweep\"][\"threshold\"],\n",
    "        seed=split_seed,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"seed\": split_seed,\n",
    "        \"model_name\": \"k-NN\",\n",
    "        \"model\": best_model,\n",
    "        \"k\": best[\"k\"],\n",
    "        \"threshold\": best[\"sweep\"][\"threshold\"],\n",
    "        \"val_sweep\": best[\"sweep\"][\"sweep\"],\n",
    "        \"val_utility\": best[\"val_utility\"],\n",
    "        \"summary\": summary,\n",
    "        \"test_scores\": test_scores,\n",
    "        \"test_pred\": test_pred,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3a2cf",
   "metadata": {},
   "source": [
    "## 7. Models trained across seeds\n",
    "Train each model on the shared splits, tune thresholds on the validation fold, and evaluate on the test fold for all seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d208764",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_runs = [\n",
    "    run_model_for_seed(\n",
    "        LogisticRegression(\n",
    "            penalty=\"l2\",\n",
    "            solver=\"liblinear\",\n",
    "            max_iter=300,\n",
    "            random_state=s,\n",
    "        ),\n",
    "        model_name=\"Logistic Regression\",\n",
    "        split_seed=s,\n",
    "        use_scaled=True,\n",
    "    )\n",
    "    for s in SEED_PLAN\n",
    "]\n",
    "\n",
    "logreg_summary_df = pd.concat([r[\"summary\"] for r in logreg_runs], ignore_index=True)\n",
    "logreg_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b80ca5",
   "metadata": {},
   "source": [
    "### 7.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df285ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_runs = [\n",
    "    run_model_for_seed(\n",
    "        DecisionTreeClassifier(\n",
    "            random_state=s,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=\"balanced\",\n",
    "        ),\n",
    "        model_name=\"Decision Tree\",\n",
    "        split_seed=s,\n",
    "        use_scaled=False,\n",
    "    )\n",
    "    for s in SEED_PLAN\n",
    "]\n",
    "\n",
    "dt_summary_df = pd.concat([r[\"summary\"] for r in dt_runs], ignore_index=True)\n",
    "dt_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7af63",
   "metadata": {},
   "source": [
    "### 7.3 SVM (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb915ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_runs = [\n",
    "    run_model_for_seed(\n",
    "        SVC(\n",
    "            kernel=\"rbf\",\n",
    "            C=1.0,\n",
    "            gamma=\"scale\",\n",
    "            probability=False,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=s,\n",
    "        ),\n",
    "        model_name=\"SVM (RBF)\",\n",
    "        split_seed=s,\n",
    "        use_scaled=True,\n",
    "    )\n",
    "    for s in SEED_PLAN\n",
    "]\n",
    "\n",
    "svm_summary_df = pd.concat([r[\"summary\"] for r in svm_runs], ignore_index=True)\n",
    "svm_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5430b6",
   "metadata": {},
   "source": [
    "### 7.4 Naive Bayes (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_runs = [\n",
    "    run_model_for_seed(\n",
    "        GaussianNB(),\n",
    "        model_name=\"Naive Bayes\",\n",
    "        split_seed=s,\n",
    "        use_scaled=True,\n",
    "    )\n",
    "    for s in SEED_PLAN\n",
    "]\n",
    "\n",
    "nb_summary_df = pd.concat([r[\"summary\"] for r in nb_runs], ignore_index=True)\n",
    "nb_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304eff37",
   "metadata": {},
   "source": [
    "### 7.5 k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_runs = [run_knn_for_seed(s, k_grid=(5, 15, 25, 35, 50)) for s in SEED_PLAN]\n",
    "\n",
    "knn_summary_df = pd.concat([r[\"summary\"] for r in knn_runs], ignore_index=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"seed\": r[\"seed\"],\n",
    "            \"best_k\": r[\"k\"],\n",
    "            \"threshold\": r[\"threshold\"],\n",
    "            \"val_utility\": r[\"val_utility\"],\n",
    "        }\n",
    "        for r in knn_runs\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a10d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs = {\n",
    "    \"Logistic Regression\": logreg_runs,\n",
    "    \"Decision Tree\": dt_runs,\n",
    "    \"SVM (RBF)\": svm_runs,\n",
    "    \"Naive Bayes\": nb_runs,\n",
    "    \"k-NN\": knn_runs,\n",
    "}\n",
    "\n",
    "model_summary_frames = {\n",
    "    \"Logistic Regression\": logreg_summary_df,\n",
    "    \"Decision Tree\": dt_summary_df,\n",
    "    \"SVM (RBF)\": svm_summary_df,\n",
    "    \"Naive Bayes\": nb_summary_df,\n",
    "    \"k-NN\": knn_summary_df,\n",
    "}\n",
    "\n",
    "combined_model_summaries = pd.concat(model_summary_frames.values(), ignore_index=True)\n",
    "combined_model_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b16f19",
   "metadata": {},
   "source": [
    "## 8. Multi-model visualizations\n",
    "Seed-level utility bars, ROC curves (baseline seed), and a confusion matrix for the top average-utility model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58abebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = combined_model_summaries.copy()\n",
    "plot_data[\"seed\"] = plot_data[\"seed\"].astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(data=plot_data, x=\"seed\", y=\"utility_per_case\", hue=\"model_name\", ax=ax)\n",
    "ax.set_title(\"Utility per case by model and seed\")\n",
    "ax.set_xlabel(\"Seed\")\n",
    "ax.set_ylabel(\"Utility per case\")\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_y_true = baseline_split[\"y_test\"].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for name, runs in model_runs.items():\n",
    "    baseline_run = next(r for r in runs if r[\"seed\"] == BASELINE_SEED)\n",
    "    fpr, tpr, _ = roc_curve(baseline_y_true, baseline_run[\"test_scores\"])\n",
    "    auc_val = baseline_run[\"summary\"][\"roc_auc\"].iloc[0]\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "ax.set_title(\"ROC curves (baseline seed)\")\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdae13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_util = combined_model_summaries.groupby(\"model_name\")[\"utility_per_case\"].mean().sort_values(ascending=False)\n",
    "top_model_name = avg_util.index[0]\n",
    "top_run = next(r for r in model_runs[top_model_name] if r[\"seed\"] == BASELINE_SEED)\n",
    "\n",
    "cm = confusion_matrix(baseline_y_true, top_run[\"test_pred\"], labels=[0, 1])\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    ax=ax,\n",
    "    xticklabels=[\"Approve\", \"Reject\"],\n",
    "    yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    ")\n",
    "ax.set_title(f\"Confusion matrix (baseline seed, {top_model_name})\")\n",
    "ax.set_xlabel(\"Prediction\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae023d",
   "metadata": {},
   "source": [
    "## 9. Summary table (all seeds)\n",
    "Per-seed metrics for every model plus the three baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_true = baseline_split[\"y_test\"].values\n",
    "\n",
    "def baseline_summary(name, y_pred):\n",
    "    return summarize_model(\n",
    "        model_name=name,\n",
    "        dataset_name=\"Test (baseline seed)\",\n",
    "        y_true=baseline_true,\n",
    "        y_pred=y_pred,\n",
    "        y_score=None,\n",
    "        threshold=np.nan,\n",
    "        seed=np.nan,\n",
    "    )\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "baseline_random = baseline_summary(\"Baseline - Random\", rng.integers(0, 2, size=len(baseline_true)))\n",
    "baseline_approve = baseline_summary(\"Baseline - Approve all\", np.zeros_like(baseline_true))\n",
    "baseline_reject = baseline_summary(\"Baseline - Reject all\", np.ones_like(baseline_true))\n",
    "\n",
    "baseline_df = pd.concat([baseline_random, baseline_approve, baseline_reject], ignore_index=True)\n",
    "\n",
    "summary_table = pd.concat([baseline_df, combined_model_summaries], ignore_index=True)\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c99d89",
   "metadata": {},
   "source": [
    "## 10. Model comparison summary (baselines + averages)\n",
    "Average the seed-level rows to compare overall performance alongside the baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a29b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rows(df, label):\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    numeric_cols_no_seed = [c for c in numeric_cols if c != \"seed\"]\n",
    "    avg_row = df[numeric_cols_no_seed].mean().to_frame().T\n",
    "    avg_row[\"model_name\"] = f\"{label} (avg seeds)\"\n",
    "    avg_row[\"dataset_name\"] = \"Test (avg seeds)\"\n",
    "    cols = [c for c in df.columns if c != \"seed\"]\n",
    "    return avg_row.reindex(columns=cols)\n",
    "\n",
    "avg_rows = [\n",
    "    average_rows(logreg_summary_df, \"Logistic Regression\"),\n",
    "    average_rows(dt_summary_df, \"Decision Tree\"),\n",
    "    average_rows(svm_summary_df, \"SVM (RBF)\"),\n",
    "    average_rows(nb_summary_df, \"Naive Bayes\"),\n",
    "    average_rows(knn_summary_df, \"k-NN\"),\n",
    "]\n",
    "\n",
    "comparison_df = pd.concat([baseline_df] + avg_rows, ignore_index=True)\n",
    "comparison_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
