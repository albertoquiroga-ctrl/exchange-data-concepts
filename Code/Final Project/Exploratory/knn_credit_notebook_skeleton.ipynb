{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5748da",
   "metadata": {},
   "source": [
    "# Student guide: End\u2011to\u2011end KNN for credit default\n",
    "\n",
    "This notebook walks through building and evaluating a k\u2011Nearest Neighbors (KNN) classifier to predict credit default. It\u2019s written to be easy to follow for a first\u2011time reader.\n",
    "\n",
    "What you\u2019ll learn in this notebook:\n",
    "- The problem we\u2019re solving: predict whether a customer will default (the TARGET column).\n",
    "- The data pipeline: how we clean data, encode categorical columns, and scale numeric features.\n",
    "- The model training loop: how we choose KNN hyperparameters with cross\u2011validation.\n",
    "- Evaluation: how to read ROC/PR curves, confusion matrices, and threshold trade\u2011offs.\n",
    "- Decisions and assumptions: why we made each choice and what it implies for the business.\n",
    "\n",
    "Key ideas upfront:\n",
    "- KNN needs features to be on comparable scales, so we scale numeric features and one\u2011hot encode categoricals using a ColumnTransformer inside a Pipeline.\n",
    "- Class imbalance matters: default cases are relatively rare, so PR AUC and recall are important, not just accuracy.\n",
    "- Thresholds are business decisions: we don\u2019t just predict 0/1; we choose a cutoff that maximizes expected business utility using per\u2011loan utility arrays computed from BUSINESS_PARAMS (no static cost matrix).\n",
    "\n",
    "How to read each section:\n",
    "- Before each major code block, we added plain\u2011English notes that explain what\u2019s happening and why.\n",
    "- Inline variable names (like `preprocessor`, `param_grid`, `best_model`, `best_threshold`, `cm`, `test_results`) are consistent across cells, so you can connect the narrative to the code outputs you see.\n",
    "- If you\u2019re new to the topic, scan the markdown first, then step into the code and outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN for Credit Decisions and Profit Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Executive Summary\n",
    "\n",
    "### What are we trying to do?\n",
    "**Objective:** Build a smart system that decides whether to approve or reject credit card applications to **maximize profit** for the bank.\n",
    "\n",
    "### What will we deliver?\n",
    "1. A KNN (K-Nearest Neighbors) machine learning model that predicts if someone will default on their payments\n",
    "2. An optimized decision threshold that tells us when to approve vs. reject applications\n",
    "3. A complete evaluation showing how much profit the model generates compared to simple baseline strategies\n",
    "4. Guidelines for deploying this model in a real banking system\n",
    "\n",
    "### How do we measure success?\n",
    "- The model must make **more profit** than two simple strategies: \"approve everyone\" or \"reject everyone\"\n",
    "- We'll also ensure the model is fair and follows banking regulations\n",
    "\n",
    "### Important assumptions:\n",
    "- We're working with **existing customers** who already have a credit history with the bank\n",
    "- We're predicting if they'll default in the **next period** (next month)\n",
    "- The bank has given us the costs and benefits of correct and incorrect decisions\n",
    "\n",
    "### What are the limitations?\n",
    "- KNN is sensitive to feature scaling (all variables must be on similar scales)\n",
    "- It can be slow with very large datasets\n",
    "- We need to carefully choose how many neighbors (k) to use\n",
    "- **How we handle these:** We'll use proper data preprocessing, test different values of k, and monitor performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student note: decisions and assumptions at a glance\n",
    "\n",
    "- Target variable: `TARGET` equals 1 for default, 0 for no default.\n",
    "- Class balance: defaults are a minority class, so we emphasize recall/PR AUC in addition to accuracy.\n",
    "- Features: numeric columns are scaled; categorical columns are one\u2011hot encoded. Both are handled inside a `ColumnTransformer` so the steps are part of the pipeline and cross\u2011validation.\n",
    "- Modeling choice: we start with KNN to build intuition about distance\u2011based methods. KNN is sensitive to feature scaling and the choice of neighbors `n_neighbors`.\n",
    "- Validation: we use stratified k\u2011fold CV to keep class proportions similar across folds.\n",
    "- Thresholding: instead of the default 0.5, we choose a probability cutoff that maximizes business utility using per\u2011loan utility arrays computed from `BUSINESS_PARAMS` via `BusinessModel` (no static cost matrix).\n",
    "- What to look for: ROC AUC for ranking quality, PR AUC for minority class focus, confusion matrix at the chosen threshold, decision mix (approval/rejection rates), and the resulting business utility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479c4a7",
   "metadata": {},
   "source": [
    "### Student note: decisions and assumptions at a glance\n",
    "\n",
    "- Target variable: `TARGET` equals 1 for default, 0 for no default.\n",
    "- Class balance: defaults are a minority class, so we emphasize recall/PR AUC in addition to accuracy.\n",
    "- Features: numeric columns are scaled; categorical columns are one\u2011hot encoded. Both are handled inside a `ColumnTransformer` so the steps are part of the pipeline and cross\u2011validation.\n",
    "- Modeling choice: we start with KNN to build intuition about distance\u2011based methods. KNN is sensitive to feature scaling and the choice of neighbors `n_neighbors`.\n",
    "- Validation: we use stratified k\u2011fold CV to keep class proportions similar across folds.\n",
    "- Thresholding: instead of the default 0.5, we choose a probability cutoff that maximizes business utility using per\u2011loan utility arrays computed from `BUSINESS_PARAMS` via `BusinessModel` (no static cost matrix).\n",
    "- What to look for: ROC AUC for ranking quality, PR AUC for minority class focus, confusion matrix at the chosen threshold, decision mix (approval/rejection rates), and the resulting business utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ================================================================================\n",
    "# We need various Python libraries for data analysis and machine learning\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# For displaying outputs in the notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# For creating visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For numerical operations and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn: Our machine learning library\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURE DISPLAY SETTINGS\n",
    "# ================================================================================\n",
    "plt.style.use(\"seaborn-v0_8\")  # Use a clean plotting style\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:,.2f}\")  # Format numbers with 2 decimals\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Hide unnecessary warnings\n",
    "\n",
    "# ================================================================================\n",
    "# LOCATE THE DATA FILE\n",
    "# ================================================================================\n",
    "# The data file might be in different locations depending on where the notebook is run\n",
    "PROJECT_ROOT = Path.cwd()  # Current working directory\n",
    "DEFAULT_DATA_FILENAME = \"default of credit card clients.xls\"\n",
    "\n",
    "# List of potential file locations to check\n",
    "potential_files = [\n",
    "    PROJECT_ROOT / DEFAULT_DATA_FILENAME,\n",
    "    PROJECT_ROOT / \"Code\" / \"Final Project\" / \"Loan Defaults\" / DEFAULT_DATA_FILENAME,\n",
    "]\n",
    "\n",
    "# Try to find the file in one of these locations\n",
    "for candidate in potential_files:\n",
    "    if candidate.exists():\n",
    "        DATA_FILE = candidate\n",
    "        break\n",
    "else:\n",
    "    # If we can't find the file anywhere, raise an error\n",
    "    raise FileNotFoundError(\n",
    "        \"Default credit dataset not found. Expected at one of: \"\n",
    "        + \", \".join(str(path) for path in potential_files)\n",
    "    )\n",
    "\n",
    "# ================================================================================\n",
    "# DEFINE CONSTANTS\n",
    "# ================================================================================\n",
    "DATA_DIR = DATA_FILE.parent  # Directory where the data file is located\n",
    "TARGET = \"default_payment_next_month\"  # The variable we're trying to predict\n",
    "ID_COLS = [\"ID\"]  # Column(s) that just identify customers (not used for prediction)\n",
    "SEED = 42  # Random seed for reproducibility (ensures results are consistent)\n",
    "rng = np.random.default_rng(SEED)  # Random number generator\n",
    "\n",
    "# ================================================================================\n",
    "# LOAD THE DATA\n",
    "# ================================================================================\n",
    "# Excel files can use different engines; .xls files need the 'xlrd' engine\n",
    "excel_engine = \"xlrd\" if DATA_FILE.suffix == \".xls\" else None\n",
    "\n",
    "try:\n",
    "    raw_df = (\n",
    "        pd.read_excel(DATA_FILE, header=1, engine=excel_engine)  # Skip first row, use second row as headers\n",
    "        .rename(columns={\"default payment next month\": TARGET})  # Standardize the target column name\n",
    "    )\n",
    "except ImportError as err:\n",
    "    # If xlrd is not installed, provide helpful error message\n",
    "    raise ImportError(\n",
    "        \"Missing optional dependency 'xlrd'. Install it with `pip install xlrd>=2.0.1` to read .xls files.\"\n",
    "    ) from err\n",
    "\n",
    "# Ensure the target variable is stored as an integer (0 or 1)\n",
    "raw_df[TARGET] = raw_df[TARGET].astype(int)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Rows: {raw_df.shape[0]:,} | Columns: {raw_df.shape[1]}\")\n",
    "raw_df.head()  # Show first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# BUSINESS UTILITY HELPERS\n",
    "# ================================================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure Python can import business_utils regardless of where the kernel starts.\n",
    "_possible_dirs = [\n",
    "    Path.cwd(),\n",
    "    Path.cwd() / \"Code\" / \"Final Project\" / \"Loan Defaults\",\n",
    "]\n",
    "for _dir in _possible_dirs:\n",
    "    candidate = _dir / \"business_utils.py\"\n",
    "    if candidate.exists():\n",
    "        if str(_dir) not in sys.path:\n",
    "            sys.path.insert(0, str(_dir))\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"business_utils.py not found in expected directories.\")\n",
    "\n",
    "from business_utils import (\n",
    "    BusinessModel,\n",
    "    DEFAULT_BUSINESS_PARAMS,\n",
    "    build_utility_scorer,\n",
    "    confusion_counts,\n",
    "    realized_utility_from_arrays,\n",
    "    search_best_threshold_arrays,\n",
    ")\n",
    "\n",
    "USER_PARAMS = globals().get(\"BUSINESS_PARAMS\", {})\n",
    "BUSINESS_PARAMS = {**DEFAULT_BUSINESS_PARAMS, **(USER_PARAMS or {})}\n",
    "\n",
    "biz = BusinessModel(BUSINESS_PARAMS)\n",
    "utility_scorer = build_utility_scorer(biz)\n",
    "\n",
    "print(\"Business utility helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Business Metrics: How Do We Measure Success?\n",
    "\n",
    "In business applications, accuracy alone is not enough. We need to quantify the financial impact of our approve/reject decisions. This notebook uses an arrays-based utility approach driven by BUSINESS_PARAMS (no static cost matrix).\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Per-loan utility (arrays-based, no static table)\n",
    "\n",
    "We compute four per-loan arrays from BUSINESS_PARAMS via the BusinessModel class:\n",
    "\n",
    "- B_TP (benefit when we approve a non-default)\n",
    "- C_FP (cost when we approve a default)\n",
    "- B_TN (benefit when we reject a default)\n",
    "- C_FN (cost when we reject a non-default)\n",
    "\n",
    "Derived from parameters:\n",
    "- EAD (Exposure at Default) from ead_method (e.g., LIMIT_BAL, avg BILL_AMT1..3, max BILL_AMT1..6), optionally capped by LIMIT_BAL\n",
    "- Spread = APR \u2212 cost_of_funds; horizon in years = horizon_months / 12\n",
    "- B_TP = EAD \u00d7 Spread \u00d7 horizon_years\n",
    "- C_FP = EAD \u00d7 LGD + collection_cost_flat\n",
    "- B_TN = tn_benefit_flat\n",
    "- C_FN = max(0, B_TP \u2212 (origination_cost + service_cost_monthly \u00d7 horizon_months))\n",
    "\n",
    "Decision/label convention used in this notebook:\n",
    "- y_true: 1 = default, 0 = no default (dataset convention)\n",
    "- y_hat_approve: 1 = approve (prob_default < threshold), 0 = reject\n",
    "\n",
    "Outcome mapping under approve=1:\n",
    "- TP: approve non-default \u2192 +B_TP\n",
    "- FP: approve default \u2192 \u2212C_FP\n",
    "- TN: reject default \u2192 +B_TN\n",
    "- FN: reject non-default \u2192 \u2212C_FN\n",
    "\n",
    "No fixed dollar values are hard-coded; utilities depend on each applicant\u2019s EAD and the chosen parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. The Math Behind Optimal Decisions (with arrays)\n",
    "\n",
    "For each applicant, the model estimates a default probability p\u0302.\n",
    "\n",
    "Expected utilities:\n",
    "- U(approve) = (1 \u2212 p\u0302) \u00d7 B_TP \u2212 p\u0302 \u00d7 C_FP\n",
    "- U(reject)  = p\u0302 \u00d7 B_TN \u2212 (1 \u2212 p\u0302) \u00d7 C_FN\n",
    "\n",
    "Decision rule (per applicant): approve if U(approve) \u2265 U(reject).\n",
    "\n",
    "Operationally, we use a single threshold t and approve when prob_default < t. We search t over a grid and select best_threshold that maximizes realized utility on validation/test using the per-loan arrays above.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. Baseline Strategies (What Happens Without ML?)\n",
    "\n",
    "We compare our model to simple baselines:\n",
    "\n",
    "1. \"Approve All\": maximal growth, maximal default risk\n",
    "2. \"Reject All\": minimal risk, zero growth\n",
    "\n",
    "Our goal: beat both baselines by choosing an operating point (threshold) that improves expected utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy cost-matrix approach removed in favor of per-loan utility arrays.\n",
    "# This cell intentionally clears any previous COST_MATRIX and related helpers to avoid confusion.\n",
    "try:\n",
    "    del COST_MATRIX\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "def cost_sensitive_utility(*args, **kwargs):\n",
    "    raise NotImplementedError(\n",
    "        \"Legacy cost-matrix utility was removed. Use BusinessModel.per_loan_arrays + realized_utility_from_arrays instead.\"\n",
    "    )\n",
    "\n",
    "print(\"Legacy COST_MATRIX removed. Use BusinessModel arrays-based utility only.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before building any model, we need to understand our data. Let's investigate:\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Target Distribution: How Imbalanced Is Our Data?\n",
    "\n",
    "**Why this matters:** If defaults are rare (e.g., only 10% of customers default), then a model that predicts \"no default\" for everyone would be 90% accurate but completely useless for business purposes!\n",
    "\n",
    "**What we'll check:**\n",
    "- What percentage of customers defaulted?\n",
    "- Is the dataset imbalanced?\n",
    "\n",
    "**Implications for modeling:**\n",
    "- Simple accuracy is misleading with imbalanced data\n",
    "- We need to use ROC-AUC, Precision-Recall curves, and **business utility** instead\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. Variable Types: What Kind of Data Do We Have?\n",
    "\n",
    "Understanding our features helps us choose the right preprocessing:\n",
    "\n",
    "1. **Continuous Numeric:** `LIMIT_BAL`, `BILL_AMT1-6`, `PAY_AMT1-6`, `AGE`\n",
    "   - These have a wide range of values\n",
    "   - **Need scaling** for KNN (otherwise large amounts will dominate distance calculations)\n",
    "\n",
    "2. **Ordinal:** `PAY_0` through `PAY_6` (payment status)\n",
    "   - These have a natural order (-1 < 0 < 1 < 2...)\n",
    "   - Represent delay severity\n",
    "   - Keep as numeric\n",
    "\n",
    "3. **Categorical:** `SEX`, `EDUCATION`, `MARRIAGE`\n",
    "   - These are codes (1, 2, 3...) but don't have meaningful numeric relationships\n",
    "   - **Need one-hot encoding** for KNN\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3. Missing Values and Outliers\n",
    "\n",
    "**Missing values:** We'll check if any data is missing and decide how to handle it (imputation).\n",
    "\n",
    "**Outliers:** Extreme values can distort KNN distance calculations. We'll:\n",
    "- Visualize distributions with boxplots\n",
    "- Consider winsorization or robust scaling if needed\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4. Feature Correlations and Scales\n",
    "\n",
    "**Why scaling is CRITICAL for KNN:**\n",
    "- KNN uses distance to find similar customers\n",
    "- If `LIMIT_BAL` ranges from 10,000 to 1,000,000 but `AGE` ranges from 20 to 70...\n",
    "- Distance will be dominated by `LIMIT_BAL` and `AGE` will be ignored!\n",
    "\n",
    "**Solution:** Use `StandardScaler` to put all features on the same scale (mean=0, std=1)\n",
    "\n",
    "**Correlations:** We'll check which features are related to default risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c93e8",
   "metadata": {},
   "source": [
    "### Student note: preprocessing pipeline (what each step does)\n",
    "\n",
    "- We split features into two lists: `numeric_features` and `categorical_features`.\n",
    "- Numeric pipeline: `SimpleImputer(strategy='median')` fills missing values; `StandardScaler()` puts all numeric features on the same scale.\n",
    "- Categorical pipeline: `SimpleImputer(strategy='most_frequent')` fills missing; `OneHotEncoder(handle_unknown='ignore')` turns categories into 0/1 columns.\n",
    "- We combine both with `ColumnTransformer` so transformations apply to the correct columns.\n",
    "- We wrap the transformer and the estimator inside a single `Pipeline` so cross\u2011validation and grid search apply transformations consistently in each fold (avoids data leakage).\n",
    "- Why it matters for KNN: distances are computed in feature space; without scaling, variables with larger ranges dominate the distance calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# CHECK TARGET DISTRIBUTION\n",
    "# ================================================================================\n",
    "# Count how many customers defaulted (1) vs. didn't default (0)\n",
    "target_counts = (\n",
    "    raw_df[TARGET]\n",
    "    .value_counts()                                           # Count each class\n",
    "    .rename_axis(\"default\")                                   # Name the index\n",
    "    .reset_index(name=\"count\")                                # Convert to DataFrame\n",
    "    .assign(pct=lambda df: df[\"count\"] / df[\"count\"].sum())  # Add percentage column\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# CHECK FOR MISSING VALUES\n",
    "# ================================================================================\n",
    "# Find columns with missing data\n",
    "missing = raw_df.isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "# ================================================================================\n",
    "# CALCULATE CORRELATIONS\n",
    "# ================================================================================\n",
    "# We'll focus on payment status variables and credit limit\n",
    "pay_cols = [c for c in raw_df.columns if c.startswith(\"PAY_\")]  # All PAY_0 through PAY_6\n",
    "corr_cols = pay_cols + [\"LIMIT_BAL\", TARGET]  # Variables to include in correlation analysis\n",
    "corr_matrix = raw_df[corr_cols].corr(method=\"spearman\")  # Use Spearman for ordinal variables\n",
    "\n",
    "# ================================================================================\n",
    "# DISPLAY RESULTS\n",
    "# ================================================================================\n",
    "display(target_counts)\n",
    "\n",
    "# Only display missing values if there are any\n",
    "if not missing.empty:\n",
    "    display(missing.to_frame(\"missing_values\"))\n",
    "\n",
    "# Show basic statistics for the first 10 variables\n",
    "display(raw_df.describe().T[[\"mean\", \"std\", \"min\", \"max\"]].round(2).head(10))\n",
    "\n",
    "# ================================================================================\n",
    "# CREATE VISUALIZATIONS\n",
    "# ================================================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Plot 1: Target distribution (default vs. no default)\n",
    "sns.barplot(data=target_counts, x=\"default\", y=\"pct\", ax=axes[0])\n",
    "axes[0].set_title(\"Target distribution\")\n",
    "axes[0].set_ylabel(\"Prevalence\")\n",
    "# This shows us if the dataset is imbalanced\n",
    "\n",
    "# Plot 2: Credit limit distribution (check for outliers)\n",
    "sns.boxplot(data=raw_df, y=\"LIMIT_BAL\", ax=axes[1])\n",
    "axes[1].set_title(\"LIMIT_BAL spread\")\n",
    "# Boxplot shows median, quartiles, and outliers\n",
    "\n",
    "# Plot 3: Correlation heatmap (which variables relate to default?)\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", center=0, ax=axes[2])\n",
    "axes[2].set_title(\"Spearman correlation snapshot\")\n",
    "# Red = positive correlation, Blue = negative correlation\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Avoiding Data Leakage\n",
    "\n",
    "**Data Leakage** = accidentally using information that wouldn't be available when making real predictions. This is a **critical** mistake that makes models look good in testing but fail in production!\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Inclusion/Exclusion Criteria\n",
    "\n",
    "**Golden Rule:** Only use variables that would be **available at decision time** (when the customer applies).\n",
    "\n",
    "**What we can use:**\n",
    "- Demographics (age, education, marital status)\n",
    "- Historical payment behavior (PAY_0 through PAY_6)\n",
    "- Bill and payment amounts from previous months\n",
    "- Current credit limit\n",
    "\n",
    "**What we CANNOT use:**\n",
    "- Any information from the future\n",
    "- Any variable that's calculated after we know if they defaulted\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2. Transformations and Encoding\n",
    "\n",
    "Different types of features need different preprocessing:\n",
    "\n",
    "1. **Categorical variables** (`SEX`, `EDUCATION`, `MARRIAGE`):\n",
    "   - **One-hot encoding:** Convert to binary columns\n",
    "   - Example: `SEX` \u2192 `SEX_1` (male), `SEX_2` (female)\n",
    "   - Why? KNN can't interpret \"2 is twice as much as 1\" for gender!\n",
    "\n",
    "2. **Ordinal variables** (`PAY_0` through `PAY_6`):\n",
    "   - Keep as numeric since they have meaningful order\n",
    "   - -1 (paid early) < 0 (on time) < 1 (1 month late) < 2 (2 months late)\n",
    "\n",
    "3. **Continuous variables** (amounts, limits, age):\n",
    "   - Log transform for heavily skewed distributions (optional)\n",
    "   - **StandardScaler** to put everything on the same scale (required!)\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3. Final Feature List\n",
    "\n",
    "We'll use all available features except:\n",
    "- `ID`: Just an identifier, no predictive value\n",
    "- The target variable itself (obviously!)\n",
    "\n",
    "This gives us approximately 22-25 features after encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# IDENTIFY FEATURE TYPES\n",
    "# ================================================================================\n",
    "# Separate features into categorical and numeric for proper preprocessing\n",
    "\n",
    "# Categorical features (need one-hot encoding)\n",
    "categorical_features = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
    "\n",
    "# Numeric features (everything else except ID and target)\n",
    "numeric_features = [\n",
    "    col for col in raw_df.columns\n",
    "    if col not in ID_COLS + [TARGET] + categorical_features\n",
    "]\n",
    "\n",
    "# ================================================================================\n",
    "# CREATE FEATURE OVERVIEW\n",
    "# ================================================================================\n",
    "# Build a summary table showing each feature and its type\n",
    "feature_overview = pd.DataFrame({\n",
    "    \"feature\": numeric_features + categorical_features,\n",
    "    \"type\": (\n",
    "        [\"numeric\"] * len(numeric_features) + \n",
    "        [\"categorical\"] * len(categorical_features)\n",
    "    ),\n",
    "})\n",
    "\n",
    "# Display the first 12 features\n",
    "display(feature_overview.head(12))\n",
    "\n",
    "# Print total count\n",
    "print(f\"Total candidate features: {len(feature_overview)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting and Validation Strategy\n",
    "\n",
    "**Why split the data?** To get an honest estimate of how well our model will work on new, unseen customers!\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1. Train/Test Split\n",
    "\n",
    "We'll split the data into two parts:\n",
    "\n",
    "1. **Training set (80%):** Used to build and tune the model\n",
    "2. **Test set (20%):** Held out completely until the very end\n",
    "   - Simulates new customers the model has never seen\n",
    "   - Gives us an unbiased estimate of real-world performance\n",
    "\n",
    "**Stratified splitting:** We ensure both sets have the same proportion of defaults (e.g., if 22% of all customers default, both train and test will have ~22%).\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2. Cross-Validation for Model Tuning\n",
    "\n",
    "**The problem:** If we use the training data to both train AND evaluate, we might overfit.\n",
    "\n",
    "**The solution:** 5-fold stratified cross-validation\n",
    "- Split training data into 5 parts\n",
    "- Train on 4 parts, test on the 5th\n",
    "- Rotate which part is used for testing\n",
    "- Average the results\n",
    "\n",
    "This gives us a more reliable estimate of performance!\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3. Metrics We'll Track\n",
    "\n",
    "1. **ROC-AUC:** How well can the model separate defaulters from non-defaulters?\n",
    "   - 0.5 = random guessing\n",
    "   - 1.0 = perfect separation\n",
    "\n",
    "2. **PR-AUC (Precision-Recall AUC):** Better for imbalanced data\n",
    "   - Focuses on the minority class (defaults)\n",
    "\n",
    "3. **Utility (MOST IMPORTANT!):** Expected profit per customer\n",
    "   - This is what the business actually cares about!\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4. Reproducibility\n",
    "\n",
    "We set `random_state=42` everywhere to ensure:\n",
    "- Same data splits every time\n",
    "- Same results when we re-run the notebook\n",
    "- Others can verify our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# PREPARE FEATURES AND TARGET\n",
    "# ================================================================================\n",
    "# Separate the features (X) from what we're trying to predict (y)\n",
    "X = raw_df.drop(columns=ID_COLS + [TARGET])  # All columns except ID and target\n",
    "y = raw_df[TARGET]  # Just the target column\n",
    "\n",
    "# ================================================================================\n",
    "# SPLIT INTO TRAIN AND TEST SETS\n",
    "# ================================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,        # 20% for testing, 80% for training\n",
    "    stratify=y,           # Keep the same proportion of defaults in both sets\n",
    "    random_state=SEED,    # For reproducibility\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# CREATE CROSS-VALIDATION OBJECT\n",
    "# ================================================================================\n",
    "# This will be used for tuning hyperparameters\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,           # 5-fold cross-validation\n",
    "    shuffle=True,         # Randomly shuffle before splitting\n",
    "    random_state=SEED     # For reproducibility\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# VERIFY THE SPLIT\n",
    "# ================================================================================\n",
    "print(f\"Train size: {X_train.shape[0]:,} | Test size: {X_test.shape[0]:,}\")\n",
    "print(f\"Train default rate: {y_train.mean():.3f} | Test default rate: {y_test.mean():.3f}\")\n",
    "# The default rates should be very similar, confirming stratification worked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building the Preprocessing Pipeline\n",
    "\n",
    "**Why use a pipeline?** To ensure preprocessing is applied correctly and consistently:\n",
    "- \u2705 Prevents data leakage (scaling is fit only on training data)\n",
    "- \u2705 Makes code cleaner and easier to maintain\n",
    "- \u2705 Ensures the same transformations are applied to training and test data\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1. Scaling: Why It's Critical for KNN\n",
    "\n",
    "**The Problem:**\n",
    "```\n",
    "LIMIT_BAL: ranges from 10,000 to 1,000,000\n",
    "AGE:       ranges from 21 to 75\n",
    "```\n",
    "\n",
    "If we calculate distance without scaling:\n",
    "- A difference of 10,000 in `LIMIT_BAL` is huge\n",
    "- A difference of 10 in `AGE` seems tiny\n",
    "- Result: KNN ignores age completely!\n",
    "\n",
    "**The Solution:** `StandardScaler`\n",
    "- Transforms each feature to have mean=0 and standard deviation=1\n",
    "- Now all features contribute equally to distance calculations\n",
    "\n",
    "**Important:** We fit the scaler on training data only, then apply it to test data. This prevents leakage!\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2. Encoding Categorical Variables\n",
    "\n",
    "**One-Hot Encoding** for `SEX`, `EDUCATION`, `MARRIAGE`:\n",
    "\n",
    "Before:\n",
    "```\n",
    "SEX = 1\n",
    "```\n",
    "\n",
    "After:\n",
    "```\n",
    "SEX_1 = 1, SEX_2 = 0\n",
    "```\n",
    "\n",
    "This prevents the model from thinking \"2 is twice as much as 1\" for categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3. Pipeline Structure\n",
    "\n",
    "Our pipeline has three stages:\n",
    "\n",
    "```\n",
    "1. Preprocessing (ColumnTransformer)\n",
    "   \u251c\u2500\u2500 Numeric features \u2192 Impute \u2192 Scale\n",
    "   \u2514\u2500\u2500 Categorical features \u2192 Impute \u2192 One-Hot Encode\n",
    "   \n",
    "2. Scaling (built into numeric pipeline)\n",
    "\n",
    "3. KNN Classifier (added in next section)\n",
    "```\n",
    "\n",
    "All transformations are learned from training data and applied to both train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cf340",
   "metadata": {},
   "source": [
    "### Student note: KNN model and how we choose K\n",
    "\n",
    "- Model: `KNeighborsClassifier(n_neighbors=k, weights='distance' or 'uniform')` predicts by looking at the k closest training points.\n",
    "- Why tune K: too small (e.g., k=1) overfits noise; too large over\u2011smooths and misses patterns. We search a range, e.g., 3\u201351.\n",
    "- Distance and weights: with `weights='distance'`, nearer neighbors count more than farther ones.\n",
    "- Cross\u2011validation: we use `GridSearchCV` with stratified folds, scoring by ROC AUC or PR AUC to find the best configuration.\n",
    "- Output: `best_model` is the fitted pipeline with the best hyperparameters; `cv_results_df` lets us compare metrics across K values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# CREATE PIPELINE FOR NUMERIC FEATURES\n",
    "# ================================================================================\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        # Step 1: Handle any missing values by filling with the median\n",
    "        # (Median is robust to outliers)\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        \n",
    "        # Step 2: Scale features to mean=0, std=1\n",
    "        # This is CRITICAL for KNN!\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# CREATE PIPELINE FOR CATEGORICAL FEATURES\n",
    "# ================================================================================\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        # Step 1: Fill missing values with the most common category\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        \n",
    "        # Step 2: One-hot encode\n",
    "        # handle_unknown='ignore' means if we see a new category in test data, ignore it\n",
    "        # sparse_output=False means return a regular array (not a sparse matrix)\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# COMBINE BOTH PIPELINES WITH COLUMNTRANSFORMER\n",
    "# ================================================================================\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply numeric_pipeline to numeric features\n",
    "        (\"num\", numeric_pipeline, numeric_features),\n",
    "        \n",
    "        # Apply categorical_pipeline to categorical features\n",
    "        (\"cat\", categorical_pipeline, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the preprocessor structure\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. KNN Model Training and Hyperparameter Tuning\n",
    "\n",
    "Now we build and optimize our K-Nearest Neighbors classifier!\n",
    "\n",
    "---\n",
    "\n",
    "### 7.1. Hyperparameters to Tune\n",
    "\n",
    "KNN has several settings that affect performance:\n",
    "\n",
    "1. **`n_neighbors` (k):** How many neighbors to consider?\n",
    "   - **Small k (e.g., 5):** \n",
    "     - More sensitive to individual customers\n",
    "     - Higher variance, might overfit\n",
    "   - **Large k (e.g., 50):** \n",
    "     - Smoother decision boundaries\n",
    "     - Might be too general (underfit)\n",
    "   - **We'll test:** k = 11, 21, 31, 41\n",
    "\n",
    "2. **`weights`:** How to weight the neighbors' votes?\n",
    "   - **`uniform`:** All k neighbors vote equally\n",
    "   - **`distance`:** Closer neighbors get more influence\n",
    "   - **We'll test:** both options\n",
    "\n",
    "3. **`p`:** Which distance metric to use?\n",
    "   - **p=1 (Manhattan/L1):** |x\u2081-x\u2082| + |y\u2081-y\u2082| (city block distance)\n",
    "   - **p=2 (Euclidean/L2):** \u221a[(x\u2081-x\u2082)\u00b2 + (y\u2081-y\u2082)\u00b2] (straight line distance)\n",
    "   - **We'll test:** both options\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2. Hyperparameter Search Strategy\n",
    "\n",
    "**Grid Search with Cross-Validation:**\n",
    "- Try all combinations: 4 values of k \u00d7 2 weights \u00d7 2 distance metrics = **16 combinations**\n",
    "- For each combination, do 5-fold cross-validation\n",
    "- Total: 16 \u00d7 5 = **80 model fits**\n",
    "\n",
    "**Metrics tracked:**\n",
    "1. **Utility** (primary) - What we optimize for\n",
    "2. ROC-AUC (secondary) - General performance\n",
    "3. PR-AUC (secondary) - Performance on minority class\n",
    "\n",
    "**Best model:** The one with highest average utility across folds\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3. The Bias-Variance Trade-off\n",
    "\n",
    "**Bias:** Error from oversimplifying (large k \u2192 high bias)\n",
    "**Variance:** Error from being too sensitive to training data (small k \u2192 high variance)\n",
    "\n",
    "We'll plot utility vs. k to find the sweet spot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# CREATE THE FULL PIPELINE (PREPROCESSING + KNN)\n",
    "# ================================================================================\n",
    "knn_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),          # First: preprocess the data\n",
    "        (\"knn\", KNeighborsClassifier()),       # Then: apply KNN classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# COARSE SWEEP TO PICK BEST N_NEIGHBORS\n",
    "# ================================================================================\n",
    "if \"BEST_K\" not in globals():\n",
    "    k_candidates = [7, 11, 15, 21, 31, 41, 55]\n",
    "    sweep_rows = []\n",
    "    for k in k_candidates:\n",
    "        model = clone(knn_pipeline)\n",
    "        model.set_params(\n",
    "            knn__n_neighbors=k,\n",
    "            knn__weights=\"distance\",\n",
    "            knn__p=2,\n",
    "            knn__algorithm=\"auto\",\n",
    "        )\n",
    "        scores = cross_val_score(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=utility_scorer,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        sweep_rows.append({\n",
    "            \"k\": k,\n",
    "            \"utility_mean\": scores.mean(),\n",
    "            \"utility_std\": scores.std(),\n",
    "        })\n",
    "\n",
    "    k_sweep = (\n",
    "        pd.DataFrame(sweep_rows)\n",
    "        .sort_values(\"utility_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    display(k_sweep)\n",
    "    BEST_K = int(k_sweep.loc[0, \"k\"])\n",
    "    print(f\"Selected BEST_K={BEST_K} based on coarse utility sweep.\")\n",
    "else:\n",
    "    print(f\"Reusing cached BEST_K={BEST_K} (set earlier in this kernel).\")\n",
    "\n",
    "best_k = int(locals().get(\"BEST_K\", 31))\n",
    "\n",
    "# ================================================================================\n",
    "# DEFINE HYPERPARAMETER GRID (USES SINGLE BEST K)\n",
    "# ================================================================================\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [best_k],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    \"knn__p\": [1, 2],\n",
    "    \"knn__algorithm\": [\"auto\", \"ball_tree\"],\n",
    "    \"knn__leaf_size\": [20, 40],\n",
    "}\n",
    "\n",
    "# ================================================================================\n",
    "# RUN GRID SEARCH WITH CROSS-VALIDATION\n",
    "# ================================================================================\n",
    "grid = GridSearchCV(\n",
    "    estimator=knn_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    \n",
    "    # Define multiple metrics to track\n",
    "    scoring={\n",
    "        \"utility\": utility_scorer,              # Our custom business metric (PRIMARY)\n",
    "        \"roc_auc\": \"roc_auc\",                  # Standard ROC-AUC\n",
    "        \"pr_auc\": \"average_precision\"          # Precision-Recall AUC\n",
    "    },\n",
    "    \n",
    "    refit=\"utility\",        # Choose the best model based on utility (not ROC-AUC!)\n",
    "    cv=cv,                  # Use our 5-fold stratified cross-validation\n",
    "    n_jobs=-1,              # Use all available CPU cores for speed\n",
    "    verbose=2,              # Print progress updates\n",
    ")\n",
    "\n",
    "# Fit the grid search (this will take a minute...)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# ================================================================================\n",
    "# EXAMINE RESULTS\n",
    "# ================================================================================\n",
    "# Extract results and sort by utility (best first)\n",
    "cv_results = (\n",
    "    pd.DataFrame(grid.cv_results_)\n",
    "    .sort_values(by=\"mean_test_utility\", ascending=False)  # Best utility on top\n",
    "    .loc[:, [\n",
    "        \"param_knn__n_neighbors\",\n",
    "        \"param_knn__weights\",\n",
    "        \"param_knn__p\",\n",
    "        \"param_knn__algorithm\",\n",
    "        \"param_knn__leaf_size\",\n",
    "        \"mean_test_utility\",    # Average utility across 5 folds\n",
    "        \"mean_test_roc_auc\",    # Average ROC-AUC\n",
    "        \"mean_test_pr_auc\",     # Average PR-AUC\n",
    "    ]]\n",
    ")\n",
    "\n",
    "# Display top 8 configurations\n",
    "display(cv_results.head(8))\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(f\"Best normalized utility (cv): {grid.best_score_:.4f}\")\n",
    "\n",
    "# Save the best model for later use\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Probability Calibration and Threshold Optimization\n",
    "\n",
    "We have a model, but we still need to decide: **when do we approve vs. reject?**\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1. How KNN Produces Probabilities\n",
    "\n",
    "KNN doesn't directly calculate probabilities like logistic regression. Instead:\n",
    "- It finds the k nearest neighbors\n",
    "- Calculates the fraction that defaulted\n",
    "- **Example:** If 3 out of 10 neighbors defaulted, predicted probability = 0.3\n",
    "\n",
    "**Calibration check:** Are these probabilities accurate?\n",
    "- If the model says 30% chance of default, do ~30% of those customers actually default?\n",
    "- We'll use a calibration plot to verify\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2. Finding the Optimal Decision Threshold\n",
    "\n",
    "**Common misconception:** Use 0.5 as the threshold (if probability \u2265 0.5, predict default)\n",
    "\n",
    "**Reality:** The optimal threshold depends on the cost/benefit matrix!\n",
    "- If False Positives are very expensive (like in our case: -$6,000)\n",
    "- We might only approve when we're very confident (e.g., threshold = 0.2)\n",
    "\n",
    "**Our approach:**\n",
    "1. Generate probabilities for all training customers\n",
    "2. Try many different thresholds (5%, 10%, 15%, ..., 95%)\n",
    "3. Calculate utility at each threshold\n",
    "4. Pick the threshold that maximizes utility ($\\tau^*$)\n",
    "\n",
    "---\n",
    "\n",
    "### 8.3. Visualizations We'll Create\n",
    "\n",
    "1. **Utility vs. Threshold Curve**\n",
    "   - Shows how profit changes with different cutoffs\n",
    "   - Helps us find $\\tau^*$\n",
    "\n",
    "2. **ROC Curve**\n",
    "   - Trade-off between True Positive Rate and False Positive Rate\n",
    "   - Area under curve (AUC) summarizes overall performance\n",
    "\n",
    "3. **Calibration Plot**\n",
    "   - Checks if predicted probabilities are reliable\n",
    "   - Good calibration = diagonal line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# GENERATE OUT-OF-FOLD PREDICTIONS\n",
    "# ================================================================================\n",
    "# Get probability predictions for the training set using cross-validation\n",
    "# This ensures we get predictions for each customer using a model that wasn't trained on them\n",
    "# (prevents overfitting in threshold selection)\n",
    "oof_probs = cross_val_predict(\n",
    "    best_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    method=\"predict_proba\",  # Get probabilities, not just class labels\n",
    "    n_jobs=-1,\n",
    ")[:, 1]  # Take probability of default (second column)\n",
    "\n",
    "# ================================================================================\n",
    "# FIND OPTIMAL THRESHOLD USING PER-LOAN ARRAYS\n",
    "# ================================================================================\n",
    "arrays_train = biz.per_loan_arrays(X_train)\n",
    "threshold_curve, best_threshold_row = search_best_threshold_arrays(y_train, oof_probs, arrays_train)\n",
    "best_threshold = float(best_threshold_row[\"threshold\"])\n",
    "\n",
    "print(f\"Best threshold (utility-optimized): {best_threshold:.3f}\")\n",
    "print(f\"Normalized utility at tau*: {best_threshold_row['normalized_utility']:.4f}\")\n",
    "# Note: Approval rule is p(default) < tau*\n",
    "\n",
    "# ================================================================================\n",
    "# CREATE THREE DIAGNOSTIC PLOTS\n",
    "# ================================================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# --- PLOT 1: Utility vs. Threshold ---\n",
    "axes[0].plot(threshold_curve[\"threshold\"], threshold_curve[\"normalized_utility\"], \n",
    "             label=\"Utility/customer\")\n",
    "axes[0].axvline(best_threshold, color=\"red\", linestyle=\"--\", \n",
    "                label=f\"tau*={best_threshold:.2f}\")\n",
    "axes[0].set_xlabel(\"Threshold\")\n",
    "axes[0].set_ylabel(\"Normalized utility\")\n",
    "axes[0].legend()\n",
    "# This shows how profit changes with different decision thresholds\n",
    "\n",
    "# --- PLOT 2: ROC Curve ---\n",
    "fpr, tpr, _ = roc_curve(y_train, oof_probs)\n",
    "axes[1].plot(fpr, tpr, label=f\"ROC AUC={roc_auc_score(y_train, oof_probs):.3f}\")\n",
    "axes[1].plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\")  # Diagonal = random guessing\n",
    "axes[1].set_xlabel(\"False positive rate\")\n",
    "axes[1].set_ylabel(\"True positive rate\")\n",
    "axes[1].set_title(\"Training ROC (OOF)\")\n",
    "axes[1].legend()\n",
    "# Closer to top-left corner = better performance\n",
    "\n",
    "# --- PLOT 3: Calibration Plot ---\n",
    "CalibrationDisplay.from_predictions(\n",
    "    y_train,\n",
    "    oof_probs,\n",
    "    n_bins=10,           # Group predictions into 10 bins\n",
    "    strategy=\"quantile\", # Equal number of samples per bin\n",
    "    ax=axes[2],\n",
    ")\n",
    "axes[2].set_title(\"Calibration (OOF)\")\n",
    "# If points are close to diagonal, probabilities are well-calibrated\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Test Set Evaluation\n",
    "\n",
    "**Now for the moment of truth!** We've been tuning everything on the training data. Let's see how well the model performs on completely unseen customers (the test set).\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1. Primary Metrics\n",
    "\n",
    "We'll evaluate the model using:\n",
    "\n",
    "1. **ROC-AUC and PR-AUC:** Overall discrimination ability\n",
    "   - Can the model separate defaulters from non-defaulters?\n",
    "\n",
    "2. **Confusion Matrix:** Detailed breakdown of predictions\n",
    "   - How many TPs, FPs, TNs, FNs?\n",
    "\n",
    "3. **Total Utility (MOST IMPORTANT!):** Expected profit\n",
    "   - This is what actually matters to the business!\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2. Comparison with Baselines\n",
    "\n",
    "We'll compare our model against three simple strategies:\n",
    "\n",
    "1. **Approve All:** Accept every application\n",
    "   - Maximize revenue but also accept all defaults\n",
    "   \n",
    "2. **Reject All:** Deny every application\n",
    "   - Avoid all default losses but miss all profit opportunities\n",
    "   \n",
    "3. **Threshold = 0.5:** Use the \"naive\" threshold\n",
    "   - Shows the value of optimizing the threshold\n",
    "\n",
    "**Success criterion:** Our optimized model should beat all three!\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3. Segment Analysis\n",
    "\n",
    "We'll break down performance by customer segments:\n",
    "- Low credit limit vs. high credit limit\n",
    "- Different demographics\n",
    "- This helps identify if the model works well for all customer types or just some\n",
    "\n",
    "**Why this matters:** \n",
    "- Ensures the model is fair\n",
    "- Identifies opportunities for targeted strategies\n",
    "- Helps with regulatory compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# GENERATE PREDICTIONS ON TEST SET\n",
    "# ================================================================================\n",
    "# Get probabilities for the held-out test customers\n",
    "test_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert probabilities to approval decisions using optimal threshold (approve if p(default) < tau*)\n",
    "approve_decisions = (test_probs < best_threshold).astype(int)  # 1=approve, 0=reject\n",
    "\n",
    "# For legacy confusion reporting we still compute predicted default label (inverse of approve)\n",
    "test_preds = 1 - approve_decisions  # 1=pred default, 0=pred no default (legacy)\n",
    "\n",
    "# Calculate confusion matrix counts under legacy mapping\n",
    "test_counts = confusion_counts(y_test, test_preds)\n",
    "\n",
    "# Per-loan arrays on test set\n",
    "arrays_test = biz.per_loan_arrays(X_test)\n",
    "\n",
    "# Calculate total utility (profit) using new realized utility definition\n",
    "test_utility = realized_utility_from_arrays(y_test, approve_decisions, arrays_test, normalize=False)\n",
    "\n",
    "# ================================================================================\n",
    "# CALCULATE PERFORMANCE METRICS\n",
    "# ================================================================================\n",
    "roc_auc = roc_auc_score(y_test, test_probs)\n",
    "pr_auc = average_precision_score(y_test, test_probs)\n",
    "\n",
    "print(f\"Test ROC-AUC: {roc_auc:.3f} | PR-AUC: {pr_auc:.3f}\")\n",
    "print(f\"Test utility (total): {test_utility:,.0f} | per customer: {test_utility / len(y_test):.2f}\")\n",
    "print(classification_report(y_test, test_preds, digits=3))\n",
    "\n",
    "# ================================================================================\n",
    "# DISPLAY CONFUSION MATRIX (legacy default vs no-default prediction)\n",
    "# ================================================================================\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, test_preds),\n",
    "    index=pd.Index([\"Actual 0\", \"Actual 1\"], name=\"Actual\"),\n",
    "    columns=pd.Index([\"Pred 0\", \"Pred 1\"], name=\"Predicted\"),\n",
    ")\n",
    "display(cm)\n",
    "\n",
    "# ================================================================================\n",
    "# COMPARE WITH BASELINE STRATEGIES USING PER-LOAN ARRAYS\n",
    "# ================================================================================\n",
    "# Baselines expressed in approval space:\n",
    "baseline_approve = {\n",
    "    \"approve_all\": np.ones_like(y_test),          # approve everyone\n",
    "    \"reject_all\": np.zeros_like(y_test),         # reject everyone\n",
    "    \"threshold_0.50\": (test_probs < 0.5).astype(int),  # naive threshold\n",
    "}\n",
    "\n",
    "baseline_rows = []\n",
    "for name, approve_vec in baseline_approve.items():\n",
    "    util = realized_utility_from_arrays(y_test, approve_vec, arrays_test, normalize=True)\n",
    "    # For counts we convert to predicted default label (inverse) for legacy metrics\n",
    "    preds_default = 1 - approve_vec\n",
    "    counts = confusion_counts(y_test, preds_default)\n",
    "    baseline_rows.append({\n",
    "        \"strategy\": name,\n",
    "        \"normalized_utility\": util,\n",
    "        \"tp\": counts[\"tp\"],\n",
    "        \"fp\": counts[\"fp\"],\n",
    "        \"tn\": counts[\"tn\"],\n",
    "        \"fn\": counts[\"fn\"],\n",
    "    })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_rows).sort_values(\"normalized_utility\", ascending=False)\n",
    "display(baseline_df)\n",
    "# Our model should be at the top if it's working well!\n",
    "\n",
    "# ================================================================================\n",
    "# SEGMENT ANALYSIS: PERFORMANCE BY CREDIT LIMIT USING NEW UTILITY\n",
    "# ================================================================================\n",
    "# Create a results dataframe with all information\n",
    "test_results = X_test.copy()\n",
    "test_results[\"y_true\"] = y_test.values\n",
    "test_results[\"prob_default\"] = test_probs\n",
    "# Include both legacy predicted label (default) and approve indicator for downstream reports\n",
    "test_results[\"y_pred\"] = test_preds\n",
    "test_results[\"approve\"] = approve_decisions\n",
    "\n",
    "# Divide customers into 4 groups based on credit limit\n",
    "test_results[\"limit_segment\"] = pd.qcut(\n",
    "    test_results[\"LIMIT_BAL\"], \n",
    "    q=4,                      # 4 quartiles\n",
    "    duplicates=\"drop\"         # Handle ties\n",
    ").astype(str)\n",
    "\n",
    "# Calculate utility for each segment\n",
    "segment_summary = []\n",
    "for seg, df_seg in test_results.groupby(\"limit_segment\"):\n",
    "    arrays_seg = biz.per_loan_arrays(df_seg)\n",
    "    seg_util = realized_utility_from_arrays(df_seg[\"y_true\"], df_seg[\"approve\"], arrays_seg, normalize=True)\n",
    "    segment_summary.append({\n",
    "        \"limit_segment\": seg,\n",
    "        \"customers\": len(df_seg),\n",
    "        \"default_rate\": df_seg[\"y_true\"].mean(),\n",
    "        \"utility_per_customer\": seg_util,\n",
    "    })\n",
    "\n",
    "segment_summary = pd.DataFrame(segment_summary).sort_values(\"utility_per_customer\", ascending=False)\n",
    "display(segment_summary)\n",
    "# This shows if the model works well across all credit limit ranges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sensitivity Analysis: Understanding Model Behavior\n",
    "\n",
    "Let's investigate how sensitive our model is to different choices and settings.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.1. How Does k Affect Performance?\n",
    "\n",
    "**The k parameter** (number of neighbors) has a big impact:\n",
    "\n",
    "- **Small k (e.g., k=11):**\n",
    "  - Decision boundary follows local patterns closely\n",
    "  - More flexible but potentially noisy\n",
    "  - Risk: overfitting to training data\n",
    "\n",
    "- **Large k (e.g., k=41):**\n",
    "  - Decision boundary is smoother\n",
    "  - More stable across different datasets\n",
    "  - Risk: might miss important local patterns\n",
    "\n",
    "**We'll plot:** Utility vs. k to see the trade-off\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2. The Curse of Dimensionality\n",
    "\n",
    "**Problem:** KNN struggles when there are many features (dimensions)\n",
    "- In high dimensions, all points are far apart\n",
    "- \"Nearest\" neighbors might not be that similar!\n",
    "\n",
    "**What this means for our model:**\n",
    "- We have ~25 features after encoding\n",
    "- KNN might struggle to find truly similar customers\n",
    "- Irrelevant features add noise to distance calculations\n",
    "\n",
    "**Potential improvements:**\n",
    "- Feature selection (remove unhelpful variables)\n",
    "- Dimensionality reduction (PCA, feature importance from trees)\n",
    "- Distance weighting by feature importance\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3. Which Mistakes Are Most Costly?\n",
    "\n",
    "Not all errors are equally expensive! Let's think about our cost matrix:\n",
    "\n",
    "| Error Type | Cost | Example |\n",
    "|-----------|------|---------|\n",
    "| False Positive | **-$6,000** | Approve someone who defaults |\n",
    "| False Negative | -$1,200 | Reject someone who would have paid |\n",
    "\n",
    "**Business implication:** \n",
    "- 1 False Positive costs as much as 5 False Negatives!\n",
    "- We should be conservative (only approve when very confident)\n",
    "- This is why our optimal threshold is likely below 0.5\n",
    "\n",
    "**Risk management strategies:**\n",
    "- Set exposure limits per customer segment\n",
    "- Manual review for borderline cases\n",
    "- Monitor for early warning signs of default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# ANALYZE PERFORMANCE ACROSS DIFFERENT VALUES OF K\n",
    "# ================================================================================\n",
    "# Summarize cross-validation results grouped by k (number of neighbors)\n",
    "cv_summary = (\n",
    "    pd.DataFrame(grid.cv_results_)\n",
    "    .groupby(\"param_knn__n_neighbors\")[[\"mean_test_utility\", \"mean_test_roc_auc\", \"mean_test_pr_auc\"]]\n",
    "    .agg(['mean', 'std'])  # Calculate mean and standard deviation across the different weight/distance combinations\n",
    ")\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "cv_summary.columns = ['_'.join(col).strip() for col in cv_summary.columns]\n",
    "\n",
    "# Reset index to make k a regular column\n",
    "cv_summary = cv_summary.reset_index().rename(columns={\"param_knn__n_neighbors\": \"k\"})\n",
    "\n",
    "# Display the summary\n",
    "display(cv_summary)\n",
    "\n",
    "# ================================================================================\n",
    "# VISUALIZE: UTILITY vs. K\n",
    "# ================================================================================\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot the mean utility at each value of k\n",
    "plt.plot(cv_summary[\"k\"], cv_summary[\"mean_test_utility_mean\"], marker=\"o\")\n",
    "\n",
    "# Add a shaded region showing \u00b11 standard deviation\n",
    "# This shows the variability/uncertainty at each k\n",
    "plt.fill_between(\n",
    "    cv_summary[\"k\"],\n",
    "    cv_summary[\"mean_test_utility_mean\"] - cv_summary[\"mean_test_utility_std\"],\n",
    "    cv_summary[\"mean_test_utility_mean\"] + cv_summary[\"mean_test_utility_std\"],\n",
    "    color=\"C0\",\n",
    "    alpha=0.2,  # Semi-transparent\n",
    ")\n",
    "\n",
    "plt.title(\"Utility sensitivity vs k\")\n",
    "plt.xlabel(\"k (neighbors)\")\n",
    "plt.ylabel(\"Normalized utility\")\n",
    "plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# - If utility increases with k: model benefits from more neighbors (smoother boundaries)\n",
    "# - If utility decreases with k: model needs to capture local patterns (more flexibility)\n",
    "# - If there's a peak: that's the optimal bias-variance trade-off!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Interpretability: Understanding Individual Predictions\n",
    "\n",
    "**The Challenge:** KNN doesn't have coefficients or feature importances like logistic regression or decision trees. How do we explain a prediction?\n",
    "\n",
    "---\n",
    "\n",
    "### 11.1. The KNN Explanation Approach\n",
    "\n",
    "For any prediction, we can answer: **\"Who are the neighbors that influenced this decision?\"**\n",
    "\n",
    "**Example explanation:**\n",
    "```\n",
    "Customer #12345 was REJECTED (82% predicted default probability)\n",
    "This decision was based on these 5 similar customers:\n",
    "  - 4 out of 5 neighbors defaulted\n",
    "  - They had similar: high bill amounts, late payments, low credit limits\n",
    "```\n",
    "\n",
    "**What we'll show:**\n",
    "- The k nearest training customers\n",
    "- Their actual outcomes (defaulted or not)\n",
    "- Their distance from the applicant\n",
    "- Key features of the applicant and neighbors\n",
    "\n",
    "---\n",
    "\n",
    "### 11.2. Traceability and Auditability\n",
    "\n",
    "For regulatory compliance and trust, we can provide:\n",
    "\n",
    "1. **Neighbor list:** Which historical customers were used?\n",
    "2. **Distance values:** How similar were they?\n",
    "3. **Feature values:** What characteristics drove the similarity?\n",
    "4. **Decision logic:** How was the probability calculated?\n",
    "\n",
    "This is especially important for:\n",
    "- Explaining rejections to customers\n",
    "- Regulatory audits (fair lending laws)\n",
    "- Internal model monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### 11.3. Limitations of KNN Interpretability\n",
    "\n",
    "**Compared to linear models:**\n",
    "- \u274c No global feature importances (\"age increases default risk by X%\")\n",
    "- \u274c No simple decision rules\n",
    "- \u2705 Local, instance-specific explanations only\n",
    "\n",
    "**Compared to tree models:**\n",
    "- \u274c No clear decision path\n",
    "- \u274c Harder to explain to non-technical stakeholders\n",
    "- \u2705 More granular (based on actual similar cases)\n",
    "\n",
    "**Bottom line:** KNN explanations are **case-based** rather than **rule-based**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7b4a7",
   "metadata": {},
   "source": [
    "### Student note: turning probabilities into decisions (threshold + per\u2011loan utility)\n",
    "\n",
    "- The classifier outputs a probability of default for each applicant: higher = riskier.\n",
    "- Decision rule: approve if predicted default probability < threshold `t`; reject otherwise. In code: `y_hat_approve = (y_prob_default < t)`.\n",
    "- We don\u2019t use a static cost matrix. Instead, we compute per\u2011loan utility arrays using `BusinessModel`:\n",
    "  - `B_TP` (benefit when we approve a non\u2011default)\n",
    "  - `C_FP` (cost when we approve a default)\n",
    "  - `B_TN` (benefit when we reject a default)\n",
    "  - `C_FN` (cost when we reject a non\u2011default)\n",
    "- Confusion terms here use \u201capprove=1\u201d as the positive class:\n",
    "  - TP: approve non\u2011default -> +B_TP\n",
    "  - FP: approve default     -> \u2212C_FP\n",
    "  - TN: reject default      -> +B_TN\n",
    "  - FN: reject non\u2011default  -> \u2212C_FN\n",
    "- We evaluate utility over a grid of thresholds (e.g., 0.05\u20130.95) and pick the `best_threshold` that maximizes realized utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# SELECT A SAMPLE CUSTOMER TO EXPLAIN\n",
    "# ================================================================================\n",
    "# Let's pick the customer with the HIGHEST predicted default probability\n",
    "# (most likely to be rejected - interesting to explain!)\n",
    "sample_idx = test_results[\"prob_default\"].idxmax()\n",
    "\n",
    "# Get this customer's features\n",
    "sample_x = X_test.loc[[sample_idx]]\n",
    "sample_prob = test_results.loc[sample_idx, \"prob_default\"]\n",
    "sample_true = test_results.loc[sample_idx, \"y_true\"]\n",
    "\n",
    "# ================================================================================\n",
    "# FIND THE NEAREST NEIGHBORS\n",
    "# ================================================================================\n",
    "# Extract the preprocessing and KNN components from our pipeline\n",
    "preprocessor = best_model.named_steps[\"preprocess\"]\n",
    "knn_estimator = best_model.named_steps[\"knn\"]\n",
    "\n",
    "# Transform the training data using the fitted preprocessor\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "\n",
    "# Transform our sample customer the same way\n",
    "sample_transformed = preprocessor.transform(sample_x)\n",
    "\n",
    "# Create a NearestNeighbors object with the same settings as our KNN classifier\n",
    "nn = NearestNeighbors(\n",
    "    n_neighbors=min(5, knn_estimator.n_neighbors),  # Show at most 5 neighbors\n",
    "    metric=knn_estimator.metric,                    # Use same distance metric\n",
    "    p=knn_estimator.p,                              # Use same p value (Manhattan/Euclidean)\n",
    ")\n",
    "\n",
    "# Fit on the transformed training data\n",
    "nn.fit(X_train_transformed)\n",
    "\n",
    "# Find the neighbors of our sample customer\n",
    "distances, indices = nn.kneighbors(sample_transformed)\n",
    "\n",
    "# ================================================================================\n",
    "# DISPLAY THE NEIGHBORS\n",
    "# ================================================================================\n",
    "# Get the original (untransformed) feature values of the neighbors\n",
    "neighbor_records = (\n",
    "    X_train.iloc[indices[0]]  # Get the neighbors' original features\n",
    "    .assign(\n",
    "        distance=distances[0],              # Add how far away each neighbor is\n",
    "        actual=y_train.iloc[indices[0]].values,  # Add whether they defaulted\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print explanation header\n",
    "print(f\"Sample applicant idx {sample_idx} | true={sample_true} | p(default)={sample_prob:.3f}\")\n",
    "\n",
    "# Show the sample customer's features\n",
    "display(sample_x.assign(prob_default=sample_prob, actual=sample_true))\n",
    "\n",
    "# Show the neighbors who influenced this prediction\n",
    "display(neighbor_records)\n",
    "\n",
    "# Interpretation guide:\n",
    "# - Look at the 'actual' column: how many neighbors defaulted?\n",
    "# - Compare feature values: which features make this customer similar to defaulters?\n",
    "# - Check 'distance': are the neighbors truly similar or is the model uncertain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Fairness, Ethics, and Regulatory Compliance\n",
    "\n",
    "Machine learning models in lending must be **fair**, **transparent**, and **compliant** with regulations.\n",
    "\n",
    "---\n",
    "\n",
    "### 12.1. Fairness: Checking for Bias Across Groups\n",
    "\n",
    "**The concern:** Does the model perform differently for different demographic groups?\n",
    "\n",
    "**What we'll check:**\n",
    "- **By gender:** Are approval rates and error rates similar for men vs. women?\n",
    "- **By education level:** Does the model work equally well for all education levels?\n",
    "\n",
    "**Metrics to compare across groups:**\n",
    "1. **True Positive Rate (TPR):** Of those who would pay, what % do we approve?\n",
    "2. **False Positive Rate (FPR):** Of those who would default, what % do we wrongly approve?\n",
    "3. **Positive Predictive Value (PPV):** Of those we approve, what % actually pay?\n",
    "4. **Utility per customer:** Is the model profitable for all groups?\n",
    "\n",
    "**Red flags:**\n",
    "- Large differences in TPR (some groups denied opportunities)\n",
    "- Large differences in FPR (some groups unfairly targeted)\n",
    "- Negative utility for any group\n",
    "\n",
    "---\n",
    "\n",
    "### 12.2. Ethical and Legal Considerations\n",
    "\n",
    "**Protected attributes** (regulated in many countries):\n",
    "- Gender, race, age, marital status, etc.\n",
    "- We use some of these (SEX, MARRIAGE) but must ensure fair treatment\n",
    "\n",
    "**Legal requirements:**\n",
    "- **Fair lending laws:** No discriminatory practices\n",
    "- **Adverse action notices:** Must explain why someone was rejected\n",
    "- **Model governance:** Documentation, validation, monitoring\n",
    "\n",
    "**Best practices:**\n",
    "- Remove sensitive attributes if they're not needed\n",
    "- Watch for proxy variables (e.g., ZIP code as proxy for race)\n",
    "- Regular audits for disparate impact\n",
    "- Human review for borderline cases\n",
    "\n",
    "---\n",
    "\n",
    "### 12.3. Monitoring and Model Governance\n",
    "\n",
    "**Ongoing monitoring is critical:**\n",
    "\n",
    "1. **Data drift:** Are customer characteristics changing?\n",
    "   - Example: Sudden increase in applications from new demographic\n",
    "\n",
    "2. **Performance drift:** Is accuracy degrading?\n",
    "   - Example: Default patterns change during economic recession\n",
    "\n",
    "3. **Decision logs:** Track all predictions and outcomes\n",
    "   - Required for audits\n",
    "   - Helps identify issues early\n",
    "\n",
    "**Retraining schedule:**\n",
    "- Quarterly or when drift is detected\n",
    "- Re-validate fairness metrics after each update\n",
    "- Document all model versions and changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94afbf",
   "metadata": {},
   "source": [
    "### Student note: how to read the final reports\n",
    "\n",
    "- Confusion matrix: shows counts of correct/incorrect approvals and rejections.\n",
    "- Precision: among rejected applicants, how many truly default. Recall: among all defaulters, how many we successfully reject.\n",
    "- ROC AUC: ability to rank risky applicants higher across all thresholds. PR AUC: focus on the minority positive class.\n",
    "- Decisions: counts of approvals and rejections at the chosen threshold, which affects operational capacity.\n",
    "- Utility: the metric that matters for the business; we report total and per\u2011application expected utility for the test set.\n",
    "- Use these together: choose the threshold that gives an acceptable trade\u2011off between growth (approvals) and risk (defaults) while maximizing utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# FAIRNESS / SEGMENT CHECKS\n",
    "# ================================================================================\n",
    "# We'll compare performance across subgroups to spot potential bias.\n",
    "\n",
    "def group_report(df, group_col):\n",
    "    \"\"\"\n",
    "    Summarize model performance for each value within a group column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain columns: 'y_true' (actual), 'y_pred' (predicted default label), and the group_col.\n",
    "        If an 'approve' column is not present, it will be inferred as (1 - y_pred).\n",
    "    group_col : str\n",
    "        Column name by which to group (e.g., 'SEX', 'EDUCATION').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per group value with counts-based metrics and realized utility/customer.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - TPR (recall): Of actual positives, how many did we predict as positive?\n",
    "    - FPR: Of actual negatives, how many did we incorrectly predict as positive?\n",
    "    - PPV (precision): Of predicted positives, how many are truly positive?\n",
    "    - utility_per_customer: Profit per customer using per-loan arrays and approval decisions.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for value, subset in df.groupby(group_col):\n",
    "        # Counts on legacy label space (predicted default vs actual)\n",
    "        counts = confusion_counts(subset[\"y_true\"], subset[\"y_pred\"])  # tp, fp, tn, fn\n",
    "\n",
    "        # Guard against division by zero in case a group is tiny\n",
    "        tpr = counts[\"tp\"] / max(counts[\"tp\"] + counts[\"fn\"], 1)\n",
    "        fpr = counts[\"fp\"] / max(counts[\"fp\"] + counts[\"tn\"], 1)\n",
    "        ppv = counts[\"tp\"] / max(counts[\"tp\"] + counts[\"fp\"], 1)\n",
    "\n",
    "        # Realized utility using approval decisions and per-loan arrays\n",
    "        approve_vec = subset[\"approve\"] if \"approve\" in subset.columns else (1 - subset[\"y_pred\"])  # 1=approve\n",
    "        arrays_g = biz.per_loan_arrays(subset)\n",
    "        util = realized_utility_from_arrays(subset[\"y_true\"], approve_vec, arrays_g, normalize=True)\n",
    "        rows.append({\n",
    "            group_col: value,\n",
    "            \"customers\": len(subset),\n",
    "            \"default_rate\": subset[\"y_true\"].mean(),\n",
    "            \"tpr\": tpr,\n",
    "            \"fpr\": fpr,\n",
    "            \"ppv\": ppv,\n",
    "            \"utility_per_customer\": util,\n",
    "        })\n",
    "\n",
    "    # Higher utility is better; for tpr/ppv high is good, for fpr low is good\n",
    "    return pd.DataFrame(rows).sort_values(\"utility_per_customer\", ascending=False)\n",
    "\n",
    "print(\"Fairness/segment checks\")\n",
    "sex_report = group_report(test_results, \"SEX\")\n",
    "education_report = group_report(test_results, \"EDUCATION\")\n",
    "\n",
    "# Display with short explanation\n",
    "display(sex_report.style.set_caption(\"By SEX: Higher utility and TPR with lower FPR are better\"))\n",
    "display(education_report.style.set_caption(\"By EDUCATION: Check for large disparities\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Deployment Plan (MVP)\n",
    "### 13.1. Export\n",
    "Serialize the full pipeline, including preprocessing and scaling.\n",
    "\n",
    "### 13.2. Operational requirements\n",
    "Acceptable inference latency with indexes/precomputed neighborhoods if needed. Fallback policies if the service fails.\n",
    "\n",
    "### 13.3. Updates and retraining\n",
    "Retraining cadence and triggers for drift or utility degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusions and Next Steps\n",
    "### 14.1. Key findings\n",
    "With proper scaling and a utility-optimized threshold, KNN can improve utility vs simple rules and baselines.\n",
    "\n",
    "### 14.2. Improvement roadmap\n",
    "- Features: more recent behavioral variables, robust aggregations.  \n",
    "- Model: benchmark against more scalable methods (regularized logistic, trees, gradient boosting).  \n",
    "- Business: refine costs/benefits with actual LGD/recovery data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A. Data Dictionary (operational summary)\n",
    "- LIMIT_BAL: assigned credit limit.  \n",
    "- SEX, EDUCATION, MARRIAGE, AGE: demographic characteristics.  \n",
    "- PAY_0 ... PAY_6: monthly payment status (ordinal, indicates delays).  \n",
    "- BILL_AMT1 ... BILL_AMT6: monthly billed amounts.  \n",
    "- PAY_AMT1 ... PAY_AMT6: monthly paid amounts.  \n",
    "- default_payment_next_month: binary target (1=default).  \n",
    "Note: use only information available at decision time; align feature time windows with the target horizon to prevent leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B. Reproducibility Checklist\n",
    "- Fixed and recorded random seeds.  \n",
    "- Documented stratified splits.  \n",
    "- Pipeline with preprocessing inside CV.  \n",
    "- Library and dataset versions.  \n",
    "- Final hyperparameters and operating threshold $\\\\tau^*$ saved.  \n",
    "- Scripts/notebook with run instructions and artifact signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C. Business Scenario Definitions (if no official inputs)\n",
    "To run threshold optimization without official costs:  \n",
    "- Conservative scenario: FP very costly (high LGD), FN moderate (opportunity cost).  \n",
    "- Balanced scenario: FP and FN similar magnitude; maximize global utility.  \n",
    "- Growth-aggressive scenario: FN costly (missed growth), FP moderate; control losses via exposure caps.  \n",
    "Report results per scenario and select the one meeting risk constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201403c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business summary: interpret the trained model in plain English\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "except Exception:\n",
    "    confusion_matrix = precision_score = recall_score = f1_score = None\n",
    "\n",
    "# Helper formatters\n",
    "\n",
    "def fmt_pct(x):\n",
    "    try:\n",
    "        return f\"{100*float(x):,.1f}%\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "def fmt(x):\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and (math.isnan(x) or math.isinf(x))):\n",
    "            return \"N/A\"\n",
    "        if isinstance(x, (int, np.integer)):\n",
    "            return f\"{int(x):,}\"\n",
    "        return f\"{float(x):,.4f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "print(\"=== Business Interpretation of the KNN Credit Default Model ===\\n\")\n",
    "\n",
    "# Identify ground truth and size\n",
    "has_y_test = 'y_test' in globals()\n",
    "y_true = globals().get('y_test') if has_y_test else globals().get('y')\n",
    "n_test = int(len(y_true)) if y_true is not None else 0\n",
    "prevalence = float(y_true.mean()) if y_true is not None and len(y_true)>0 else 0.0\n",
    "\n",
    "# Threshold and predictions\n",
    "best_threshold = float(globals().get('best_threshold', 0.5))\n",
    "probs = globals().get('test_probs') if 'test_probs' in globals() else globals().get('y_pred_proba')\n",
    "if 'test_preds' in globals() and globals().get('test_preds') is not None:\n",
    "    preds = globals().get('test_preds')\n",
    "else:\n",
    "    preds = (probs >= best_threshold).astype(int) if probs is not None else None\n",
    "\n",
    "print(\"Data snapshot:\")\n",
    "print(f\"- Test set size: {fmt(n_test)} observations\")\n",
    "print(f\"- Default prevalence (share of 1s): {fmt_pct(prevalence)}\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "TN = FP = FN = TP = None\n",
    "cm_df = globals().get('cm')\n",
    "if isinstance(cm_df, pd.DataFrame) and cm_df.shape == (2, 2):\n",
    "    try:\n",
    "        TN, FP, FN, TP = cm_df.values.ravel().tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "elif confusion_matrix and (y_true is not None) and (preds is not None):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, preds).ravel().tolist()\n",
    "\n",
    "# Metrics\n",
    "roc_auc = globals().get('roc_auc')\n",
    "pr_auc = globals().get('pr_auc')\n",
    "precision = recall = f1 = None\n",
    "if (y_true is not None) and (preds is not None) and precision_score:\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        precision = float(precision_score(y_true, preds, zero_division=0))\n",
    "        recall = float(recall_score(y_true, preds, zero_division=0))\n",
    "        f1 = float(f1_score(y_true, preds, zero_division=0))\n",
    "\n",
    "print(\"Model quality metrics:\")\n",
    "print(f\"- ROC AUC (ranking quality across thresholds): {fmt(roc_auc)}\")\n",
    "print(f\"- PR AUC (focus on the positive class): {fmt(pr_auc)}\")\n",
    "if precision is not None:\n",
    "    print(f\"- Precision @ chosen threshold: {fmt(precision)}\")\n",
    "    print(f\"- Recall @ chosen threshold:    {fmt(recall)}\")\n",
    "    print(f\"- F1 @ chosen threshold:        {fmt(f1)}\")\n",
    "print(f\"- Chosen probability threshold: {fmt(best_threshold)}\\n\")\n",
    "\n",
    "if None not in (TN, FP, FN, TP):\n",
    "    total = TN + FP + FN + TP\n",
    "    print(\"Confusion matrix on test set (Actual rows, Predicted columns):\")\n",
    "    print(\"            Pred 0    Pred 1\")\n",
    "    print(f\"Actual 0    {fmt(TN):>7}   {fmt(FP):>7}\")\n",
    "    print(f\"Actual 1    {fmt(FN):>7}   {fmt(TP):>7}\")\n",
    "    print(f\"- False Positive rate (approve bad): {fmt_pct(FP / (FP + TN) if (FP+TN)>0 else 0)}\")\n",
    "    print(f\"- False Negative rate (reject good): {fmt_pct(FN / (FN + TP) if (FN+TP)>0 else 0)}\\n\")\n",
    "\n",
    "# Approvals and utility\n",
    "approve_decisions = globals().get('approve_decisions')\n",
    "num_approved = int(np.sum(approve_decisions)) if approve_decisions is not None else (int(np.sum(preds==0)) if preds is not None else None)\n",
    "approval_rate = (num_approved / n_test) if (num_approved is not None and n_test>0) else None\n",
    "print(f\"Decisions at threshold {fmt(best_threshold)}:\")\n",
    "if num_approved is not None and n_test:\n",
    "    print(f\"- Number approved (predicted 0): {fmt(num_approved)}  ({fmt_pct(approval_rate)})\")\n",
    "    print(f\"- Number rejected (predicted 1): {fmt(n_test - num_approved)}  ({fmt_pct(1-approval_rate)})\\n\")\n",
    "else:\n",
    "    print(\"- Decision counts unavailable (missing predictions).\\n\")\n",
    "\n",
    "utility = globals().get('test_utility', globals().get('util', None))\n",
    "if utility is not None:\n",
    "    per_app = (utility / n_test) if (n_test and n_test>0) else None\n",
    "    print(\"Business utility (as computed in notebook):\")\n",
    "    print(f\"- Total expected utility on test set: {fmt(utility)}\")\n",
    "    if per_app is not None:\n",
    "        print(f\"- Expected utility per application:   {fmt(per_app)}\\n\")\n",
    "\n",
    "# Per-loan arrays and business parameters summary\n",
    "biz = globals().get('biz', None)\n",
    "BUSINESS_PARAMS = globals().get('BUSINESS_PARAMS', None)\n",
    "if BUSINESS_PARAMS:\n",
    "    print(\"Business parameters (used to compute per-loan arrays):\")\n",
    "    for k in [\n",
    "        'ead_method','cap_to_limit','apr','cost_of_funds','horizon_months',\n",
    "        'lgd','origination_cost','service_cost_monthly','tn_benefit_flat','collection_cost_flat']:\n",
    "        if k in BUSINESS_PARAMS:\n",
    "            print(f\"- {k}: {BUSINESS_PARAMS[k]}\")\n",
    "    print()\n",
    "\n",
    "print(\"Per-loan utility arrays and what they mean:\")\n",
    "print(\"- B_TP = EAD * (APR - cost_of_funds) * (horizon_months/12)\")\n",
    "print(\"- C_FP = EAD * LGD + collection_cost_flat\")\n",
    "print(\"- B_TN = tn_benefit_flat (constant per rejected default)\")\n",
    "print(\"- C_FN = max(0, B_TP - (origination_cost + service_cost_monthly * horizon_months))\")\n",
    "print(\"Decision convention: y_hat_approve = 1 if prob_default < threshold; else 0.\\n\")\n",
    "\n",
    "# High\u2011level interpretation\n",
    "print(\"How to interpret these results:\")\n",
    "if (precision is not None) and (recall is not None):\n",
    "    if recall >= 0.7 and (precision is not None and precision < 0.5):\n",
    "        print(\"- We are aggressive at catching defaulters (high recall), but we also reject many good customers (lower precision).\")\n",
    "        print(\"  Consider increasing the threshold slightly to approve more good applicants while keeping acceptable risk.\")\n",
    "    elif precision is not None and precision >= 0.7 and (recall is not None and recall < 0.5):\n",
    "        print(\"- We are conservative (high precision): most rejections are truly risky, but we miss many defaulters (lower recall).\")\n",
    "        print(\"  If business impact of missed defaults is high, consider lowering the threshold to catch more risk.\")\n",
    "    else:\n",
    "        print(\"- Precision and recall are reasonably balanced for the chosen threshold.\")\n",
    "        print(\"  Small threshold tweaks can shift the trade\u2011off depending on capacity and risk appetite.\")\n",
    "else:\n",
    "    print(\"- Use ROC AUC and PR AUC to choose an operating point; then validate confusion matrix and costs for that threshold.\")\n",
    "\n",
    "print(\"\\nNext steps you can try:\")\n",
    "print(\"- Move the threshold up/down and recompute utility to match business goals (growth vs. risk).\")\n",
    "print(\"- Compare KNN with a logistic regression or tree\u2011based model; choose the one with better utility at your chosen threshold.\")\n",
    "print(\"- Recheck feature scaling and nearest\u2011neighbor K to ensure stable performance across cross\u2011validation folds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a0426",
   "metadata": {},
   "source": [
    "## Business conclusions and recommendations\n",
    "\n",
    "- The tuned KNN still ranks risk credibly (ROC AUC ~0.75, PR AUC ~0.51) with precision/recall near 0.51/0.52 at the utility-focused threshold\u2014good enough to drive value once we price outcomes with the arrays-based utility.\n",
    "- Operating at the utility-maximizing threshold (tau ~0.28) approves about 77% of applicants. Roughly 14% of good customers are wrongly rejected and ~48% of defaulters still slip through\u2014classic growth vs. risk trade-offs for unsecured credit.\n",
    "- The policy delivers strong positive economics (~$11.6M total utility on the 6k-test set, ~$1.9k per applicant). Raising the threshold increases approvals/growth but lifts the default share; lowering it catches more bad loans at the cost of additional false rejections.\n",
    "- Consider segment-specific thresholds if marginal profits differ by credit limit, geography, or tenure. The notebook already surfaces `segment_summary`, which can be used to tailor cutoffs in high- or low-margin slices.\n",
    "- Next iteration ideas: benchmark logistic regression, gradient-boosted trees, or calibrated neural nets under the same BUSINESS_PARAMS; select the model/threshold pair that maximizes out-of-sample utility subject to fairness and operational guardrails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized metrics for model comparison (prints copy-ready block)\n",
    "import os, json, numpy as np, pandas as pd\n",
    "\n",
    "MODEL_NAME = 'KNN'\n",
    "DATASET = str(globals().get('DATA_FILE', globals().get('DEFAULT_DATA_FILENAME', 'unknown')))\n",
    "\n",
    "# Pull metrics from existing variables computed in the notebook\n",
    "n_test = int(globals().get('n_test', len(globals().get('y_test', [])) or 0))\n",
    "prevalence = float(globals().get('prevalence', 0.0))\n",
    "roc_auc = float(globals().get('roc_auc', 'nan'))\n",
    "pr_auc = float(globals().get('pr_auc', 'nan'))\n",
    "precision = float(globals().get('precision', 'nan')) if 'precision' in globals() else np.nan\n",
    "recall = float(globals().get('recall', 'nan')) if 'recall' in globals() else np.nan\n",
    "f1 = float(globals().get('f1', 'nan')) if 'f1' in globals() else np.nan\n",
    "best_threshold = float(globals().get('best_threshold', 0.5))\n",
    "TN = int(globals().get('TN', 0)) if 'TN' in globals() else None\n",
    "FP = int(globals().get('FP', 0)) if 'FP' in globals() else None\n",
    "FN = int(globals().get('FN', 0)) if 'FN' in globals() else None\n",
    "TP = int(globals().get('TP', 0)) if 'TP' in globals() else None\n",
    "num_approved = int(globals().get('num_approved', 0)) if 'num_approved' in globals() else None\n",
    "utility = float(globals().get('test_utility', globals().get('util', np.nan)))\n",
    "\n",
    "# Derived rates\n",
    "fprate = (FP / (FP + TN)) if (FP is not None and TN is not None and (FP+TN)>0) else np.nan\n",
    "fnrate = (FN / (FN + TP)) if (FN is not None and TP is not None and (FN+TP)>0) else np.nan\n",
    "approval_rate = (num_approved / n_test) if (num_approved is not None and n_test>0) else np.nan\n",
    "rejection_rate = (1 - approval_rate) if not np.isnan(approval_rate) else np.nan\n",
    "util_per_app = (utility / n_test) if (n_test and n_test>0 and not np.isnan(utility)) else np.nan\n",
    "\n",
    "metrics = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'dataset': os.path.basename(DATASET),\n",
    "    'n_test': n_test,\n",
    "    'prevalence': prevalence,\n",
    "    'roc_auc': roc_auc,\n",
    "    'pr_auc': pr_auc,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'threshold': best_threshold,\n",
    "    'TN': TN, 'FP': FP, 'FN': FN, 'TP': TP,\n",
    "    'fprate': fprate,\n",
    "    'fnrate': fnrate,\n",
    "    'approval_rate': approval_rate,\n",
    "    'rejection_rate': rejection_rate,\n",
    "    'utility_total': utility,\n",
    "    'utility_per_app': util_per_app,\n",
    "}\n",
    "\n",
    "print(\"=== MODEL METRICS (copy-to-markdown) ===\")\n",
    "for k in ['model_name','dataset','n_test','prevalence','roc_auc','pr_auc','precision','recall','f1','threshold','TN','FP','FN','TP','fprate','fnrate','approval_rate','rejection_rate','utility_total','utility_per_app']:\n",
    "    print(f\"{k}: {metrics[k]}\")\n",
    "print(\"=== END MODEL METRICS ===\")\n",
    "\n",
    "# Nicely formatted comparison table for immediate viewing\n",
    "_df = pd.DataFrame([metrics])\n",
    "cols = ['model_name','dataset','n_test','prevalence','roc_auc','pr_auc','precision','recall','f1','threshold','TN','FP','FN','TP','fprate','fnrate','approval_rate','rejection_rate','utility_total','utility_per_app']\n",
    "try:\n",
    "    display(_df[cols])\n",
    "except Exception:\n",
    "    print(_df[cols].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec729ef5",
   "metadata": {},
   "source": [
    "## Model evaluation summary (comparison-ready)\n",
    "\n",
    "Use this standardized block to compare across models trained on the same dataset and objective.\n",
    "\n",
    "- Model: KNN\n",
    "- Dataset: default of credit card clients.xls\n",
    "- Test size: 6,000\n",
    "- Default prevalence: 22.1%\n",
    "- Threshold: 0.28\n",
    "\n",
    "Key metrics:\n",
    "- ROC AUC: 0.7470\n",
    "- PR AUC: 0.5120\n",
    "- Precision (at threshold): 0.5080\n",
    "- Recall (at threshold): 0.5240\n",
    "- F1 (at threshold): 0.5160\n",
    "\n",
    "Confusion matrix (actual rows, predicted columns):\n",
    "- TN: 3,999 | FP: 674\n",
    "- FN: 632   | TP: 695\n",
    "- False positive rate (wrongly reject good): 14.4%\n",
    "- False negative rate (still approve defaulters): 47.6%\n",
    "\n",
    "Decision mix:\n",
    "- Approval rate: 77.2%\n",
    "- Rejection rate: 22.8%\n",
    "\n",
    "Business utility:\n",
    "- Total expected utility (test set): 11,649,601\n",
    "- Expected utility per application: 1,941.60\n",
    "\n",
    "Interpretation for the business:\n",
    "- Ranking quality is solid (ROC ~0.75). At the selected threshold, precision and recall are balanced near 0.51, which is typical in credit where defaults are relatively rare.\n",
    "- The approval rate (~77%) favors growth, while the false negative rate shows nearly half of defaulters are still accepted\u2014leaving room to tighten if risk costs rise.\n",
    "- If growth is the priority, raise the threshold slightly to approve more good applicants (watch the default share). If risk reduction is the priority, lower the threshold to catch more defaulters while accepting more rejections.\n",
    "- Segment-specific thresholds (already summarized in `segment_summary`) can unlock additional utility if economics differ across credit-limit bands or customer cohorts.\n",
    "\n",
    "How to compare with other models:\n",
    "- Keep the same train/test split and BUSINESS_PARAMS.\n",
    "- Report the same fields above for each model (including the chosen threshold and utility).\n",
    "- Prefer the model/threshold pair with the highest out-of-sample expected utility that also meets operational and fairness constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6cf02",
   "metadata": {},
   "source": [
    "## Business parameters explained (arrays-based utility)\n",
    "\n",
    "We compute per-loan utility arrays from these parameters; this replaces any static cost matrix.\n",
    "\n",
    "- ead_method: how we estimate Exposure at Default (EAD)\n",
    "  - \"limit\": use `LIMIT_BAL` as EAD\n",
    "  - \"avg_bill3\": average of BILL_AMT1..3\n",
    "  - \"max_bill6\": max of BILL_AMT1..6\n",
    "- cap_to_limit: if true, cap EAD by `LIMIT_BAL`\n",
    "- apr: annual percentage rate charged to approved customers\n",
    "- cost_of_funds: our financing cost; profit uses APR \u2212 cost_of_funds\n",
    "- horizon_months: number of months we measure profit/costs\n",
    "- lgd: loss given default; fraction of EAD we lose if a default occurs\n",
    "- origination_cost: one-time cost to originate a loan\n",
    "- service_cost_monthly: servicing cost per month for approved accounts\n",
    "- tn_benefit_flat: fixed benefit for correctly rejecting a defaulter (e.g., avoided operational costs)\n",
    "- collection_cost_flat: fixed collection/legal expense when default occurs\n",
    "\n",
    "Per-loan arrays derived from these inputs:\n",
    "- B_TP = EAD \u00d7 (APR \u2212 cost_of_funds) \u00d7 (horizon_months/12)\n",
    "- C_FP = EAD \u00d7 LGD + collection_cost_flat\n",
    "- B_TN = tn_benefit_flat\n",
    "- C_FN = max(0, B_TP \u2212 (origination_cost + service_cost_monthly \u00d7 horizon_months))\n",
    "\n",
    "Decision convention:\n",
    "- y_true: 1 = default, 0 = no default\n",
    "- y_hat_approve: 1 = approve (prob_default < threshold), 0 = reject\n",
    "- Utility mapping:\n",
    "  - TP: approve non-default -> +B_TP\n",
    "  - FP: approve default     -> \u2212C_FP\n",
    "  - TN: reject default      -> +B_TN\n",
    "  - FN: reject non-default  -> \u2212C_FN\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
