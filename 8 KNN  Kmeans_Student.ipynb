{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e32b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1f609",
   "metadata": {},
   "source": [
    "# 1. KNN for Classification\n",
    "\n",
    "Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for ``sklearn.neighbors.KNeighborsClassifier``.  Below are three important parametersÔºö \n",
    "\n",
    "- **n_neighbors**: int,`` default=5``. Number of neighbors to use. \n",
    "- **p**: int, ``default=2``, which means euclidean_distance (l2) will be used. When `p` = 1, use manhattan_distance (l1). \n",
    "-  **weights**:  Weight function used in prediction.  ``Default= ‚Äòuniform‚Äô``, which means all neighbors are weighted equally. Other options include ``‚Äòdistance‚Äô`` : i.e., weight instances by the inverse of their distance to the new instance (i.e., weighted voting).\n",
    "\n",
    "Let's use the default setting for modeling and prediction: i.e., euclidean distance with uniform weights for all neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6b148",
   "metadata": {},
   "source": [
    "## 1.1 Data Preparation\n",
    "\n",
    "The **iris** data set consists of **4 features for 150 flowers**:  Sepal Length, Sepal Width, Petal Length, and Petal Width.  The target variable is the **species** of those flowers: ``Setosa(0)``, ``Versicolour(1)``, and ``Virginica(2)``. \n",
    "\n",
    "Check [this link]( https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) for more details of this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y = True, as_frame = True)\n",
    "\n",
    "display(y.unique(), X.describe())  # display unique values of y,   descriptive statistics of X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516591b",
   "metadata": {},
   "source": [
    "### 1.1(a) Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d783cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)\n",
    "\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "328c4333",
   "metadata": {},
   "source": [
    "### 1.1(b) Scale features\n",
    "\n",
    "``StandardScaler`` standardizes the data by removing the mean from each value and devide the result by standard deviation. Check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for details.\n",
    "\n",
    "\n",
    "$$\n",
    "z = \\frac{x - u}{\\sigma\\ } \\quad \\\n",
    "$$\n",
    "\n",
    "\n",
    "After standardization, the mean of scaled training feature will be 0, and standard deviation will be 1.\n",
    "\n",
    "- Check the mean (u) and standard deviation (ùùà) of the scaled test features, why their mean is not 0 and std is not 1?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d73b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "\n",
    "scaler = StandardScaler()                           # define the scaler\n",
    "\n",
    "scaler.fit(X_train)                                 # train the scaler on training data   \n",
    "X_train_scaled = scaler.transform(X_train)          # apply the scaler to transform the training data\n",
    "#X_train_scaled = scaler.fit_transform(X_train)     # combine the above two lines together\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)            # apply the scaler to transform test data\n",
    "\n",
    "print(\"Mean for X_train_scaled: \", X_train_scaled.mean(axis = 0))   # aggregate across rows (default = None for numpy array) \n",
    "print(\"Std for X_train_scaled: \", X_train_scaled.std(axis = 0))\n",
    "print(\"Mean for X_test_scaled: \", X_test_scaled.mean(axis = 0))\n",
    "print(\"Std for X_test_scaled: \", X_test_scaled.std(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4f107",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2  Modeling\n",
    "\n",
    "\n",
    "### 1.2(a) Train ``m1`` with ``k``=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b843f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "m1.fit(X_train_scaled, y_train)\n",
    "\n",
    "train_score = m1.score(X_train_scaled, y_train)\n",
    "test_score = m1.score(X_test_scaled,y_test)\n",
    "\n",
    "print(\"1-NN Train Acc: {:.2%}; Test Acc: {:.2%}\".format(train_score,test_score))  # 100% training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afdc46",
   "metadata": {},
   "source": [
    "**Check the neighbors**\n",
    "\n",
    "The ``kneighbors`` method shows the neighbors for given instances:   (1) the distance(s) between each instance and its nearest neighbor(s); (2) the index of the neighbor(s) for each instance.\n",
    "\n",
    "Let's check the training instances' neighbors (can also be applied to test).\n",
    "\n",
    "- Note that when ``k`` = 1, the nearest neighbor for a training instance is itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f59c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m1.kneighbors(X_train_scaled)   # uncomment to see the result: distances are all 0, index are themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238593d",
   "metadata": {},
   "source": [
    "\n",
    "**Predict class labels**\n",
    "\n",
    "- Let's apply the model to predict test instance's target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26446f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.predict(X_test_scaled)       #  use X_test_scaled for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123d1f1",
   "metadata": {},
   "source": [
    "**Estimate class probability**\n",
    "\n",
    "Estimate the class probability for each test instance.  **Why the probabilities are either 100% or 0%?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.predict_proba(X_test_scaled)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54391c8",
   "metadata": {},
   "source": [
    "### 1.2(b) Train  ``m2`` (k = 5)  and ``m3`` (k = 50)\n",
    "\n",
    "- Please check the train and test accuracy for each model. Which one is better?\n",
    "- Estimate the class probabilities for test instances. \n",
    "\n",
    "<font color=red>***Exercise 1: Your Codes Here***</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c666e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca445174-868e-4f0d-b896-63eb2f48f3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9223b-6640-4eba-b95d-eaf3372a2d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bd27f-ade7-447b-8c43-10f25aec1a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee85d1d6",
   "metadata": {},
   "source": [
    "## 1.3 GridSearchCV\n",
    "\n",
    "- Bigger ``k`` values:     more neighbors used in prediction, simpler model (decision boundary).\n",
    "- Smaller ``k`` values:    less neigbors used, more complicated model which tends to overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "k_range = {'n_neighbors': np.arange(1,84,2)}                            #  k = 1,3,5, ... 83, why max k = 83? \n",
    "\n",
    "grid = GridSearchCV(estimator = knn, param_grid = k_range, cv = 5)      # 5-CV on train (105 - sub_train 84 + validation 21)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)       # search over the values on Train data (with 5-cv)\n",
    "\n",
    "best_param = grid.best_params_          # k value that returns highest mean cv score during cv\n",
    "best_cv_score = grid.best_score_        # mean cv score of the best k  \n",
    "\n",
    "print(\"Best Params: {}\".format(best_param)) \n",
    "print(\"Mean cv score of the best k: {:.2%}\".format(best_cv_score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a768cfa",
   "metadata": {},
   "source": [
    "### 1.3(a) Visualize mean cv scores for each k value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783edf4-cde3-436b-92b5-c8677c592159",
   "metadata": {},
   "source": [
    "\n",
    "Use `matplotlib.pyplot.plot` function (check [documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)) to create a line (i.e., the model).   By default, a solid blue line is created to connect all points. \n",
    "\n",
    "- `x` : the horizontal coordinate of the data points.\n",
    "- `y` : the vertical coordinate of the data points.\n",
    "\n",
    "Note the function relies on **positional arguments** for `x`,  `y` (and `fmt`), NOT **keyword argument**, therefore there is no need to specify the parameter names.  \n",
    "\n",
    "- Other keyword parameters such as  `linewidth`, `markersize`, `color`/`c` (which override the color in `fmt`) should be placed after positional paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = grid.cv_results_['param_n_neighbors']        # k values, same as k_range['n_neighbors']\n",
    "cv_scores = grid.cv_results_['mean_test_score']         # mean cv scores for each k   \n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(k_values,  cv_scores)        # a line plot (default blue solid line)\n",
    "plt.xlabel('No. of Neighbors (k values)')\n",
    "plt.ylabel('Mean CV Score')\n",
    "plt.title('k-NN: Best k = {} & Mean CV Score : {:.2%}'.format(best_param['n_neighbors'], best_cv_score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1291a9",
   "metadata": {},
   "source": [
    "### 1.3(b) Evaluate and apply the best model \n",
    "\n",
    "- Note that the best model was refitted on entire training data in the search process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac50feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test_scaled,y_test)     #check the best model's generalization performance\n",
    "\n",
    "#Alternatively\n",
    "#knn_best = grid.best_estimator_    \n",
    "#knn_best.score(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24816f79-75f0-408f-a06b-8e779a862009",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.predict(X_test_scaled)          # class predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119e866-ebdd-4c7b-8b33-c6cd2efd36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.predict_proba(X_test_scaled)   # class probability estimation test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5109113",
   "metadata": {},
   "source": [
    "# 2. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f4f54b",
   "metadata": {},
   "source": [
    "## 2.1 Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5747ba6",
   "metadata": {},
   "source": [
    "The **Wholesale Customer Data**  comes from the Machine Learning Data Repository of UC Irvine.  Check [this link](https://archive.ics.uci.edu/ml/datasets/wholesale+customers) for more details \n",
    "\n",
    "- Here we removed 19 outliers for simplicity, so the data contains only 421 instances.\n",
    "\n",
    "There are in total 8 variables: \n",
    "\n",
    "- ``Channel``: Horeca (Hotel/Restaurant/Cafe) or Retail channel (Categorical);\n",
    "- ``Region``: Lisnon, Oporto or Other (Categorical); \n",
    "- ``Fresh``: annual spending on fresh products (Continuous);\n",
    "- ``Milk``: annual spending on milk products (Continuous);\n",
    "- ``Grocery``: annual spending on grocery products (Continuous);\n",
    "- ``Frozen``: annual spending on frozen products (Continuous);\n",
    "- ``Detergents_Paper``: annual spending on detergents and paper products (Continuous);\n",
    "- ``Delicassen``: annual spending on and delicatessen products (Continuous);\n",
    "\n",
    "\n",
    "We'd like to group those customers into different clusters according to their purchasing patterns. **As here we only want to explore the underlying structure of our customers, here we use the entire dataset in model training.**  \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99797e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cust_df = pd.read_csv('Wholesale_customers_v2.csv')   # modify data path if needed\n",
    "\n",
    "X = cust_df.drop(columns = ['Channel', 'Region'])     # remove two irrelant columns \n",
    "\n",
    "display(X.shape, X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d761a",
   "metadata": {},
   "source": [
    "**Scale the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)                             # fit/transform on the entire data\n",
    "\n",
    "X_scaled = pd.DataFrame(data = X_scaled, columns = X.columns)  # convert the array as a named dataframe   \n",
    "\n",
    "X_scaled.describe()   # After standardization: mean = 0, std = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e44e0",
   "metadata": {},
   "source": [
    "## 2.2 Modeling \n",
    "\n",
    "The ``sklearn.cluster.KMeans`` function find cluster centroids that minimise the **inertia** or **SSE**. \n",
    "\n",
    "- <font color=red>**n_clusters**</font>: int, ``default=8``. The number of clusters to form.\n",
    "- **init**: method for initialization, ``default=‚Äòk-means++‚Äô`` pushes the centroids as far as possible from each other.  Other options include ``‚Äòrandom‚Äô`` or any array.\n",
    "- **n_init**: int, the number of iterations the algorithm will be run with different initial centroids,  that returns the best output after `n_init` consecutive runs will be the used as initial centroids. By `default = 'auto'`: i.e., 10 if `init='random'`; 1 if `init='k-means++'` or an array-like.\n",
    "- <font color=red>**random_state**</font>: int, ``default = None``, it determines the random number generation for centroid initialization. \n",
    "- **max_iter**: int, ``default=300``, the maximum number of iterations of the algorithm for a single run.\n",
    "- **tol**: float, ``default=1e-4``, the minimum SSE change between two consecutive iterations to declare convergence.\n",
    "\n",
    "Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)  for details.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8a28a",
   "metadata": {},
   "source": [
    "### 2.2(a) Train a model ``model`` with  ``k`` = 2\n",
    "\n",
    "- Let's use two features ``Fresh`` & ``Milk`` only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3668e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = X_scaled[['Fresh','Milk']]                     # 2D array    \n",
    "\n",
    "kmeans = KMeans(n_clusters = 2, random_state = 0)      # set random_state for reproducible result\n",
    "\n",
    "kmeans.fit(X_sub)           \n",
    "\n",
    "kmeans.n_iter_             #  iterations taken to converge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09da3d6",
   "metadata": {},
   "source": [
    "### 2.2(b) Cluster allocation and clustering quality \n",
    "\n",
    "**Cluster Allocation**  \n",
    "\n",
    "Cluster labels will be assigned to each instance automatically in model training process, the labels can be obtained with ``.labels_`` attribute.  \n",
    "- When apply the model to predict the cluster labels for new instances, use the ``predict`` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9028a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_                # cluster labels (for training instances)\n",
    "\n",
    "#kmeans.predict(X_sub)        # same as above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f1c12-4fc7-4d55-bef5-3dab2e655ff0",
   "metadata": {},
   "source": [
    "**Clustering Quality**\n",
    "\n",
    "Clustering quality is measured by **SSE/Inertia**, which is obtained with the `.inertia_` attribute.\n",
    "- The ``.score`` method will return the opposite of SSE (a negative SSE): larger scores means better clustering quality. It is useful in GridSearch and Cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82cdbdf-5f94-46b4-88e9-86446b4a79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_              # SSE: the smaller, the better the model is\n",
    "\n",
    "#kmeans.score(X_sub)         # score: negative SSE (the bigger, the better the model is)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6561cb4",
   "metadata": {},
   "source": [
    "### 2.2(c) Check cluster centroids\n",
    "\n",
    "The centroids are usually the **means feature values of instances in the same cluster**, which can be obtained with the `.cluster_centers_` attribute.\n",
    "\n",
    "- Note if the algorithm stops before fully converging (due to the setting of ``tol`` or ``max_iter``),  the ``cluster_centers_`` are NOT the means of instances in each cluster (i.e., cluster means).\n",
    "\n",
    "  The reason is in the last iteration, the model stops after cluster allocation, then the cluster centroids will not be updated with the new means.\n",
    "\n",
    "- Here our algorithm has been fully converged (i.e., no change in the cluster centroids in the last iteration).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_      # cluster centroids, save for later use\n",
    "centroids\n",
    "\n",
    "# alternatively, caculate the centroids mannually\n",
    "#[X_sub.loc[kmeans.labels_ == 0,'Fresh'].mean(), X_sub.loc[kmeans.labels_ == 0,'Milk'].mean()]    # m1 centroid \n",
    "#[X_sub.loc[kmeans.labels_ == 1,'Fresh'].mean(), X_sub.loc[kmeans.labels_ == 1,'Milk'].mean()]    # m2 centroid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e27f30",
   "metadata": {},
   "source": [
    "### 2.2(d) Visualize data and cluster centroids\n",
    "\n",
    "- Use `matplotlib.pyplot.scatter` function to create scatter plots. Check [documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_sub.copy()                      # create a copy to avoid warning message (optional)\n",
    "\n",
    "df['cluster'] = kmeans.labels_         # save cluster labels for each instance in a new col\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.scatter(x = df['Fresh'], y = df['Milk'], c = df['cluster'])              # map color to cluster labels (numbers)\n",
    "plt.scatter(x = centroids[0,0], y = centroids[0,1], c = 'red', s = 100)      # mark 1st centroid as red, adjust marker size a bit\n",
    "plt.scatter(x = centroids[1,0], y = centroids[1,1], c = 'green', s = 100)    # mark 2nd centroid as green\n",
    "plt.xlabel(\"Fresh\")\n",
    "plt.ylabel(\"Milk\")\n",
    "plt.title('Model 1 with 2 Clusters,  2 Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ead73",
   "metadata": {},
   "source": [
    "## 2.3 Find the Best ``k`` Value  \n",
    "\n",
    "Now let's try all 6 features here: i.e., use `X_scaled` in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00fe46",
   "metadata": {},
   "source": [
    "### 2.3(a) The elbow method \n",
    "\n",
    "The elbow method compares the **clustering quality** for k-means with different k values (often on the training data itself) and choose the **minimum k** value which yields a **relatively low SSE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = np.arange(1, 31, 1)       # compare 30 k values: 1,2,3..,30\n",
    "\n",
    "SSEs = []           # create an empty list \n",
    "for k in k_range:   # loop over all k values\n",
    "    model = KMeans(n_clusters = k, random_state = 0).fit(X_scaled)      # train the model \n",
    "    SSEs.append(model.inertia_)                                         # append SSE one by one to the list\n",
    "\n",
    "len(SSEs)           # how many SSE are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aebc11-a840-4390-a28f-f877b5f3d618",
   "metadata": {},
   "source": [
    "Again, we use `matplotlib.pyplot.plot` function (check [documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)) to create a line plot.  A simple way to adjust the line and marker is format string (`fmt`).\n",
    "\n",
    "- `fmt`: optional, a format string consists of color, marker and line.\n",
    "\n",
    "Note the function relies on **positional arguments** for `x`,  `y` (and `fmt`), NOT **keyword argument**, therefore there is no need to specify the parameter names.  \n",
    "\n",
    "- Other keyword parameters such as  `linewidth`, `markersize`, `color`/`c` (which override the color in `fmt`) should be placed after positional paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the plot\n",
    "\n",
    "plt.figure(figsize = (8,5)) \n",
    "\n",
    "plt.plot(k_range, SSEs,  'bx-',  markersize = 7)                      # 'bx-': blue line with cross as markers\n",
    "#plt.plot(k_range, SSEs, c = 'blue', linestyle = '-', marker = 'x', markersize = 7)       # same as above\n",
    "plt.xlabel(\"K values (No. of Clusters)\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title('KMeans: Elbow Method for the Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a66178",
   "metadata": {},
   "source": [
    "### 2.3(b) The GridSearchCV method\n",
    "\n",
    "<font color=red>***Exercise 2: Your Codes Here***</font>  \n",
    "\n",
    "Please perform ``GridSearchCV`` with 5-fold cv on `X_scaled` to find the best ``k`` from the same range as 2.3(a), use the same random state.   \n",
    "\n",
    "\n",
    " \n",
    "- Check the best k value and its mean cv score during cross-validation.  **Does the best k value make sense?** \n",
    "- Visualize the mean cv scores againt the k values.     \n",
    "\n",
    "Note the ``GridSearchCV`` function compares **the mean generalization score (i.e., negative SSE)** in cross-validation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e4f67-c86d-47ed-b6a7-8edc9c89ce0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a3d9c-80a2-44a5-b705-1403f207aec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
