{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `SVC` function from `sklearn.svm` module for **C-Support Vector Classification**. Check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for details. \n",
    "\n",
    "\n",
    "- ``c = 1`` default \n",
    "- ``degree = 3`` default (ignored in non-poly kernels)\n",
    "- ``kernel = 'rbf'`` default, other options include {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}.  To learn more about the kernel functions, check [this link](https://scikit-learn.org/stable/modules/svm.html#kernel-functions). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear SVC\n",
    "\n",
    "To build a linear SVM model for classification, we need to set `kernel` parameter as follows.  \n",
    "\n",
    "```python\n",
    ">>>SVC(kernel='linear')  \n",
    "```\n",
    "\n",
    "Alternatively, use ``LinearSVC``  function (check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) for details) for linear tasks.   \n",
    "\n",
    "- ``LinearSVC`` function use ``'squared_hinge'`` as the loss function by default, while ``SVC`` function use ``'hinge'`` as the loss function. \n",
    "\n",
    "- ``LinearSVC`` function is implemented with the ``liblinear`` optimization method (for linear tasks only), while  `SVC` use  ``libsvm`` optimization method for both linear and non-linear classification tasks. Therefore,  `LinearSVC` function has more flexibility in the choice of penalties (default `'L2'`) and loss functions (default `squared_hinge`) and **scale better to larger datsets**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('svm_data1.csv')   # simulated data (a balanced dataset)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Data** \n",
    "\n",
    "Use `matplotlib.pyplot.scatter` function to create scatter plots. Check [documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) for details.\n",
    "\n",
    "- The `color` or `c` parameter takes a single color (to set same color for all instances) or a list of [colors](https://matplotlib.org/stable/users/explain/colors/colors.html#colors-def) (to set different colors for instances). \n",
    "\n",
    "- To map colors to a numerical array (e.g., the target variable **y**), we should only use `c` parameter (not `color`).  Colors are automatically assigned  according to the numerical values (here 1 as gold, -1 as purple). To change the default setting, you may adjust the `cmap` together with the `c` parameter. \n",
    "\n",
    "In case with non-numeric target **y**,  create a list of colors according to the target values and pass to the `c`/`color` parameter as follows: \n",
    "\n",
    "```python\n",
    ">>> colors = df1['y'].replace({1: 'gold', -1: 'lightblue'})\n",
    ">>> plt.scatter(x = df1['x1'], y = df1['x2'], c = colors) \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = df1['x1'], y = df1['x2'], c = df1['y'])     # same as plt.scatter(data = df1, x = 'x1', y = 'x2', c = 'y') \n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('The 1st Simulated Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split and Scale Data**\n",
    "\n",
    "Regularized models requires scale features for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1[['x1','x2']]\n",
    "\n",
    "y = df1['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)   # test_size = 0.25 (default)\n",
    "\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)   \n",
    "\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "display(X_train_scaled.shape, X_test_scaled.shape)   # 2d arrays without name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.2  Train Two Linear SVC with Different C\n",
    "\n",
    "``C``: inversely proportional to the regularization strength of the coefficients. Must be strictly positive.\n",
    "\n",
    "- Large ``C``  ->  Less tolerant of hinge loss  -> More complicated model (bigger coefficients, better fit) with less SVs. \n",
    "- Small ``C``  ->  More tolerant of hinge loss -> Simpler model (smaller coefficients, worse fit) with more SVs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2(a) Train svm1 with C  =  0.01\n",
    "\n",
    "By default, the `SVC` function doesn't estimate class probability due to the default setting `probabilty = False`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1 = SVC(kernel='linear', C = 0.01)    \n",
    "\n",
    "svm1.fit(X_train_scaled, y_train)\n",
    "\n",
    "display(svm1.intercept_, svm1.coef_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Confident Scores: i.e., f(x).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1.decision_function(X_train_scaled)    # f(x) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1.predict(X_train_scaled)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on Class Probability Estimation**\n",
    "\n",
    "If class probabilities are needed, set `SVC(probability = True, kernel = 'linear')` before model fitting, then use `predict_proba` method of the model to estimate class probabilities.  \n",
    "\n",
    "Here is a brief summary of how SVC models estimate class probabilities.\n",
    "- 1. Split training data into different folds of sub-train and validation.\n",
    "- 2. For each fold, (1) train a SVC model on the sub-train fold and apply it on the validation fold to obtain confidence scores, then (2) fit a logistic regression model with confidence scores (from validation fold) as the feature and their true class labels as target.\n",
    "- 3. Average the parameter values from each validation fold for a final logistic regression model.\n",
    "- 4. When applying the `SVC` model for probability estimation: (1) apply the initial SVC model trained on the entire training set (i.e., the **svm1** here) to the data to obtain confidence scores and then (2) apply the final logistic regression model, obtained from cross-validation, on those confidence scores to obtain probabilities.  \n",
    "  \n",
    "Check the [documentation](https://scikit-learn.org/stable/modules/svm.html#scores-probabilities) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which are Support Vectors?**\n",
    "\n",
    "- Check the number of SVs for each class with `n_support_` attribute.\n",
    "- Check the feature values of each SV with the `support_vectors_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(svm1.n_support_, svm1.support_vectors_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training data and Support Vectors for **svm1**.\n",
    "\n",
    "- visualize the training data in different color based on their target value.\n",
    "- visualize the support vector as red crosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV1 = svm1.support_vectors_   # SV features \n",
    "C1 = svm1.get_params()['C']   # C value \n",
    "\n",
    "plt.scatter(x = X_train_scaled[:,0], y = X_train_scaled[:,1], c = y_train)      # plot data (colors auto-assigned)\n",
    "plt.scatter(x = SV1[:, 0], y = SV1[:, 1],  c = 'red',  marker='x')              # plot SVs as red crosses \n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(\"Linear SVC (C={})\".format(C1))       #  format string     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2(b) Train svm2 with C = 100\n",
    "\n",
    "- Less tolerant of hinge loss, resulting in a more complicated model (larger coefficients -> narrower margins and less SVs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm2 = SVC(kernel='linear', C = 100) \n",
    "\n",
    "svm2.fit(X_train_scaled, y_train)\n",
    "\n",
    "display(svm2.intercept_, svm2.coef_, svm2.n_support_)    # a more complicated model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training data and support vectors for **svm2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV2 = svm2.support_vectors_   # SV features\n",
    "C2 = svm2.get_params()['C']   # C value\n",
    "\n",
    "plt.scatter(x = X_train_scaled[:,0], y = X_train_scaled[:,1], c = y_train)\n",
    "plt.scatter(x = SV2[:, 0], y = SV2[:, 1], c = 'red', marker='x')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(\"Linear SVC (C = {})\".format(C2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Holdout Testing  \n",
    "\n",
    "<font color=red>***Exercise 1: Your Codes Here***</font>  \n",
    "\n",
    "Is **svm2** overfitting? Please compare their train and test performance respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4(a) Splitting Strategies\n",
    "\n",
    "Both the two splitting strategies return the training and test indices in each split (interation).\n",
    "\n",
    "**K-Fold**\n",
    "\n",
    "`K-Fold` CV Splitter (check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)) splits data into `k` consecutive folds (no shuffling by default), and iterate over each fold: with one fold used for testing while the remaining k-1 folds for model training.\n",
    "\n",
    "- Note in each split, the class distribution are different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "splitter1 = KFold(n_splits = 5)                                      # no shuffling by default\n",
    "#splitter1 = KFold(n_splits = 5, shuffle = True, random_state = 1)   # to shuffle data first\n",
    "\n",
    "for train_ind, test_ind in splitter1.split(X):    # X - features(800,2)  \n",
    "    train_no = len(train_ind)                     # no. of training indices       \n",
    "    test_no =  len(test_ind)                      # no. of test indices  \n",
    "    train_pos = sum(y[train_ind]==1)              # no. of pos instances in training  \n",
    "    test_pos = sum(y[test_ind]==1)                # no. of pos instances in test   \n",
    "    print('No. of Instances: Train {}, Test {}'.format(train_no, test_no))                 # display no of instances in train/test\n",
    "    print('%Pos: Train {:.2%}, Test {:.2%}'.format(train_pos/train_no, test_pos/test_no))  # display %pos in train/test\n",
    "    print('-------------------') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Stratified KFold** \n",
    "\n",
    "The ``StratifiedKFold`` CV Splitter (check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)) is a variation of ``KFold``. It returns stratified folds that **preserves class distribution** in the whole dataset.  \n",
    "\n",
    "- Note in each split, the class distribution are the same/similiar to the class distribution in the whole dataset (i.e. 50.37%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "splitter2 = StratifiedKFold(n_splits = 5)                                        # no shuffling by default\n",
    "#splitter2 = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)     # to shuffle data first\n",
    "\n",
    "for train_ind, test_ind in splitter2.split(X, y):         # provide both X (features) and y (target)\n",
    "    train_no = len(train_ind)\n",
    "    test_no =  len(test_ind)\n",
    "    train_pos = sum(y[train_ind]==1)\n",
    "    test_pos = sum(y[test_ind]==1)\n",
    "    print('No. of Instances: Train {}, Test {}'.format(train_no, test_no))      \n",
    "    print('%Pos: Train {:.2%}, Test {:.2%}'.format(train_pos/train_no, test_pos/test_no))  \n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4(b) Cross-validation for Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, in each split (iteration), we should train and apply the scaler on training features before model training, then apply the scaler to transform test features before model evaluation. That means,  in each split, we should use a **pipeline** to chain the **scaler** and **model** together.  Check [here](https://scikit-learn.org/1.5/modules/compose.html#pipeline) if you are interested in Pipeline.\n",
    "\n",
    "- For simplicity, here we use the unscaled data directly for cross-validation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Comparison with Cross-Validation**\n",
    "\n",
    "The ``cross_val_score`` function from `sklearn.model_selection` module requires the following parameters.   Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)  for details.\n",
    "\n",
    "\n",
    "- ``estimator``: the model\n",
    "- ``X``: feature values\n",
    "- ``y``: target values\n",
    "- ``scoring``: model evaluation metrics (default = None, i.e., `accuracy` for classification models, `R2` for regression models)\n",
    "- ``cv``: plitting strategy (default = None). You may pass an `integer` (which use `StratifiedKFold` for classification,  `KFold` for other models) or a `CV splitter`.\n",
    "\n",
    "    By default ``shuffle`` = False, to enable data shuffing, you may pass a customized splitter as follows. \n",
    " \n",
    "```python\n",
    "splitter = StratifiedKFold(n_splits = 5, shuffle=True, random_state= 1)          # shuffle data first\n",
    "svm1_cv = cross_val_score(estimator = svm1, X = X_scaled, y = y, cv = splitter)  # pass a splitter to `cv` \n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation for svm1\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svm1 = SVC(kernel='linear', C = 0.01)           # define a model \n",
    "\n",
    "svm1_cv = cross_val_score(estimator = svm1,    # the model\n",
    "                          X = X,               # features (entire dataset)\n",
    "                          y = y,               # target (entire dataset)\n",
    "                          cv = 5)              # 5-fold cv without shuffling\n",
    "\n",
    "display(svm1_cv, svm1_cv.mean(), svm1_cv.std())   # return test scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 2: Your Codes Here***</font>  \n",
    "\n",
    "\n",
    "Please also conduct 5-fold cross validation for **svm2** (C = 100).  Which model is better? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 GridSearchCV: Find the Best C Value\n",
    "\n",
    "The `GridSearchCV` function from `sklearn.model_selection` search over specified parameter values for a model. Check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for more details. \n",
    "\n",
    "- ``refit``= ``True`` (default): refit an model on the entire training set using the best parameter found in cross validation. \n",
    "\n",
    "- By default, only the validation scores are returned.  If training scores for each split are needed, set ``return_train_score`` = ``True`` (default ``False``). \n",
    "\n",
    "How many models have been trained in this process?  ``No.of C values`` * ``cv`` + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_linear = SVC(kernel='linear')  \n",
    "\n",
    "range1 = {'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 5, 10, 100, 1000, 2000]}    # 10 C values \n",
    "\n",
    "grid1 = GridSearchCV(estimator = svm_linear,      # the model \n",
    "                     param_grid = range1,         # C values to compare \n",
    "                     cv = 5)                      # StratifiedKFold without shuffling (default)\n",
    "\n",
    "grid1.fit(X_train_scaled, y_train)               # search over all C values on training data  \n",
    "\n",
    "print(\"Best Params: \", grid1.best_params_)                 # best C value (which returns the highest mean cv score)   \n",
    "print(\"Best cv score: {:.2%}\".format(grid1.best_score_))   # average validation score of the best C value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cv_results_` attribute of the gridsearch object returns the results in a dictionary.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid1.cv_results_)      # convert as dataframe for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Best Model**  \n",
    "\n",
    "With the best ``C`` value found,  a new model was re-fitted on the entire train set (note by default ``refit``= ``True``).  \n",
    "\n",
    "- We can check the coefficients, number of support vectors, or apply the model for prediction and evaluation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_best = grid1.best_estimator_       # get the best model refitted on the training set\n",
    "\n",
    "#linear_best = SVC(kernel = 'linear', C = 0.1).fit(X_train_scaled, y_train)  # alternatively \n",
    "\n",
    "display(linear_best.intercept_,linear_best.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also evaluate the best model on test data.\n",
    "\n",
    "- The ``GridSearchCV`` object has ``predict`` and ``score`` methods, which use the best model refit on the entire training set for prediction and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1.score(X_test_scaled, y_test)        # same as linear_best.score(X_test_scaled, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Nonlinear SVC: Polynomial Kernel\n",
    "\n",
    "To train a polynomial SVC,  change the `kernel` parameter as folows.  \n",
    " \n",
    "\n",
    "\n",
    "```python\n",
    ">>>SVC(kernel='poly')  \n",
    "```\n",
    "\n",
    "\n",
    "To control model complexity of a polynomial kernel, we adjust the parameter ``degree`` (int, default=``3``) as well.  \n",
    "\n",
    "- Note that attributes such as ``coef_`` is only available when ``kernel = 'linear'``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('svm_data2.csv')   \n",
    "\n",
    "df2.shape   # a small simulated data with 200 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization \n",
    "\n",
    "plt.scatter(x = df2['x1'], y = df2['x2'], c = df2['y'])   \n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('The 2nd Simulated Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split and Scale Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2[['x1','x2']]\n",
    "\n",
    "y = df2['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)   # testsize=0.25 by default\n",
    "\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)    \n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "display(X_train_scaled.shape, X_test_scaled.shape)   # 2d arrays without name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GridSearchCV: Find the Best C and Degree Together \n",
    "\n",
    "Now we have two regularization/complexity parameters to tune together!  It may takes a while as the model is  more complicated now.\n",
    "\n",
    "- How many models will be trained in this process?  ``No. of C`` * ``No. of degree`` * ``CV`` + 1\n",
    "\n",
    "- Let's set ``cv = 3`` to reduce the searching time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range2 = {'C': [0.01, 1, 100], 'degree': [3, 5, 6]}     # 3 * 3 params \n",
    "\n",
    "poly = SVC(kernel='poly')      \n",
    "\n",
    "grid2 = GridSearchCV(estimator = poly, param_grid = range2, cv = 3) \n",
    "\n",
    "grid2.fit(X_train_scaled, y_train)      \n",
    "\n",
    "print(\"Best Params:{}\".format(grid2.best_params_))  # best param found in cross validation\n",
    "print(\"Mean cv score of the best Params: {:.2%}\".format(grid2.best_score_))   # average validation score of the best param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 3: Your Codes Here***</font>  \n",
    "\n",
    "\n",
    "Check the result and answer this question: **why the searching process takes so long?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Nested Cross-validation for GridSearch (Optional)\n",
    "\n",
    "The **nested cross-validation for GridSearch** (1) splits the data into multiple train-test folds (outer CV); and (2) in each split, a GridSearchCV object (inner CV) is fitted on the train folds to find the best params, then a best model (with best param) is refitted on entire train set and then evaluated on the test set.   \n",
    "\n",
    "- Here we use the same model (`poly`),  parameter range (`range2`), as well as the same number of folds (`3`), as in Section 2.2 to conduct **GridSearch with nested 3-fold cross-validation**. \n",
    "\n",
    "- Nested cross-validation for GridSearch is a very expensive procedure: here as the `range2` contains 9 combinations of parameter settings, therefore (9 * 3 + 1) * 3 = 84 models have been built!   \n",
    "\n",
    "You may noticed that each inner CV  (i.e., GridSearchCV) may return different best parameters, as the training folds are different in each split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a split strategy for outer CV (3 fold for time sake)\n",
    "cv_folds = StratifiedKFold(n_splits = 3)   \n",
    "\n",
    "# create an empty list to save test scores of the best models in each inner CV\n",
    "out_scores = []  \n",
    "\n",
    "# loop over each split (outer CV -  no data scaling for simplicity)\n",
    "for train_ind, test_ind  in cv_folds.split(X, y):           \n",
    "\n",
    "    # get train and test folds in each split\n",
    "    X_train, y_train = X.iloc[train_ind], y[train_ind]     \n",
    "    X_test, y_test = X.iloc[test_ind], y[test_ind]          \n",
    "        \n",
    "    # search over params on train folds (inner CV)\n",
    "    search = GridSearchCV(estimator = poly, param_grid = range2,  cv = 3)    # same as section 2.2\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    best_param = search.best_params_           # best param found in inner CV  (sub-train)\n",
    "    best_cv_score = search.best_score_         # average cv score of the best param \n",
    "    print('Best params= {}:  mean cv score= {:.2%}'.format(best_param, best_cv_score))  # display them\n",
    "\n",
    "    # evaluate the best model (refitted on train) on test fold\n",
    "    test_score = search.score(X_test, y_test)   \n",
    "    out_scores.append(test_score)              # append test score in a list\n",
    "    \n",
    "# when all splits are looped over,  display test score for the three best models\n",
    "print(\"Test scores for the best models:  \", out_scores)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above codes can be simplified as follows.\n",
    "\n",
    "```python\n",
    "search = GridSearchCV(estimator = poly, param_grid = range2, cv = 3)          # inner CV  \n",
    "out_scores = cross_val_score(estimator = search, X = X, y = y, cv = 3)        # outer CV (for-loop)\n",
    "print(\"Test scores for the best models: \", out_scores)                        # best models's test scores  \n",
    "```\n",
    "\n",
    "Here the result is **a list of test scores for the best models** (same as above).  The test scores tell us how well a model generalizes, given the best parameters found by GridSearch (which is not reported).  \n",
    "\n",
    "- As it doesn’t provide a best model (nor parameter setting), we cannot use it on future prediction.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ca8fa919ad3f5b95432d0cc3734ec3ee062223d9e74cb89aba2bf763f64a40f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
