{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5324e2a5",
   "metadata": {},
   "source": [
    "# Assignment 2 Notebook Overview\n",
    "\n",
    "Short guide for working through the assignment deliverables.\n",
    "\n",
    "**Contents**\n",
    "- Section 1: environment setup and first dataset inspection.\n",
    "- Section 2: Question 1 logistic-regression walkthrough with visualization.\n",
    "- Section 3: Question 2 odds-ratio arithmetic.\n",
    "- Section 4: Question 3 SVM interpretation notes.\n",
    "- Section 5: Question 4 end-to-end logistic regression using `smoking.csv`.\n",
    "\n",
    "> All numeric answers remain rounded to two decimals unless otherwise noted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268044af",
   "metadata": {},
   "source": [
    "## Assignment 2 Instructions\n",
    "Assignment 2 mimics the original prompt so the notebook stands alone. Keep the following global notes visible while you work:\n",
    "- **Note 1:** When answering with Python, show the source cells and the printed or plotted results. Additional explanation can go in comments, markdown, or `print()` statements.\n",
    "- **Note 2:** When answering manually, write the algebraic steps so graders can follow your reasoning.\n",
    "- **Note 3:** Round every numeric answer to two decimal places unless a question explicitly requests different precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1002cbc",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and First Look\n",
    "\n",
    "Load helper libraries, style the plotting backend, and pull the shared `smoking.csv` dataset for reuse across later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822e58c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:39.071593Z",
     "iopub.status.busy": "2025-10-09T11:04:39.071193Z",
     "iopub.status.idle": "2025-10-09T11:04:43.076592Z",
     "shell.execute_reply": "2025-10-09T11:04:43.075553Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Assignment 2 | Setup + first data inspection\n",
    "# ----------------------------------------------\n",
    "\n",
    "from pathlib import Path  # safer path handling that works on any OS\n",
    "\n",
    "# Core data wrangling / math helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries (used for quick EDA and model diagnostics)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn utilities you will need throughout the assignment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "# Keep matplotlib plots inside the notebook and apply a readable default style\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load the assignment dataset (ships with the notebook repository)\n",
    "# ------------------------------------------------------------------\n",
    "DATA_PATH = Path(\"smoking.csv\")          # update if you store the CSV elsewhere\n",
    "smoking_df = pd.read_csv(DATA_PATH)      # load the CSV into a pandas DataFrame\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Lightweight structural checks (verbose on purpose for clarity)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Dataset overview\")\n",
    "print(f\"Rows:    {smoking_df.shape[0]}\")\n",
    "print(f\"Columns: {smoking_df.shape[1]}\")\n",
    "print(\"Column names in order (helps when writing formulas later):\")\n",
    "for idx, col in enumerate(smoking_df.columns, start=1):\n",
    "    print(f\"  {idx:>2}. {col}\")\n",
    "\n",
    "display(smoking_df.head())  # Quick structure overview\n",
    "display(smoking_df.info())  # Field types and null counts\n",
    "display(smoking_df.describe(include='all').T)  # Summary stats for numeric + categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afdf30",
   "metadata": {},
   "source": [
    "## 2. Question 1 - Logistic Regression Walkthrough\n",
    "\n",
    "Implements the analytical steps for the toy two-feature model (log-odds, predicted probability, decision boundary plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d8453",
   "metadata": {},
   "source": [
    "### Question 1 prompt (Logistic Regression, 30 pts)\n",
    "We observe students with predictors $X_1 =$ hours studied and $X_2 =$ cumulative GPA. The binary outcome is $y = 1$ if the student earns an A and $y = 0$ otherwise. A logistic regression model with parameters $w_0 = -7$, $w_1 = 0.05$, and $w_2 = 1$ has already been trained. Complete each part (Python or manual work is acceptable):\n",
    "- **(a)** Compute the probability that a student who studies for 40 hours and has GPA 3.5 earns an A.\n",
    "- **(b)** Solve for the number of study hours needed for a 50% chance at an A when GPA remains 3.5.\n",
    "- **(c)** Report the odds ratio and log-odds for the student in part (a), and interpret the odds ratio.\n",
    "- **(d)** Plot the linear decision boundary $w_0 + w_1 x_1 + w_2 x_2 = 0$ with $X_1$ on the x-axis and $X_2$ on the y-axis. Highlight the regions predicting A vs. not-A and emphasize the (40, 3.5) example (manual or software plot is acceptable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a15b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:43.082192Z",
     "iopub.status.busy": "2025-10-09T11:04:43.081602Z",
     "iopub.status.idle": "2025-10-09T11:04:43.520729Z",
     "shell.execute_reply": "2025-10-09T11:04:43.519757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 1 - Logistic Regression walkthrough\n",
    "\n",
    "# Break down the toy logistic-regression model that uses study hours (X1) and GPA (X2).\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Model parameters provided in the prompt ----\n",
    "w0 = -7.0   # intercept term\n",
    "w1 = 0.05   # coefficient for X1 = hours studied\n",
    "w2 = 1.0    # coefficient for X2 = cumulative GPA\n",
    "\n",
    "# ---- Helper values reused across parts (a)-(c) ----\n",
    "hours_example = 40        # X1 from the problem statement\n",
    "gpa_example = 3.5         # X2 from the problem statement\n",
    "\n",
    "def sigmoid(z: float) -> float:\n",
    "    # Standard logistic (sigmoid) function.\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# Part (a) -----------------------------------------------------\n",
    "# Compute the linear combination (log-odds) and convert to probability.\n",
    "logit_a = w0 + w1 * hours_example + w2 * gpa_example\n",
    "prob_a = sigmoid(logit_a)\n",
    "\n",
    "print(\"Part (a)\")\n",
    "print(f\"  Log-odds for 40 hrs / 3.5 GPA: {logit_a:.2f}\")\n",
    "print(f\"  Probability of earning an A:  {prob_a:.2f}\n",
    "\")\n",
    "\n",
    "# Part (b) -----------------------------------------------------\n",
    "# Solve w0 + w1 * hours + w2 * GPA = 0 for hours when GPA stays at 3.5\n",
    "# (since logit = 0 corresponds to P = 0.5).\n",
    "target_probability = 0.50\n",
    "target_logit = np.log(target_probability / (1 - target_probability))  # equals 0.0\n",
    "hours_for_half = (-w0 - w2 * gpa_example) / w1\n",
    "\n",
    "print(\"Part (b)\")\n",
    "print(f\"  Hours needed for 50% chance (same GPA): {hours_for_half:.2f}\n",
    "\")\n",
    "\n",
    "# Part (c) -----------------------------------------------------\n",
    "# Odds (p / (1 - p)) equal exp(log-odds) for logistic regression outputs.\n",
    "odds_a = np.exp(logit_a)\n",
    "\n",
    "print(\"Part (c)\")\n",
    "print(f\"  Odds (p/(1-p)) for these inputs: {odds_a:.2f}\")\n",
    "print(f\"  Log-odds:                     {logit_a:.2f}\")\n",
    "print(\n",
    "    \"  Interpretation: For every 1 student predicted to earn an A, \"\n",
    "    f\"~{(1 / odds_a):.2f} students are predicted not to (given these inputs).\n",
    "\"\n",
    ")\n",
    "\n",
    "# Part (d) -----------------------------------------------------\n",
    "# Plot the linear decision boundary w0 + w1*x1 + w2*x2 = 0.\n",
    "# Rearranged: x2 = (-w0 - w1*x1) / w2, which simplifies to x2 = 7 - 0.05*x1.\n",
    "x1_vals = np.linspace(0, 120, 200)                      # hours studied range\n",
    "decision_boundary = (-w0 - w1 * x1_vals) / w2           # GPA threshold per hour value\n",
    "boundary_label = \"Decision boundary x2 = 7 - 0.05*x1 (log-odds = 0)\"\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x1_vals, decision_boundary, color=\"black\", lw=2, label=boundary_label)\n",
    "plt.scatter(\n",
    "    hours_example,\n",
    "    gpa_example,\n",
    "    color=\"darkred\",\n",
    "    s=80,\n",
    "    label=\"Example student (40 hrs, 3.5 GPA)\",\n",
    ")\n",
    "\n",
    "# Shade the positive (A grade) region: GPA >= boundary (log-odds > 0).\n",
    "plt.fill_between(\n",
    "    x1_vals,\n",
    "    decision_boundary,\n",
    "    4.2,                         # cap shading near the top of the GPA axis\n",
    "    where=decision_boundary <= 4.2,\n",
    "    color=\"mediumseagreen\",\n",
    "    alpha=0.25,\n",
    "    label=\"Region predicting A (y=1)\",\n",
    ")\n",
    "\n",
    "# Shade the negative region: GPA < boundary (log-odds < 0).\n",
    "plt.fill_between(\n",
    "    x1_vals,\n",
    "    0,\n",
    "    decision_boundary,\n",
    "    color=\"lightcoral\",\n",
    "    alpha=0.25,\n",
    "    label=\"Region predicting not-A (y=0)\",\n",
    ")\n",
    "\n",
    "plt.annotate(\n",
    "    f\"p = {prob_a:.2f}\",\n",
    "    xy=(hours_example, gpa_example),\n",
    "    xytext=(hours_example + 10, gpa_example + 0.25),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"dimgray\"),\n",
    "    bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"dimgray\", alpha=0.9),\n",
    ")\n",
    "\n",
    "plt.title(\"Logistic Regression Decision Boundary\\n(x2 = 7 - 0.05*x1)\")\n",
    "plt.xlabel(\"Hours Studied (X1)\")\n",
    "plt.ylabel(\"Cumulative GPA (X2)\")\n",
    "plt.ylim(0, 4.2)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0149297",
   "metadata": {},
   "source": [
    "## 3. Question 2 - Odds-Ratio and Probability\n",
    "\n",
    "Closed-form transforms between probabilities and odds/odds-ratios for the credit default examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2e180",
   "metadata": {},
   "source": [
    "### Question 2 prompt (Odds-ratio and Probability, 20 pts)\n",
    "Answer with Python or manual steps. Round to two decimals.\n",
    "- **(a)** A borrower has a 20% chance of defaulting on her credit card payment ($y = 1$). Compute the odds ratio of defaulting.\n",
    "- **(b)** A manâ€™s odds ratio of defaulting is 0.4. Convert this odds ratio to the corresponding probability of default and report the probability of paying on time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befce649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:43.524805Z",
     "iopub.status.busy": "2025-10-09T11:04:43.524217Z",
     "iopub.status.idle": "2025-10-09T11:04:43.530856Z",
     "shell.execute_reply": "2025-10-09T11:04:43.530053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 2 - Odds and probability transforms\n",
    "\n",
    "# Part (a): convert the known probability into odds (single-event perspective).\n",
    "prob_default = 0.20\n",
    "odds_default = prob_default / (1 - prob_default)\n",
    "\n",
    "# Part (b): convert the provided odds value back into a probability.\n",
    "odds_input = 0.40\n",
    "prob_from_odds = odds_input / (1 + odds_input)\n",
    "\n",
    "print(f\"(a) Odds (p/(1-p)) for defaulting with P=0.20: {odds_default:.2f} (single-event odds, not a ratio of two odds)\")\n",
    "print(f\"    Expected on-time payments per default: {(1 / odds_default):.2f}\")\n",
    "print(f\"(b) Probability implied by odds 0.40: {prob_from_odds:.2f} | probability of paying on time: {1 - prob_from_odds:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5d079",
   "metadata": {},
   "source": [
    "## 4. Question 3 - Support Vector Machine interpretation\n",
    "\n",
    "To make the written answers traceable, I recreate the tiny 12-point dataset\n",
    "shown in the PDF and walk through the SVM geometry with code. The coding\n",
    "checklist is:\n",
    "1. Encode approximate coordinates and labels for the 12 numbered points.\n",
    "2. Evaluate the separating hyperplane, margins, hinge loss, and predicted labels\n",
    "   for every observation.\n",
    "3. Plot the hyperplane plus both margins and annotate each numbered point.\n",
    "4. Pull the exact counts requested in parts (a)?(d) directly from the computed\n",
    "   table before restating the answers in words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2bf10",
   "metadata": {},
   "source": [
    "### Question 3 prompt (Support Vector Machine, 20 pts)\n",
    "A support vector classifier is fit on 12 labeled points (blue = positive, red = negative). The separating hyperplane (solid line) and two margins (dashed lines) are provided. Respond in words:\n",
    "- **(a)** List how many observations are support vectors and identify them.\n",
    "- **(b)** If instance 4 (a red point) moves closer to its margin, explain whether the separating hyperplane changes.\n",
    "- **(c)** Identify the observations that incur hinge loss while remaining correctly classified.\n",
    "- **(d)** Identify the observations that the classifier misclassifies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ac564",
   "metadata": {},
   "source": [
    "### Step 1 - Encode coordinates that mimic the diagram\n",
    "\n",
    "I approximate the 2-D layout by hand so that the blue points (label = +1) sit on\n",
    "or near the positive margin at x = +1, while the red points (label = -1) sit on\n",
    "or near the negative margin at x = -1. Instances 11 and 12 are intentionally\n",
    "placed across the hyperplane (x = 0) to reproduce the misclassifications.\n",
    "These coordinates are approximate and only serve to re-create the qualitative\n",
    "geometry from the PDF figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Each record mirrors the numbering, class, and a short geometric note.\n",
    "step1_points = [\n",
    "    {\"instance\": 1, \"label\": 1, \"color\": \"blue\", \"x1\": 0.80, \"x2\": 2.10,\n",
    "     \"note\": \"Blue point tucked just inside the +1 margin.\"},\n",
    "    {\"instance\": 2, \"label\": 1, \"color\": \"blue\", \"x1\": 0.70, \"x2\": 1.25,\n",
    "     \"note\": \"Second blue point still inside the positive margin.\"},\n",
    "    {\"instance\": 3, \"label\": 1, \"color\": \"blue\", \"x1\": 1.55, \"x2\": 1.60,\n",
    "     \"note\": \"Comfortably outside the positive margin.\"},\n",
    "    {\"instance\": 4, \"label\": -1, \"color\": \"red\", \"x1\": -1.55, \"x2\": 1.45,\n",
    "     \"note\": \"Negative point kept well outside its margin.\"},\n",
    "    {\"instance\": 5, \"label\": 1, \"color\": \"blue\", \"x1\": 1.35, \"x2\": -0.60,\n",
    "     \"note\": \"Blue point on the safe side of the positive margin.\"},\n",
    "    {\"instance\": 6, \"label\": -1, \"color\": \"red\", \"x1\": -1.60, \"x2\": -1.10,\n",
    "     \"note\": \"Negative point with generous margin buffer.\"},\n",
    "    {\"instance\": 7, \"label\": 1, \"color\": \"blue\", \"x1\": 0.60, \"x2\": 0.45,\n",
    "     \"note\": \"Blue support vector nestled inside the margin.\"},\n",
    "    {\"instance\": 8, \"label\": 1, \"color\": \"blue\", \"x1\": 0.55, \"x2\": -0.35,\n",
    "     \"note\": \"Another blue support vector just inside the margin.\"},\n",
    "    {\"instance\": 9, \"label\": -1, \"color\": \"red\", \"x1\": -0.60, \"x2\": 0.65,\n",
    "     \"note\": \"Red point sitting inside the negative margin.\"},\n",
    "    {\"instance\": 10, \"label\": -1, \"color\": \"red\", \"x1\": -1.25, \"x2\": -0.30,\n",
    "     \"note\": \"Negative point safely outside the margin.\"},\n",
    "    {\"instance\": 11, \"label\": -1, \"color\": \"red\", \"x1\": 0.35, \"x2\": 0.20,\n",
    "     \"note\": \"Red point that crosses the hyperplane (misclassified).\"},\n",
    "    {\"instance\": 12, \"label\": -1, \"color\": \"red\", \"x1\": 0.55, \"x2\": -0.50,\n",
    "     \"note\": \"Second misclassified red point on the positive side.\"},\n",
    "]\n",
    "\n",
    "svm_points = (\n",
    "    pd.DataFrame(step1_points)\n",
    "    .set_index(\"instance\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "display(svm_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd460f9",
   "metadata": {},
   "source": [
    "### Step 2 - Evaluate the hyperplane, margins, and hinge loss\n",
    "\n",
    "The diagram's separator is vertical, so I model it with the hyperplane f(x) = x1 = 0,\n",
    "which yields margin lines at x1 = +/- 1. I apply the standard hinge definition\n",
    "hinge = max(0, 1 - y*f(x)), and I tag a point as a support vector whenever\n",
    "y*f(x) <= 1 (on/inside a margin) or when it becomes misclassified. These rules\n",
    "match the geometry drawn in the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a449bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.array([1.0, 0.0])  # Normal vector of the hyperplane x1 = 0\n",
    "b = 0.0                   # Intercept term\n",
    "margin_width = 1.0        # With |W| = 1, the canonical margins sit at x1 = +/- 1\n",
    "\n",
    "def decision_value(row: pd.Series) -> float:\n",
    "    # Return w dot x + b for the chosen hyperplane.\n",
    "    return float(np.dot(W, [row[\"x1\"], row[\"x2\"]]) + b)\n",
    "\n",
    "svm_points = svm_points.copy()\n",
    "svm_points[\"decision_value\"] = svm_points.apply(decision_value, axis=1)\n",
    "svm_points[\"signed_margin\"] = svm_points[\"label\"] * svm_points[\"decision_value\"]\n",
    "svm_points[\"hinge_loss\"] = (1 - svm_points[\"signed_margin\"]).clip(lower=0.0)\n",
    "svm_points[\"support_vector\"] = svm_points[\"signed_margin\"] <= margin_width + 1e-9\n",
    "svm_points[\"predicted_label\"] = np.where(svm_points[\"decision_value\"] >= 0, 1, -1)\n",
    "svm_points[\"misclassified\"] = svm_points[\"predicted_label\"] != svm_points[\"label\"]\n",
    "\n",
    "columns_to_show = [\n",
    "    \"label\", \"x1\", \"x2\", \"decision_value\", \"signed_margin\",\n",
    "    \"hinge_loss\", \"support_vector\", \"misclassified\", \"note\"\n",
    "]\n",
    "display(svm_points[columns_to_show])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ba5b1",
   "metadata": {},
   "source": [
    "### Step 3 ? Plot the hyperplane, both margins, and the numbered points\n",
    "\n",
    "The plot below reproduces the setup programmatically: a solid line for the\n",
    "hyperplane x1 = 0, dashed lines for the two margins, and labels for each\n",
    "observation number so we can visually confirm the table above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 6.5))\n",
    "\n",
    "color_map = {1: \"#3776ab\", -1: \"#c23b22\"}\n",
    "edge_map = {True: \"black\", False: \"none\"}\n",
    "\n",
    "# Draw the hyperplane and its two margins.\n",
    "ax.axvline(0, color=\"black\", linewidth=2.0, label=\"hyperplane (x1 = 0)\")\n",
    "ax.axvline(1, color=\"black\", linestyle=\"--\", linewidth=1.2, label=\"positive margin\")\n",
    "ax.axvline(-1, color=\"black\", linestyle=\"--\", linewidth=1.2, label=\"negative margin\")\n",
    "\n",
    "for instance, row in svm_points.iterrows():\n",
    "    ax.scatter(\n",
    "        row[\"x1\"],\n",
    "        row[\"x2\"],\n",
    "        s=80,\n",
    "        color=color_map[row[\"label\"]],\n",
    "        edgecolor=edge_map[row[\"support_vector\"]],\n",
    "        linewidth=1.5,\n",
    "        alpha=0.9,\n",
    "        zorder=3,\n",
    "    )\n",
    "    ax.text(\n",
    "        row[\"x1\"] + 0.03,\n",
    "        row[\"x2\"] + 0.03,\n",
    "        str(instance),\n",
    "        fontsize=10,\n",
    "        weight=\"bold\",\n",
    "        color=\"#1f1f1f\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"x1 (axis perpendicular to the hyperplane)\")\n",
    "ax.set_ylabel(\"x2 (axis parallel to the hyperplane)\")\n",
    "ax.set_title(\"Reconstructed SVM geometry for the 12 labelled points\")\n",
    "ax.set_xlim(-2.0, 2.0)\n",
    "ax.set_ylim(-1.5, 2.5)\n",
    "ax.grid(True, linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"blue / +1\", markerfacecolor=color_map[1], markersize=8),\n",
    "    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"red / -1\", markerfacecolor=color_map[-1], markersize=8),\n",
    "    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"support vector\", markerfacecolor=\"none\", markeredgecolor=\"black\", markersize=8),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"upper left\", frameon=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538de5d",
   "metadata": {},
   "source": [
    "### Step 4 ? Extract the exact counts for parts (a)?(d)\n",
    "\n",
    "With the computed table we can answer each question mechanically before\n",
    "translating the results back into prose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_ids = svm_points.query(\"support_vector\").index.tolist()\n",
    "hinge_ids = svm_points.query(\"hinge_loss > 0 and misclassified == False\").index.tolist()\n",
    "misclassified_ids = svm_points.query(\"misclassified\").index.tolist()\n",
    "\n",
    "print(\"(a) Support vectors (sorted):\", support_ids)\n",
    "print(\"(b) Instance 4 signed margin:\", round(float(svm_points.loc[4, \"signed_margin\"]), 2))\n",
    "print(\"(c) Hinge loss but correctly classified:\", hinge_ids)\n",
    "print(\"(d) Misclassified instances:\", misclassified_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2c5c4",
   "metadata": {},
   "source": [
    "### Question 3 answers with detailed reasoning\n",
    "\n",
    "**(a)** The code confirms **7 support vectors**: instances 1, 2, 7, 8, 9, 11, and 12.\n",
    "Each lies on or inside a margin (or across the separator), so removing any one of\n",
    "these would move the optimal boundary.\n",
    "\n",
    "**(b)** Instance 4 stays well outside its negative margin (signed margin ~ 1.55), so\n",
    "small nudges toward the dashed line do **not** move the hyperplane. Only when\n",
    "instance 4 reaches the dashed line (signed margin = 1.00) would it become a support\n",
    "vector capable of shifting the separator.\n",
    "\n",
    "**(c)** Hinge loss without misclassification appears for **five instances**: 1, 2, 7, 8,\n",
    "and 9. They reside inside a margin (hinge > 0) yet remain on the correct side of\n",
    "the hyperplane (predicted label matches the true label).\n",
    "\n",
    "**(d)** The reconstruction flags **two misclassified points**: instances 11 and 12. Both\n",
    "are negative points that cross into the positive half-space, so they are\n",
    "misclassified while also acting as support vectors with large hinge loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818a2a3",
   "metadata": {},
   "source": [
    "## 5. Question 4 - Logistic Regression with smoking.csv\n",
    "\n",
    "Workflow checklist:\n",
    "- Features: smkban, female, age; target: smoker.\n",
    "- Deterministic split with random_state=2025 plus stratify=y to keep class balance.\n",
    "- MinMax scaling applied to every feature. It is redundant for smkban/female but\n",
    "  keeps the preprocessing consistent; the rescaling really matters for age.\n",
    "- Metrics reported with two-decimal rounding for easy cross-checking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3703a3b",
   "metadata": {},
   "source": [
    "### Question 4 prompt (Logistic Regression with `smoking.csv`, 30 pts)\n",
    "Use Python with the provided dataset to complete each part:\n",
    "- **(a)** Load `smoking.csv`, report the total number of observations, and count smokers (`smoker = 1`) and non-smokers (`smoker = 0`).\n",
    "- **(b)** Select features `smkban`, `female`, and `age` with target `smoker`. Split the data 80%/20% (train/test) using `random_state=2025` and scale features using `MinMaxScaler`.\n",
    "- **(c)** Train a logistic regression model, show the intercept and coefficients, and interpret each coefficient.\n",
    "- **(d)** Evaluate accuracy on train and test sets (rounded to two decimals) and display the confusion matrix for the test set.\n",
    "- **(e)** Predict smoking status and probability for a 48-year-old male in a country without a smoking ban (use the fitted scaler before scoring).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fa958",
   "metadata": {},
   "source": [
    "### 5.a Dataset overview and class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13923ef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:43.534939Z",
     "iopub.status.busy": "2025-10-09T11:04:43.534504Z",
     "iopub.status.idle": "2025-10-09T11:04:43.541568Z",
     "shell.execute_reply": "2025-10-09T11:04:43.540515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part (a) - dataset overview for logistic regression\n",
    "# Focus on the three requested predictors plus the binary target.\n",
    "feature_cols = [\"smkban\", \"female\", \"age\"]\n",
    "target_col = \"smoker\"\n",
    "\n",
    "num_rows = len(smoking_df)\n",
    "class_counts = smoking_df[target_col].value_counts().sort_index()\n",
    "baseline_label = class_counts.idxmax()\n",
    "baseline_accuracy = class_counts.max() / num_rows\n",
    "\n",
    "print(f\"Total rows: {num_rows}\")\n",
    "print(\"Class counts (0 = non-smoker, 1 = smoker):\")\n",
    "for cls_value, count in class_counts.items():\n",
    "    share = count / num_rows\n",
    "    print(f\"  {cls_value}: {count} ({share:.2%})\")\n",
    "\n",
    "print(\n",
    "    f\"Baseline accuracy if we always predict {baseline_label}: {baseline_accuracy:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa7a41",
   "metadata": {},
   "source": [
    "### 5.b Train/test split with MinMax scaling\n",
    "\n",
    "Split 80/20 with stratification and deterministic seeding, then apply\n",
    "MinMaxScaler. Scaling the binary indicators (smkban, female) does not change\n",
    "their information content but keeps the pipeline uniform; the main rescaling\n",
    "impact falls on age.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10d362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:43.545695Z",
     "iopub.status.busy": "2025-10-09T11:04:43.545306Z",
     "iopub.status.idle": "2025-10-09T11:04:43.569928Z",
     "shell.execute_reply": "2025-10-09T11:04:43.568999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part (b) - split and scale features\n",
    "# Keep stratified sampling to preserve the original class balance.\n",
    "X = smoking_df[feature_cols]\n",
    "y = smoking_df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=2025, stratify=y\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Capture min/max values for transparency after scaling.\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "scaling_summary = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"train_min\": X_train_scaled_df.min().values,\n",
    "    \"train_max\": X_train_scaled_df.max().values,\n",
    "})\n",
    "print(\"Scaled feature range summary (train set)\")\n",
    "display(scaling_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df21b10",
   "metadata": {},
   "source": [
    "### 5.c Model fitting and coefficient interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55ee83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:43.573436Z",
     "iopub.status.busy": "2025-10-09T11:04:43.572990Z",
     "iopub.status.idle": "2025-10-09T11:04:43.598245Z",
     "shell.execute_reply": "2025-10-09T11:04:43.597204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part (c) - fit logistic regression and inspect coefficients\n",
    "# Use lbfgs with a higher iteration cap for reliable convergence.\n",
    "log_reg_model = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "log_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "intercept = float(log_reg_model.intercept_[0])\n",
    "coefficients = pd.Series(log_reg_model.coef_[0], index=feature_cols)\n",
    "\n",
    "print(f\"Intercept: {intercept:.2f}\")\n",
    "coef_df = coefficients.round(2).rename(\"coefficient\").to_frame()\n",
    "display(coef_df)\n",
    "print(\"Interpretation (log-odds change per +1 in scaled feature):\")\n",
    "for feature, coef in coefficients.items():\n",
    "    effect = abs(coef)\n",
    "    if coef > 0:\n",
    "        direction = \"increases\"\n",
    "    elif coef < 0:\n",
    "        direction = \"decreases\"\n",
    "    else:\n",
    "        direction = \"does not change\"\n",
    "    if direction == \"does not change\":\n",
    "        print(f\"  +1.00 in scaled {feature} {direction} the smoker log-odds.\")\n",
    "    else:\n",
    "        print(f\"  +1.00 in scaled {feature} {direction} smoker log-odds by {effect:.2f}\")\n",
    "\n",
    "age_range = smoking_df[\"age\"].max() - smoking_df[\"age\"].min()\n",
    "age_step = 10 / age_range\n",
    "logit_shift_10yrs = coefficients[\"age\"] * age_step\n",
    "odds_multiplier_10yrs = np.exp(logit_shift_10yrs)\n",
    "print(\n",
    "    f\"Age interpretation: +10 actual years (~{age_step:.2f} scaled units) \"\n",
    "    f\"shifts smoker log-odds by {logit_shift_10yrs:.2f} \"\n",
    "    f\"(odds multiplier approx {odds_multiplier_10yrs:.2f}).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e91641",
   "metadata": {},
   "source": [
    "### 5.d Evaluation metrics on train/test splits (accuracy, class metrics, AUCs)\n",
    "\n",
    "Report accuracy alongside precision/recall/F1 for the smoker class, compare\n",
    "against the baseline classifier, and examine ROC-AUC / PR-AUC plus a lower\n",
    "threshold scenario for business-sensitive recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ed762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:43.601811Z",
     "iopub.status.busy": "2025-10-09T11:04:43.601373Z",
     "iopub.status.idle": "2025-10-09T11:04:43.616950Z",
     "shell.execute_reply": "2025-10-09T11:04:43.615968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part (d) - accuracy on train and test splits plus richer diagnostics\n",
    "train_pred = log_reg_model.predict(X_train_scaled)\n",
    "train_proba = log_reg_model.predict_proba(X_train_scaled)[:, 1]\n",
    "test_pred = log_reg_model.predict(X_test_scaled)\n",
    "test_proba = log_reg_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "precision_pos = precision_score(y_test, test_pred, pos_label=1)\n",
    "recall_pos = recall_score(y_test, test_pred, pos_label=1)\n",
    "f1_pos = f1_score(y_test, test_pred, pos_label=1)\n",
    "roc_auc = roc_auc_score(y_test, test_proba)\n",
    "pr_auc = average_precision_score(y_test, test_proba)\n",
    "improvement_pct = (test_acc - baseline_accuracy) * 100\n",
    "\n",
    "print(f\"Train accuracy: {train_acc:.2f}\")\n",
    "print(f\"Test accuracy:  {test_acc:.2f}\")\n",
    "print(f\"Baseline accuracy (always predict {baseline_label}): {baseline_accuracy:.2f}\")\n",
    "print(f\"Accuracy lift vs. baseline: {improvement_pct:.2f} percentage points\")\n",
    "print(f\"Precision (class 1): {precision_pos:.2f}\")\n",
    "print(f\"Recall (class 1):    {recall_pos:.2f}\")\n",
    "print(f\"F1 (class 1):        {f1_pos:.2f}\")\n",
    "print(f\"ROC-AUC:             {roc_auc:.2f}\")\n",
    "print(f\"PR-AUC:              {pr_auc:.2f}\")\n",
    "\n",
    "cm_default = pd.DataFrame(\n",
    "    confusion_matrix(y_test, test_pred),\n",
    "    index=[\"actual_0\", \"actual_1\"],\n",
    "    columns=[\"pred_0\", \"pred_1\"],\n",
    ")\n",
    "print()\n",
    "print(\"Test confusion matrix @ threshold 0.50:\")\n",
    "display(cm_default)\n",
    "\n",
    "alt_threshold = 0.35\n",
    "alt_pred = (test_proba >= alt_threshold).astype(int)\n",
    "cm_alt = pd.DataFrame(\n",
    "    confusion_matrix(y_test, alt_pred),\n",
    "    index=[\"actual_0\", \"actual_1\"],\n",
    "    columns=[\"pred_0\", \"pred_1\"],\n",
    ")\n",
    "precision_alt = precision_score(y_test, alt_pred, pos_label=1)\n",
    "recall_alt = recall_score(y_test, alt_pred, pos_label=1)\n",
    "print()\n",
    "print(f\"Threshold 0.35 -> precision: {precision_alt:.2f}, recall: {recall_alt:.2f}\")\n",
    "print(\"Test confusion matrix @ threshold 0.35:\")\n",
    "display(cm_alt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70280ebf",
   "metadata": {},
   "source": [
    "### 5.e Prediction for the hypothetical individual\n",
    "\n",
    "Transform the new profile with the fitted scaler before evaluating the logistic\n",
    "model so the coefficients align with the earlier preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52576389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T11:04:43.620254Z",
     "iopub.status.busy": "2025-10-09T11:04:43.619912Z",
     "iopub.status.idle": "2025-10-09T11:04:43.630604Z",
     "shell.execute_reply": "2025-10-09T11:04:43.629655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part (e) - prediction for a new individual\n",
    "# Transform the single observation with the same scaler prior to scoring.\n",
    "new_person = pd.DataFrame({\n",
    "    \"smkban\": [0],\n",
    "    \"female\": [0],\n",
    "    \"age\": [48],\n",
    "})\n",
    "new_scaled = scaler.transform(new_person[feature_cols])\n",
    "p_smoker = log_reg_model.predict_proba(new_scaled)[0, 1]\n",
    "pred_label = log_reg_model.predict(new_scaled)[0]\n",
    "print(f\"Predicted class (1=smoker): {int(pred_label)}\")\n",
    "print(f\"Estimated probability of smoking: {p_smoker:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}